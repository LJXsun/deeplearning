{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from solver import Solver\n",
    "from visualize import plot_loss_and_acc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][55]\t Training Loss 2.3767\t Accuracy 0.0780\n",
      "Epoch [0][30]\t Batch [50][55]\t Training Loss 1.9074\t Accuracy 0.3570\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0897\t Average training accuracy 0.2281\n",
      "Epoch [0]\t Average validation loss 1.8266\t Average validation accuracy 0.4256\n",
      "\n",
      "Epoch [1][30]\t Batch [0][55]\t Training Loss 1.8311\t Accuracy 0.4150\n",
      "Epoch [1][30]\t Batch [50][55]\t Training Loss 1.5574\t Accuracy 0.6170\n",
      "\n",
      "Epoch [1]\t Average training loss 1.6976\t Average training accuracy 0.5256\n",
      "Epoch [1]\t Average validation loss 1.4972\t Average validation accuracy 0.6518\n",
      "\n",
      "Epoch [2][30]\t Batch [0][55]\t Training Loss 1.5372\t Accuracy 0.6080\n",
      "Epoch [2][30]\t Batch [50][55]\t Training Loss 1.3459\t Accuracy 0.6810\n",
      "\n",
      "Epoch [2]\t Average training loss 1.4395\t Average training accuracy 0.6511\n",
      "Epoch [2]\t Average validation loss 1.2733\t Average validation accuracy 0.7372\n",
      "\n",
      "Epoch [3][30]\t Batch [0][55]\t Training Loss 1.3455\t Accuracy 0.6900\n",
      "Epoch [3][30]\t Batch [50][55]\t Training Loss 1.1857\t Accuracy 0.7280\n",
      "\n",
      "Epoch [3]\t Average training loss 1.2613\t Average training accuracy 0.7092\n",
      "Epoch [3]\t Average validation loss 1.1155\t Average validation accuracy 0.7792\n",
      "\n",
      "Epoch [4][30]\t Batch [0][55]\t Training Loss 1.1985\t Accuracy 0.7190\n",
      "Epoch [4][30]\t Batch [50][55]\t Training Loss 1.0841\t Accuracy 0.7410\n",
      "\n",
      "Epoch [4]\t Average training loss 1.1334\t Average training accuracy 0.7435\n",
      "Epoch [4]\t Average validation loss 0.9999\t Average validation accuracy 0.8080\n",
      "\n",
      "Epoch [5][30]\t Batch [0][55]\t Training Loss 1.1052\t Accuracy 0.7470\n",
      "Epoch [5][30]\t Batch [50][55]\t Training Loss 1.0072\t Accuracy 0.7630\n",
      "\n",
      "Epoch [5]\t Average training loss 1.0380\t Average training accuracy 0.7661\n",
      "Epoch [5]\t Average validation loss 0.9123\t Average validation accuracy 0.8242\n",
      "\n",
      "Epoch [6][30]\t Batch [0][55]\t Training Loss 0.9565\t Accuracy 0.7970\n",
      "Epoch [6][30]\t Batch [50][55]\t Training Loss 0.9191\t Accuracy 0.7880\n",
      "\n",
      "Epoch [6]\t Average training loss 0.9644\t Average training accuracy 0.7821\n",
      "Epoch [6]\t Average validation loss 0.8436\t Average validation accuracy 0.8382\n",
      "\n",
      "Epoch [7][30]\t Batch [0][55]\t Training Loss 0.9626\t Accuracy 0.7770\n",
      "Epoch [7][30]\t Batch [50][55]\t Training Loss 0.8757\t Accuracy 0.8150\n",
      "\n",
      "Epoch [7]\t Average training loss 0.9059\t Average training accuracy 0.7950\n",
      "Epoch [7]\t Average validation loss 0.7885\t Average validation accuracy 0.8490\n",
      "\n",
      "Epoch [8][30]\t Batch [0][55]\t Training Loss 0.8696\t Accuracy 0.8010\n",
      "Epoch [8][30]\t Batch [50][55]\t Training Loss 0.8674\t Accuracy 0.7880\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8582\t Average training accuracy 0.8049\n",
      "Epoch [8]\t Average validation loss 0.7433\t Average validation accuracy 0.8562\n",
      "\n",
      "Epoch [9][30]\t Batch [0][55]\t Training Loss 0.8420\t Accuracy 0.8060\n",
      "Epoch [9][30]\t Batch [50][55]\t Training Loss 0.8078\t Accuracy 0.8140\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8186\t Average training accuracy 0.8130\n",
      "Epoch [9]\t Average validation loss 0.7056\t Average validation accuracy 0.8632\n",
      "\n",
      "Epoch [10][30]\t Batch [0][55]\t Training Loss 0.7548\t Accuracy 0.8420\n",
      "Epoch [10][30]\t Batch [50][55]\t Training Loss 0.7526\t Accuracy 0.8390\n",
      "\n",
      "Epoch [10]\t Average training loss 0.7851\t Average training accuracy 0.8186\n",
      "Epoch [10]\t Average validation loss 0.6736\t Average validation accuracy 0.8682\n",
      "\n",
      "Epoch [11][30]\t Batch [0][55]\t Training Loss 0.7528\t Accuracy 0.8230\n",
      "Epoch [11][30]\t Batch [50][55]\t Training Loss 0.7718\t Accuracy 0.8110\n",
      "\n",
      "Epoch [11]\t Average training loss 0.7564\t Average training accuracy 0.8236\n",
      "Epoch [11]\t Average validation loss 0.6461\t Average validation accuracy 0.8740\n",
      "\n",
      "Epoch [12][30]\t Batch [0][55]\t Training Loss 0.7492\t Accuracy 0.8260\n",
      "Epoch [12][30]\t Batch [50][55]\t Training Loss 0.7432\t Accuracy 0.8120\n",
      "\n",
      "Epoch [12]\t Average training loss 0.7315\t Average training accuracy 0.8283\n",
      "Epoch [12]\t Average validation loss 0.6222\t Average validation accuracy 0.8764\n",
      "\n",
      "Epoch [13][30]\t Batch [0][55]\t Training Loss 0.7286\t Accuracy 0.8280\n",
      "Epoch [13][30]\t Batch [50][55]\t Training Loss 0.7560\t Accuracy 0.8230\n",
      "\n",
      "Epoch [13]\t Average training loss 0.7096\t Average training accuracy 0.8326\n",
      "Epoch [13]\t Average validation loss 0.6013\t Average validation accuracy 0.8790\n",
      "\n",
      "Epoch [14][30]\t Batch [0][55]\t Training Loss 0.7087\t Accuracy 0.8440\n",
      "Epoch [14][30]\t Batch [50][55]\t Training Loss 0.6906\t Accuracy 0.8180\n",
      "\n",
      "Epoch [14]\t Average training loss 0.6903\t Average training accuracy 0.8354\n",
      "Epoch [14]\t Average validation loss 0.5827\t Average validation accuracy 0.8810\n",
      "\n",
      "Epoch [15][30]\t Batch [0][55]\t Training Loss 0.6887\t Accuracy 0.8330\n",
      "Epoch [15][30]\t Batch [50][55]\t Training Loss 0.6810\t Accuracy 0.8310\n",
      "\n",
      "Epoch [15]\t Average training loss 0.6730\t Average training accuracy 0.8386\n",
      "Epoch [15]\t Average validation loss 0.5662\t Average validation accuracy 0.8832\n",
      "\n",
      "Epoch [16][30]\t Batch [0][55]\t Training Loss 0.6442\t Accuracy 0.8430\n",
      "Epoch [16][30]\t Batch [50][55]\t Training Loss 0.6684\t Accuracy 0.8260\n",
      "\n",
      "Epoch [16]\t Average training loss 0.6575\t Average training accuracy 0.8411\n",
      "Epoch [16]\t Average validation loss 0.5514\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [17][30]\t Batch [0][55]\t Training Loss 0.7067\t Accuracy 0.8140\n",
      "Epoch [17][30]\t Batch [50][55]\t Training Loss 0.6241\t Accuracy 0.8570\n",
      "\n",
      "Epoch [17]\t Average training loss 0.6435\t Average training accuracy 0.8432\n",
      "Epoch [17]\t Average validation loss 0.5380\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [18][30]\t Batch [0][55]\t Training Loss 0.6637\t Accuracy 0.8310\n",
      "Epoch [18][30]\t Batch [50][55]\t Training Loss 0.6457\t Accuracy 0.8440\n",
      "\n",
      "Epoch [18]\t Average training loss 0.6307\t Average training accuracy 0.8457\n",
      "Epoch [18]\t Average validation loss 0.5258\t Average validation accuracy 0.8892\n",
      "\n",
      "Epoch [19][30]\t Batch [0][55]\t Training Loss 0.6124\t Accuracy 0.8430\n",
      "Epoch [19][30]\t Batch [50][55]\t Training Loss 0.6048\t Accuracy 0.8510\n",
      "\n",
      "Epoch [19]\t Average training loss 0.6190\t Average training accuracy 0.8477\n",
      "Epoch [19]\t Average validation loss 0.5147\t Average validation accuracy 0.8896\n",
      "\n",
      "Epoch [20][30]\t Batch [0][55]\t Training Loss 0.6253\t Accuracy 0.8500\n",
      "Epoch [20][30]\t Batch [50][55]\t Training Loss 0.6182\t Accuracy 0.8470\n",
      "\n",
      "Epoch [20]\t Average training loss 0.6083\t Average training accuracy 0.8494\n",
      "Epoch [20]\t Average validation loss 0.5046\t Average validation accuracy 0.8916\n",
      "\n",
      "Epoch [21][30]\t Batch [0][55]\t Training Loss 0.5948\t Accuracy 0.8620\n",
      "Epoch [21][30]\t Batch [50][55]\t Training Loss 0.5879\t Accuracy 0.8510\n",
      "\n",
      "Epoch [21]\t Average training loss 0.5985\t Average training accuracy 0.8511\n",
      "Epoch [21]\t Average validation loss 0.4952\t Average validation accuracy 0.8926\n",
      "\n",
      "Epoch [22][30]\t Batch [0][55]\t Training Loss 0.5897\t Accuracy 0.8530\n",
      "Epoch [22][30]\t Batch [50][55]\t Training Loss 0.5695\t Accuracy 0.8600\n",
      "\n",
      "Epoch [22]\t Average training loss 0.5893\t Average training accuracy 0.8528\n",
      "Epoch [22]\t Average validation loss 0.4866\t Average validation accuracy 0.8936\n",
      "\n",
      "Epoch [23][30]\t Batch [0][55]\t Training Loss 0.5941\t Accuracy 0.8520\n",
      "Epoch [23][30]\t Batch [50][55]\t Training Loss 0.6243\t Accuracy 0.8440\n",
      "\n",
      "Epoch [23]\t Average training loss 0.5808\t Average training accuracy 0.8543\n",
      "Epoch [23]\t Average validation loss 0.4786\t Average validation accuracy 0.8954\n",
      "\n",
      "Epoch [24][30]\t Batch [0][55]\t Training Loss 0.5424\t Accuracy 0.8790\n",
      "Epoch [24][30]\t Batch [50][55]\t Training Loss 0.5576\t Accuracy 0.8630\n",
      "\n",
      "Epoch [24]\t Average training loss 0.5729\t Average training accuracy 0.8553\n",
      "Epoch [24]\t Average validation loss 0.4711\t Average validation accuracy 0.8968\n",
      "\n",
      "Epoch [25][30]\t Batch [0][55]\t Training Loss 0.5642\t Accuracy 0.8520\n",
      "Epoch [25][30]\t Batch [50][55]\t Training Loss 0.5688\t Accuracy 0.8530\n",
      "\n",
      "Epoch [25]\t Average training loss 0.5655\t Average training accuracy 0.8565\n",
      "Epoch [25]\t Average validation loss 0.4642\t Average validation accuracy 0.8974\n",
      "\n",
      "Epoch [26][30]\t Batch [0][55]\t Training Loss 0.5843\t Accuracy 0.8480\n",
      "Epoch [26][30]\t Batch [50][55]\t Training Loss 0.5538\t Accuracy 0.8480\n",
      "\n",
      "Epoch [26]\t Average training loss 0.5586\t Average training accuracy 0.8581\n",
      "Epoch [26]\t Average validation loss 0.4577\t Average validation accuracy 0.8984\n",
      "\n",
      "Epoch [27][30]\t Batch [0][55]\t Training Loss 0.5586\t Accuracy 0.8550\n",
      "Epoch [27][30]\t Batch [50][55]\t Training Loss 0.5235\t Accuracy 0.8810\n",
      "\n",
      "Epoch [27]\t Average training loss 0.5521\t Average training accuracy 0.8595\n",
      "Epoch [27]\t Average validation loss 0.4516\t Average validation accuracy 0.8984\n",
      "\n",
      "Epoch [28][30]\t Batch [0][55]\t Training Loss 0.5404\t Accuracy 0.8660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28][30]\t Batch [50][55]\t Training Loss 0.5275\t Accuracy 0.8610\n",
      "\n",
      "Epoch [28]\t Average training loss 0.5460\t Average training accuracy 0.8608\n",
      "Epoch [28]\t Average validation loss 0.4459\t Average validation accuracy 0.8988\n",
      "\n",
      "Epoch [29][30]\t Batch [0][55]\t Training Loss 0.5372\t Accuracy 0.8620\n",
      "Epoch [29][30]\t Batch [50][55]\t Training Loss 0.5301\t Accuracy 0.8650\n",
      "\n",
      "Epoch [29]\t Average training loss 0.5402\t Average training accuracy 0.8617\n",
      "Epoch [29]\t Average validation loss 0.4406\t Average validation accuracy 0.8990\n",
      "\n",
      "spend time: 43.563608169555664\n"
     ]
    }
   ],
   "source": [
    "# train without momentum\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 30,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "s = time.time()\n",
    "loss1, acc1 = runner.train()\n",
    "e = time.time()\n",
    "print(\"spend time:\",e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.8755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = runner.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][55]\t Training Loss 2.6052\t Accuracy 0.0780\n",
      "Epoch [0][30]\t Batch [50][55]\t Training Loss 0.8515\t Accuracy 0.8000\n",
      "\n",
      "Epoch [0]\t Average training loss 1.4608\t Average training accuracy 0.5670\n",
      "Epoch [0]\t Average validation loss 0.7183\t Average validation accuracy 0.8522\n",
      "\n",
      "Epoch [1][30]\t Batch [0][55]\t Training Loss 0.8038\t Accuracy 0.8110\n",
      "Epoch [1][30]\t Batch [50][55]\t Training Loss 0.6463\t Accuracy 0.8300\n",
      "\n",
      "Epoch [1]\t Average training loss 0.6846\t Average training accuracy 0.8320\n",
      "Epoch [1]\t Average validation loss 0.5115\t Average validation accuracy 0.8860\n",
      "\n",
      "Epoch [2][30]\t Batch [0][55]\t Training Loss 0.6136\t Accuracy 0.8420\n",
      "Epoch [2][30]\t Batch [50][55]\t Training Loss 0.5293\t Accuracy 0.8670\n",
      "\n",
      "Epoch [2]\t Average training loss 0.5655\t Average training accuracy 0.8543\n",
      "Epoch [2]\t Average validation loss 0.4387\t Average validation accuracy 0.8970\n",
      "\n",
      "Epoch [3][30]\t Batch [0][55]\t Training Loss 0.5618\t Accuracy 0.8500\n",
      "Epoch [3][30]\t Batch [50][55]\t Training Loss 0.4885\t Accuracy 0.8810\n",
      "\n",
      "Epoch [3]\t Average training loss 0.5104\t Average training accuracy 0.8664\n",
      "Epoch [3]\t Average validation loss 0.3991\t Average validation accuracy 0.9030\n",
      "\n",
      "Epoch [4][30]\t Batch [0][55]\t Training Loss 0.4986\t Accuracy 0.8650\n",
      "Epoch [4][30]\t Batch [50][55]\t Training Loss 0.4283\t Accuracy 0.8770\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4766\t Average training accuracy 0.8737\n",
      "Epoch [4]\t Average validation loss 0.3738\t Average validation accuracy 0.9072\n",
      "\n",
      "Epoch [5][30]\t Batch [0][55]\t Training Loss 0.4617\t Accuracy 0.8810\n",
      "Epoch [5][30]\t Batch [50][55]\t Training Loss 0.4178\t Accuracy 0.8750\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4532\t Average training accuracy 0.8786\n",
      "Epoch [5]\t Average validation loss 0.3559\t Average validation accuracy 0.9110\n",
      "\n",
      "Epoch [6][30]\t Batch [0][55]\t Training Loss 0.4486\t Accuracy 0.8850\n",
      "Epoch [6][30]\t Batch [50][55]\t Training Loss 0.3895\t Accuracy 0.9050\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4357\t Average training accuracy 0.8829\n",
      "Epoch [6]\t Average validation loss 0.3421\t Average validation accuracy 0.9122\n",
      "\n",
      "Epoch [7][30]\t Batch [0][55]\t Training Loss 0.4066\t Accuracy 0.8840\n",
      "Epoch [7][30]\t Batch [50][55]\t Training Loss 0.4056\t Accuracy 0.8870\n",
      "\n",
      "Epoch [7]\t Average training loss 0.4221\t Average training accuracy 0.8859\n",
      "Epoch [7]\t Average validation loss 0.3316\t Average validation accuracy 0.9134\n",
      "\n",
      "Epoch [8][30]\t Batch [0][55]\t Training Loss 0.4050\t Accuracy 0.8960\n",
      "Epoch [8][30]\t Batch [50][55]\t Training Loss 0.4107\t Accuracy 0.8900\n",
      "\n",
      "Epoch [8]\t Average training loss 0.4109\t Average training accuracy 0.8885\n",
      "Epoch [8]\t Average validation loss 0.3230\t Average validation accuracy 0.9162\n",
      "\n",
      "Epoch [9][30]\t Batch [0][55]\t Training Loss 0.3987\t Accuracy 0.8820\n",
      "Epoch [9][30]\t Batch [50][55]\t Training Loss 0.3677\t Accuracy 0.9020\n",
      "\n",
      "Epoch [9]\t Average training loss 0.4016\t Average training accuracy 0.8908\n",
      "Epoch [9]\t Average validation loss 0.3160\t Average validation accuracy 0.9172\n",
      "\n",
      "Epoch [10][30]\t Batch [0][55]\t Training Loss 0.4031\t Accuracy 0.8960\n",
      "Epoch [10][30]\t Batch [50][55]\t Training Loss 0.3822\t Accuracy 0.8980\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3938\t Average training accuracy 0.8923\n",
      "Epoch [10]\t Average validation loss 0.3099\t Average validation accuracy 0.9190\n",
      "\n",
      "Epoch [11][30]\t Batch [0][55]\t Training Loss 0.3857\t Accuracy 0.8920\n",
      "Epoch [11][30]\t Batch [50][55]\t Training Loss 0.4102\t Accuracy 0.8820\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3869\t Average training accuracy 0.8938\n",
      "Epoch [11]\t Average validation loss 0.3047\t Average validation accuracy 0.9210\n",
      "\n",
      "Epoch [12][30]\t Batch [0][55]\t Training Loss 0.4360\t Accuracy 0.8860\n",
      "Epoch [12][30]\t Batch [50][55]\t Training Loss 0.3877\t Accuracy 0.8920\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3809\t Average training accuracy 0.8950\n",
      "Epoch [12]\t Average validation loss 0.3002\t Average validation accuracy 0.9220\n",
      "\n",
      "Epoch [13][30]\t Batch [0][55]\t Training Loss 0.3416\t Accuracy 0.9080\n",
      "Epoch [13][30]\t Batch [50][55]\t Training Loss 0.3377\t Accuracy 0.8980\n",
      "\n",
      "Epoch [13]\t Average training loss 0.3756\t Average training accuracy 0.8961\n",
      "Epoch [13]\t Average validation loss 0.2961\t Average validation accuracy 0.9220\n",
      "\n",
      "Epoch [14][30]\t Batch [0][55]\t Training Loss 0.3783\t Accuracy 0.9030\n",
      "Epoch [14][30]\t Batch [50][55]\t Training Loss 0.3707\t Accuracy 0.8910\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3708\t Average training accuracy 0.8971\n",
      "Epoch [14]\t Average validation loss 0.2926\t Average validation accuracy 0.9224\n",
      "\n",
      "Epoch [15][30]\t Batch [0][55]\t Training Loss 0.3624\t Accuracy 0.9030\n",
      "Epoch [15][30]\t Batch [50][55]\t Training Loss 0.3671\t Accuracy 0.9080\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3665\t Average training accuracy 0.8984\n",
      "Epoch [15]\t Average validation loss 0.2894\t Average validation accuracy 0.9230\n",
      "\n",
      "Epoch [16][30]\t Batch [0][55]\t Training Loss 0.3378\t Accuracy 0.9030\n",
      "Epoch [16][30]\t Batch [50][55]\t Training Loss 0.3489\t Accuracy 0.8940\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3627\t Average training accuracy 0.8992\n",
      "Epoch [16]\t Average validation loss 0.2867\t Average validation accuracy 0.9248\n",
      "\n",
      "Epoch [17][30]\t Batch [0][55]\t Training Loss 0.3330\t Accuracy 0.9010\n",
      "Epoch [17][30]\t Batch [50][55]\t Training Loss 0.3864\t Accuracy 0.9020\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3592\t Average training accuracy 0.9002\n",
      "Epoch [17]\t Average validation loss 0.2837\t Average validation accuracy 0.9248\n",
      "\n",
      "Epoch [18][30]\t Batch [0][55]\t Training Loss 0.3731\t Accuracy 0.9030\n",
      "Epoch [18][30]\t Batch [50][55]\t Training Loss 0.3163\t Accuracy 0.9000\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3559\t Average training accuracy 0.9010\n",
      "Epoch [18]\t Average validation loss 0.2817\t Average validation accuracy 0.9246\n",
      "\n",
      "Epoch [19][30]\t Batch [0][55]\t Training Loss 0.3318\t Accuracy 0.9020\n",
      "Epoch [19][30]\t Batch [50][55]\t Training Loss 0.3604\t Accuracy 0.8980\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3529\t Average training accuracy 0.9015\n",
      "Epoch [19]\t Average validation loss 0.2795\t Average validation accuracy 0.9262\n",
      "\n",
      "Epoch [20][30]\t Batch [0][55]\t Training Loss 0.4119\t Accuracy 0.8920\n",
      "Epoch [20][30]\t Batch [50][55]\t Training Loss 0.3280\t Accuracy 0.9080\n",
      "\n",
      "Epoch [20]\t Average training loss 0.3501\t Average training accuracy 0.9023\n",
      "Epoch [20]\t Average validation loss 0.2772\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [21][30]\t Batch [0][55]\t Training Loss 0.3514\t Accuracy 0.8940\n",
      "Epoch [21][30]\t Batch [50][55]\t Training Loss 0.3815\t Accuracy 0.8920\n",
      "\n",
      "Epoch [21]\t Average training loss 0.3477\t Average training accuracy 0.9031\n",
      "Epoch [21]\t Average validation loss 0.2753\t Average validation accuracy 0.9274\n",
      "\n",
      "Epoch [22][30]\t Batch [0][55]\t Training Loss 0.3486\t Accuracy 0.9030\n",
      "Epoch [22][30]\t Batch [50][55]\t Training Loss 0.3196\t Accuracy 0.9060\n",
      "\n",
      "Epoch [22]\t Average training loss 0.3451\t Average training accuracy 0.9035\n",
      "Epoch [22]\t Average validation loss 0.2736\t Average validation accuracy 0.9272\n",
      "\n",
      "Epoch [23][30]\t Batch [0][55]\t Training Loss 0.3557\t Accuracy 0.8940\n",
      "Epoch [23][30]\t Batch [50][55]\t Training Loss 0.3215\t Accuracy 0.9120\n",
      "\n",
      "Epoch [23]\t Average training loss 0.3430\t Average training accuracy 0.9039\n",
      "Epoch [23]\t Average validation loss 0.2721\t Average validation accuracy 0.9278\n",
      "\n",
      "Epoch [24][30]\t Batch [0][55]\t Training Loss 0.3599\t Accuracy 0.9030\n",
      "Epoch [24][30]\t Batch [50][55]\t Training Loss 0.3412\t Accuracy 0.9000\n",
      "\n",
      "Epoch [24]\t Average training loss 0.3408\t Average training accuracy 0.9046\n",
      "Epoch [24]\t Average validation loss 0.2704\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [25][30]\t Batch [0][55]\t Training Loss 0.3433\t Accuracy 0.8990\n",
      "Epoch [25][30]\t Batch [50][55]\t Training Loss 0.3026\t Accuracy 0.9240\n",
      "\n",
      "Epoch [25]\t Average training loss 0.3388\t Average training accuracy 0.9052\n",
      "Epoch [25]\t Average validation loss 0.2690\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [26][30]\t Batch [0][55]\t Training Loss 0.3401\t Accuracy 0.8870\n",
      "Epoch [26][30]\t Batch [50][55]\t Training Loss 0.3741\t Accuracy 0.8990\n",
      "\n",
      "Epoch [26]\t Average training loss 0.3369\t Average training accuracy 0.9058\n",
      "Epoch [26]\t Average validation loss 0.2678\t Average validation accuracy 0.9276\n",
      "\n",
      "Epoch [27][30]\t Batch [0][55]\t Training Loss 0.3490\t Accuracy 0.9070\n",
      "Epoch [27][30]\t Batch [50][55]\t Training Loss 0.2862\t Accuracy 0.9110\n",
      "\n",
      "Epoch [27]\t Average training loss 0.3351\t Average training accuracy 0.9060\n",
      "Epoch [27]\t Average validation loss 0.2665\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [28][30]\t Batch [0][55]\t Training Loss 0.3184\t Accuracy 0.9070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28][30]\t Batch [50][55]\t Training Loss 0.3774\t Accuracy 0.9070\n",
      "\n",
      "Epoch [28]\t Average training loss 0.3334\t Average training accuracy 0.9067\n",
      "Epoch [28]\t Average validation loss 0.2651\t Average validation accuracy 0.9278\n",
      "\n",
      "Epoch [29][30]\t Batch [0][55]\t Training Loss 0.3645\t Accuracy 0.8910\n",
      "Epoch [29][30]\t Batch [50][55]\t Training Loss 0.3059\t Accuracy 0.9190\n",
      "\n",
      "Epoch [29]\t Average training loss 0.3318\t Average training accuracy 0.9072\n",
      "Epoch [29]\t Average validation loss 0.2640\t Average validation accuracy 0.9274\n",
      "\n",
      "spend time: 44.06272029876709\n"
     ]
    }
   ],
   "source": [
    "# train with momentum\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 30,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "s = time.time()\n",
    "loss2, acc2 = runner.train()\n",
    "e = time.time()\n",
    "print(\"spend time:\",e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.9138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = runner.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({\n",
    "    \"momentum=0\": [loss1, acc1],\n",
    "    \"momentum=0.9\": [loss2, acc2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.4316\t Accuracy 0.0700\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.7891\t Accuracy 0.8300\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.6570\t Accuracy 0.8500\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.6203\t Accuracy 0.8300\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.4487\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.5462\t Accuracy 0.8500\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.4563\t Accuracy 0.8800\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.5322\t Accuracy 0.8300\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.4751\t Accuracy 0.8800\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.2835\t Accuracy 0.9500\n",
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.3756\t Accuracy 0.9600\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5810\t Average training accuracy 0.8408\n",
      "Epoch [0]\t Average validation loss 0.3173\t Average validation accuracy 0.9162\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.3392\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.4949\t Accuracy 0.8600\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.2582\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.4123\t Accuracy 0.8700\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.4989\t Accuracy 0.8500\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.4229\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.2376\t Accuracy 0.9400\n",
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.3849\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.3124\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.3809\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.3802\t Accuracy 0.8700\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3742\t Average training accuracy 0.8965\n",
      "Epoch [1]\t Average validation loss 0.2791\t Average validation accuracy 0.9254\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.2062\t Accuracy 0.9700\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.3077\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.4111\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.2665\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.4414\t Accuracy 0.8700\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.2942\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.3543\t Accuracy 0.8700\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.3486\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.4358\t Accuracy 0.8700\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.3480\t Accuracy 0.9100\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.3234\t Accuracy 0.9300\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3437\t Average training accuracy 0.9035\n",
      "Epoch [2]\t Average validation loss 0.2641\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.2864\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.4520\t Accuracy 0.8800\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.4152\t Accuracy 0.9000\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.3242\t Accuracy 0.9000\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.4170\t Accuracy 0.8900\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.3566\t Accuracy 0.8800\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.3063\t Accuracy 0.8800\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.3565\t Accuracy 0.9000\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.2856\t Accuracy 0.8900\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.4040\t Accuracy 0.8700\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.2751\t Accuracy 0.9300\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3280\t Average training accuracy 0.9081\n",
      "Epoch [3]\t Average validation loss 0.2558\t Average validation accuracy 0.9300\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.2762\t Accuracy 0.8900\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.2666\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.2343\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.2601\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.3533\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.2025\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.4417\t Accuracy 0.8800\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.2785\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.2814\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.3975\t Accuracy 0.8600\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.5363\t Accuracy 0.8600\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3175\t Average training accuracy 0.9112\n",
      "Epoch [4]\t Average validation loss 0.2494\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.2521\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.3801\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.1981\t Accuracy 0.9400\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.2124\t Accuracy 0.9400\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.2832\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.2621\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.3696\t Accuracy 0.8800\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.3983\t Accuracy 0.8800\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.2829\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.4573\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.2522\t Accuracy 0.9400\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3102\t Average training accuracy 0.9131\n",
      "Epoch [5]\t Average validation loss 0.2446\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.3454\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.3565\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.3041\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.4129\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.3253\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.4711\t Accuracy 0.8600\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.4272\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.2633\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.3567\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.2401\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.3140\t Accuracy 0.9500\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3046\t Average training accuracy 0.9148\n",
      "Epoch [6]\t Average validation loss 0.2416\t Average validation accuracy 0.9322\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.4393\t Accuracy 0.8800\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.2957\t Accuracy 0.8900\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.2609\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.2838\t Accuracy 0.8800\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.3748\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.3136\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.2136\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.3182\t Accuracy 0.8900\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.2318\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.4026\t Accuracy 0.8800\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.2693\t Accuracy 0.9300\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3000\t Average training accuracy 0.9160\n",
      "Epoch [7]\t Average validation loss 0.2394\t Average validation accuracy 0.9332\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.2000\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.2651\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.4555\t Accuracy 0.8800\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.4257\t Accuracy 0.8800\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.2034\t Accuracy 0.9400\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.3030\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.4946\t Accuracy 0.8600\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.1995\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.3475\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.3802\t Accuracy 0.8900\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.1707\t Accuracy 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.2963\t Average training accuracy 0.9174\n",
      "Epoch [8]\t Average validation loss 0.2364\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.2796\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.3733\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.3772\t Accuracy 0.8600\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.3001\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.2243\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.1999\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.2079\t Accuracy 0.9500\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.2736\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.1732\t Accuracy 0.9700\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.3383\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.2212\t Accuracy 0.9300\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2928\t Average training accuracy 0.9175\n",
      "Epoch [9]\t Average validation loss 0.2350\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.2578\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.2496\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.2519\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.3090\t Accuracy 0.8700\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.3791\t Accuracy 0.8800\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.4152\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.3247\t Accuracy 0.8800\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.1942\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.2474\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.2237\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.2937\t Accuracy 0.8900\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2900\t Average training accuracy 0.9188\n",
      "Epoch [10]\t Average validation loss 0.2332\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.2733\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.2348\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.1977\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.3428\t Accuracy 0.8800\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.3103\t Accuracy 0.9600\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.3491\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.3550\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.1572\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.2657\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.4176\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.3026\t Accuracy 0.8900\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2880\t Average training accuracy 0.9199\n",
      "Epoch [11]\t Average validation loss 0.2312\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.4787\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.3529\t Accuracy 0.8800\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.2153\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.2090\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.4344\t Accuracy 0.8800\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.3106\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.2549\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.2520\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.3434\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.2188\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.3237\t Accuracy 0.9100\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2858\t Average training accuracy 0.9203\n",
      "Epoch [12]\t Average validation loss 0.2303\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.3844\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.2162\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.1920\t Accuracy 0.9500\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.2787\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.1855\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.2774\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.4024\t Accuracy 0.8900\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.2660\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.2059\t Accuracy 0.9500\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.2825\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.3118\t Accuracy 0.9200\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2840\t Average training accuracy 0.9208\n",
      "Epoch [13]\t Average validation loss 0.2291\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.2325\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.3099\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.3039\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.4058\t Accuracy 0.8700\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.2500\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.1913\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.1990\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.2188\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.3398\t Accuracy 0.8900\n",
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.4057\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.2075\t Accuracy 0.9500\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2821\t Average training accuracy 0.9216\n",
      "Epoch [14]\t Average validation loss 0.2287\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.3488\t Accuracy 0.9200\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.4119\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.3961\t Accuracy 0.8800\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.3222\t Accuracy 0.8900\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.2408\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.2255\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.2785\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.4017\t Accuracy 0.8900\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.2137\t Accuracy 0.9200\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.1786\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.2967\t Accuracy 0.9000\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2806\t Average training accuracy 0.9217\n",
      "Epoch [15]\t Average validation loss 0.2301\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.2800\t Accuracy 0.9000\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.2841\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.2924\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.4081\t Accuracy 0.8600\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.1579\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.4194\t Accuracy 0.8500\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.4856\t Accuracy 0.8800\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.3735\t Accuracy 0.8900\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.1934\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.1983\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.6771\t Accuracy 0.8800\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2791\t Average training accuracy 0.9218\n",
      "Epoch [16]\t Average validation loss 0.2269\t Average validation accuracy 0.9368\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.2223\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.2152\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.2269\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.1886\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.1396\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.3197\t Accuracy 0.8800\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.2203\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.1830\t Accuracy 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.2007\t Accuracy 0.8800\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.1915\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.2946\t Accuracy 0.9300\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2780\t Average training accuracy 0.9226\n",
      "Epoch [17]\t Average validation loss 0.2270\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.1796\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.2113\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.3318\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.2867\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.1851\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.2051\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.2239\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.2490\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.2792\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.2589\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.3394\t Accuracy 0.8800\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2766\t Average training accuracy 0.9226\n",
      "Epoch [18]\t Average validation loss 0.2258\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.2081\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.1683\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.1853\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.1882\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.4535\t Accuracy 0.8700\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.1949\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.2197\t Accuracy 0.9600\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.3026\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.2041\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.3803\t Accuracy 0.9000\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.3738\t Accuracy 0.9000\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2753\t Average training accuracy 0.9232\n",
      "Epoch [19]\t Average validation loss 0.2267\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.3106\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.1867\t Accuracy 0.9600\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.2555\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.1931\t Accuracy 0.9600\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.2808\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.2980\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.3125\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.3043\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.2504\t Accuracy 0.9500\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.2396\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.1753\t Accuracy 0.9500\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2742\t Average training accuracy 0.9232\n",
      "Epoch [20]\t Average validation loss 0.2258\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.3298\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.3841\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.1451\t Accuracy 0.9600\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.4703\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.3211\t Accuracy 0.8700\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.1600\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.3611\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.2432\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.2831\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.3564\t Accuracy 0.8900\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.2337\t Accuracy 0.9300\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2734\t Average training accuracy 0.9235\n",
      "Epoch [21]\t Average validation loss 0.2248\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.2120\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.2591\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.2058\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.2448\t Accuracy 0.9600\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.1851\t Accuracy 0.9600\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.2293\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.3968\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.1852\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.2483\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.2342\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.2808\t Accuracy 0.9500\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2724\t Average training accuracy 0.9242\n",
      "Epoch [22]\t Average validation loss 0.2237\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.2418\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.3458\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.2094\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.3782\t Accuracy 0.8700\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.3992\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.3664\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.2780\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.2151\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.1202\t Accuracy 0.9800\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.2963\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.2553\t Accuracy 0.9300\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2717\t Average training accuracy 0.9243\n",
      "Epoch [23]\t Average validation loss 0.2239\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.1804\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.2858\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.2119\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.2955\t Accuracy 0.8900\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.1833\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.1653\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.3859\t Accuracy 0.8800\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.2819\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.4856\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.2334\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.2404\t Accuracy 0.9100\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2709\t Average training accuracy 0.9249\n",
      "Epoch [24]\t Average validation loss 0.2231\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.2632\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.2408\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.3582\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.2940\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.2447\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.3150\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.1538\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.2718\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.3886\t Accuracy 0.8700\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.2349\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.2947\t Accuracy 0.9100\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2700\t Average training accuracy 0.9245\n",
      "Epoch [25]\t Average validation loss 0.2219\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.3273\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.3198\t Accuracy 0.8900\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.1544\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.1549\t Accuracy 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.3092\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.2698\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.1097\t Accuracy 0.9800\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.2839\t Accuracy 0.9000\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.3954\t Accuracy 0.8800\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.2285\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.3022\t Accuracy 0.9200\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2694\t Average training accuracy 0.9249\n",
      "Epoch [26]\t Average validation loss 0.2220\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.1616\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.3017\t Accuracy 0.9000\n",
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.3204\t Accuracy 0.8900\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.4070\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.1852\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.3094\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.1717\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.4117\t Accuracy 0.8900\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.3918\t Accuracy 0.8900\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.3959\t Accuracy 0.8900\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.2144\t Accuracy 0.9300\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2685\t Average training accuracy 0.9253\n",
      "Epoch [27]\t Average validation loss 0.2225\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.3657\t Accuracy 0.8700\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.3426\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.1848\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.2589\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.1726\t Accuracy 0.9600\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.2704\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.2705\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.4549\t Accuracy 0.8900\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.3062\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.2829\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.2957\t Accuracy 0.9300\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2678\t Average training accuracy 0.9256\n",
      "Epoch [28]\t Average validation loss 0.2225\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.2922\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.2678\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.2930\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.4183\t Accuracy 0.8900\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.2971\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.2043\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.2066\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.3016\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.0829\t Accuracy 0.9800\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.3943\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.1732\t Accuracy 0.9500\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2672\t Average training accuracy 0.9253\n",
      "Epoch [29]\t Average validation loss 0.2227\t Average validation accuracy 0.9398\n",
      "\n",
      "spend time: 42.136417865753174\n",
      "Final test accuracy 0.9223\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.3172\t Accuracy 0.1800\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.4621\t Accuracy 0.8600\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.3496\t Accuracy 0.9300\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.3236\t Accuracy 0.9200\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.4468\t Accuracy 0.8900\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.3089\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.6170\t Accuracy 0.8500\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.3479\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.2777\t Accuracy 0.9400\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.3378\t Accuracy 0.9200\n",
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.3753\t Accuracy 0.9000\n",
      "\n",
      "Epoch [0]\t Average training loss 0.4445\t Average training accuracy 0.8747\n",
      "Epoch [0]\t Average validation loss 0.2627\t Average validation accuracy 0.9294\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.3321\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.1787\t Accuracy 0.9700\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.3016\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.3974\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.2398\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.3200\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.2671\t Accuracy 0.9400\n",
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.3643\t Accuracy 0.8900\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.2931\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.3431\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.3301\t Accuracy 0.9100\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3229\t Average training accuracy 0.9089\n",
      "Epoch [1]\t Average validation loss 0.2504\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.1639\t Accuracy 0.9700\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.3429\t Accuracy 0.8700\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.3901\t Accuracy 0.9100\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.2647\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.3040\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.2057\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.5004\t Accuracy 0.8500\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.3785\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.2625\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.2865\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.2563\t Accuracy 0.9400\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3053\t Average training accuracy 0.9143\n",
      "Epoch [2]\t Average validation loss 0.2375\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.1902\t Accuracy 0.9600\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.1707\t Accuracy 0.9700\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.1864\t Accuracy 0.9600\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.2091\t Accuracy 0.9500\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.4568\t Accuracy 0.8800\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.1791\t Accuracy 0.9500\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.2543\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.4664\t Accuracy 0.8600\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.2466\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.4305\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.1913\t Accuracy 0.9600\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2954\t Average training accuracy 0.9174\n",
      "Epoch [3]\t Average validation loss 0.2364\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.2807\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.2346\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.2785\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.1534\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.1755\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.4156\t Accuracy 0.8800\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.2126\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.2956\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.1994\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.3470\t Accuracy 0.8700\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.4768\t Accuracy 0.8900\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2891\t Average training accuracy 0.9192\n",
      "Epoch [4]\t Average validation loss 0.2292\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.2455\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.1548\t Accuracy 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.5012\t Accuracy 0.8500\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.3561\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.2192\t Accuracy 0.9400\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.3768\t Accuracy 0.8700\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.3465\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.2552\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.2511\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.2516\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.4934\t Accuracy 0.8400\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2842\t Average training accuracy 0.9209\n",
      "Epoch [5]\t Average validation loss 0.2287\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.1688\t Accuracy 0.9800\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.2642\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.3147\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.3708\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.2882\t Accuracy 0.9500\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.1782\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.3194\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.3232\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.2779\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.2325\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.2312\t Accuracy 0.9600\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2808\t Average training accuracy 0.9213\n",
      "Epoch [6]\t Average validation loss 0.2267\t Average validation accuracy 0.9370\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.2094\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.2128\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.5391\t Accuracy 0.8200\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.2163\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.2080\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.3417\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.2845\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.1930\t Accuracy 0.9500\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.4154\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.3862\t Accuracy 0.8800\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.3225\t Accuracy 0.9500\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2776\t Average training accuracy 0.9226\n",
      "Epoch [7]\t Average validation loss 0.2245\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.2984\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.1636\t Accuracy 0.9800\n",
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.3319\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.3027\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.1955\t Accuracy 0.9400\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.4124\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.3589\t Accuracy 0.9700\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.2140\t Accuracy 0.9200\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.2078\t Accuracy 0.9700\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.2872\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.1325\t Accuracy 0.9800\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2749\t Average training accuracy 0.9232\n",
      "Epoch [8]\t Average validation loss 0.2235\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.3183\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.1516\t Accuracy 0.9700\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.1421\t Accuracy 0.9700\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.2806\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.1102\t Accuracy 0.9800\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.3251\t Accuracy 0.8800\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.3455\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.3570\t Accuracy 0.8800\n",
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.0994\t Accuracy 0.9800\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.3273\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.2137\t Accuracy 0.9100\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2733\t Average training accuracy 0.9234\n",
      "Epoch [9]\t Average validation loss 0.2251\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.2177\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.3109\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.2708\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.2673\t Accuracy 0.9600\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.2872\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.2370\t Accuracy 0.9500\n",
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.3535\t Accuracy 0.8800\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.3127\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.1547\t Accuracy 0.9700\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.1850\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.2647\t Accuracy 0.9300\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2715\t Average training accuracy 0.9241\n",
      "Epoch [10]\t Average validation loss 0.2225\t Average validation accuracy 0.9410\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.1922\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.3761\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.4311\t Accuracy 0.8800\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.3848\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.2414\t Accuracy 0.9600\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.4430\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.3629\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.3373\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.2018\t Accuracy 0.9600\n",
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.2448\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.2387\t Accuracy 0.9300\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2696\t Average training accuracy 0.9248\n",
      "Epoch [11]\t Average validation loss 0.2237\t Average validation accuracy 0.9380\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.1480\t Accuracy 0.9500\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.1547\t Accuracy 0.9700\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.1937\t Accuracy 0.9500\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.2565\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.1901\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.3115\t Accuracy 0.8800\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.1676\t Accuracy 0.9600\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.2760\t Accuracy 0.9000\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.3422\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.2027\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.4281\t Accuracy 0.9300\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2680\t Average training accuracy 0.9253\n",
      "Epoch [12]\t Average validation loss 0.2251\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.4475\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.2213\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.2208\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.3641\t Accuracy 0.8600\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.2833\t Accuracy 0.8800\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.1671\t Accuracy 0.9900\n",
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.1324\t Accuracy 0.9600\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.3987\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.2711\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.2664\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.1740\t Accuracy 0.9500\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2670\t Average training accuracy 0.9251\n",
      "Epoch [13]\t Average validation loss 0.2222\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.2266\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.3417\t Accuracy 0.8800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.2849\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.2239\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.3250\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.2745\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.2708\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.1581\t Accuracy 0.9600\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.3704\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.3096\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.3637\t Accuracy 0.9100\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2650\t Average training accuracy 0.9259\n",
      "Epoch [14]\t Average validation loss 0.2226\t Average validation accuracy 0.9410\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.4159\t Accuracy 0.8800\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.3515\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.2651\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.3341\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.3375\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.1871\t Accuracy 0.9600\n",
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.2849\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.2657\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.4136\t Accuracy 0.9200\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.2256\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.2036\t Accuracy 0.9500\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2641\t Average training accuracy 0.9263\n",
      "Epoch [15]\t Average validation loss 0.2237\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.3511\t Accuracy 0.8700\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.2925\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.3107\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.1848\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.5302\t Accuracy 0.8800\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.1982\t Accuracy 0.9600\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.2107\t Accuracy 0.9600\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.3503\t Accuracy 0.8800\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.2481\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.2851\t Accuracy 0.8900\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.3152\t Accuracy 0.9200\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2631\t Average training accuracy 0.9266\n",
      "Epoch [16]\t Average validation loss 0.2221\t Average validation accuracy 0.9420\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.3449\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.2019\t Accuracy 0.9600\n",
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.5143\t Accuracy 0.9000\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.4232\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.1357\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.2050\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.3928\t Accuracy 0.8900\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.3281\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.5575\t Accuracy 0.8300\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.1607\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.3307\t Accuracy 0.9200\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2625\t Average training accuracy 0.9269\n",
      "Epoch [17]\t Average validation loss 0.2215\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.2297\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.2837\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.2395\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.3228\t Accuracy 0.9000\n",
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.2939\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.3111\t Accuracy 0.9000\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.1334\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.2608\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.1647\t Accuracy 0.9800\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.2733\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.1944\t Accuracy 0.9300\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2612\t Average training accuracy 0.9269\n",
      "Epoch [18]\t Average validation loss 0.2205\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.2626\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.2507\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.2364\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.1828\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.1964\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.2132\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.1629\t Accuracy 0.9600\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.2388\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.3747\t Accuracy 0.8700\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.3613\t Accuracy 0.8500\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.1353\t Accuracy 0.9500\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2601\t Average training accuracy 0.9274\n",
      "Epoch [19]\t Average validation loss 0.2182\t Average validation accuracy 0.9412\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.2785\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.2496\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.2545\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.1669\t Accuracy 0.9600\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.1011\t Accuracy 0.9700\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.1781\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.2650\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.1637\t Accuracy 0.9500\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.2431\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.3203\t Accuracy 0.9000\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.5246\t Accuracy 0.8700\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2597\t Average training accuracy 0.9282\n",
      "Epoch [20]\t Average validation loss 0.2215\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.1165\t Accuracy 0.9500\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.2196\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.1619\t Accuracy 0.9800\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.2378\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.1821\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.2611\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.2787\t Accuracy 0.9600\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.3157\t Accuracy 0.8700\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.5107\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.1777\t Accuracy 0.9700\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.2958\t Accuracy 0.9600\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2588\t Average training accuracy 0.9282\n",
      "Epoch [21]\t Average validation loss 0.2202\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.2324\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.4246\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.3395\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.3142\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.2780\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.2932\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.3206\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.4650\t Accuracy 0.8800\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.3061\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.1842\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.1892\t Accuracy 0.9300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [22]\t Average training loss 0.2584\t Average training accuracy 0.9283\n",
      "Epoch [22]\t Average validation loss 0.2200\t Average validation accuracy 0.9432\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.2708\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.2817\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.2433\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.2501\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.2352\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.4885\t Accuracy 0.8900\n",
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.3127\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.0936\t Accuracy 0.9900\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.1665\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.1913\t Accuracy 0.9600\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.1509\t Accuracy 0.9600\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2574\t Average training accuracy 0.9288\n",
      "Epoch [23]\t Average validation loss 0.2219\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.1265\t Accuracy 0.9600\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.3926\t Accuracy 0.9000\n",
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.3104\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.1730\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.3251\t Accuracy 0.9100\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.0792\t Accuracy 0.9900\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.3453\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.4521\t Accuracy 0.9100\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.1762\t Accuracy 0.9700\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.1667\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.2237\t Accuracy 0.9500\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2570\t Average training accuracy 0.9285\n",
      "Epoch [24]\t Average validation loss 0.2190\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.3787\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.3023\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.2897\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.1808\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.1798\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.1579\t Accuracy 0.9700\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.1662\t Accuracy 0.9600\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.1764\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.2023\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.2651\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.2755\t Accuracy 0.9100\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2560\t Average training accuracy 0.9288\n",
      "Epoch [25]\t Average validation loss 0.2183\t Average validation accuracy 0.9422\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.2401\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.1669\t Accuracy 0.9600\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.2323\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.1367\t Accuracy 0.9700\n",
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.2538\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.4081\t Accuracy 0.9100\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.1970\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.1421\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.1541\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.3946\t Accuracy 0.9000\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.2013\t Accuracy 0.9400\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2557\t Average training accuracy 0.9285\n",
      "Epoch [26]\t Average validation loss 0.2206\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.3896\t Accuracy 0.8900\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.2578\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.3647\t Accuracy 0.8800\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.3206\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.2927\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.3969\t Accuracy 0.8900\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.3577\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.1548\t Accuracy 0.9600\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.2167\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.2443\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.2157\t Accuracy 0.9100\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2548\t Average training accuracy 0.9291\n",
      "Epoch [27]\t Average validation loss 0.2178\t Average validation accuracy 0.9430\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.2274\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.2751\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.1793\t Accuracy 0.9700\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.2008\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.2287\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.3678\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.2309\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.1999\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.1945\t Accuracy 0.9600\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.1765\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.2905\t Accuracy 0.9400\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2546\t Average training accuracy 0.9295\n",
      "Epoch [28]\t Average validation loss 0.2194\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.3604\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.3951\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.2759\t Accuracy 0.8900\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.2908\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.2480\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.2887\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.3893\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.1045\t Accuracy 0.9900\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.2218\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.3253\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.1399\t Accuracy 0.9500\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2541\t Average training accuracy 0.9290\n",
      "Epoch [29]\t Average validation loss 0.2246\t Average validation accuracy 0.9396\n",
      "\n",
      "spend time: 42.57029104232788\n",
      "Final test accuracy 0.9251\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.5216\t Accuracy 0.1100\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.3865\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.5633\t Accuracy 0.8600\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.2991\t Accuracy 0.9300\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.2097\t Accuracy 0.9500\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.3756\t Accuracy 0.8600\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.4057\t Accuracy 0.8800\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.4297\t Accuracy 0.8900\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.4039\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.3214\t Accuracy 0.8800\n",
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.3918\t Accuracy 0.9000\n",
      "\n",
      "Epoch [0]\t Average training loss 0.4105\t Average training accuracy 0.8805\n",
      "Epoch [0]\t Average validation loss 0.2618\t Average validation accuracy 0.9260\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.4828\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.1701\t Accuracy 0.9500\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.2905\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.2826\t Accuracy 0.9400\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.3953\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.3524\t Accuracy 0.8600\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.4825\t Accuracy 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.2209\t Accuracy 0.9600\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.2520\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.1852\t Accuracy 0.9600\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.2095\t Accuracy 0.9500\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3129\t Average training accuracy 0.9126\n",
      "Epoch [1]\t Average validation loss 0.2390\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.2024\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.4702\t Accuracy 0.8800\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.3609\t Accuracy 0.8800\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.2633\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.1715\t Accuracy 0.9500\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.3577\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.3086\t Accuracy 0.9100\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.2519\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.2964\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.3053\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.2183\t Accuracy 0.9300\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2966\t Average training accuracy 0.9161\n",
      "Epoch [2]\t Average validation loss 0.2350\t Average validation accuracy 0.9352\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.4216\t Accuracy 0.8500\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.1387\t Accuracy 0.9600\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.3496\t Accuracy 0.8800\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.4185\t Accuracy 0.8600\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.2903\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.4506\t Accuracy 0.8800\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.4497\t Accuracy 0.8700\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.2575\t Accuracy 0.9400\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.3659\t Accuracy 0.8900\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.3803\t Accuracy 0.9500\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.2530\t Accuracy 0.9300\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2891\t Average training accuracy 0.9192\n",
      "Epoch [3]\t Average validation loss 0.2331\t Average validation accuracy 0.9322\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.2357\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.2094\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.1522\t Accuracy 0.9800\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.1875\t Accuracy 0.9600\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.1499\t Accuracy 0.9600\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.3163\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.2731\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.2159\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.2879\t Accuracy 0.8800\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.2936\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.3436\t Accuracy 0.9100\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2836\t Average training accuracy 0.9210\n",
      "Epoch [4]\t Average validation loss 0.2292\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.2534\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.2942\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.1886\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.3137\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.3611\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.1842\t Accuracy 0.9600\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.4351\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.2377\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.2691\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.2389\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.1658\t Accuracy 0.9600\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2790\t Average training accuracy 0.9223\n",
      "Epoch [5]\t Average validation loss 0.2264\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.1827\t Accuracy 0.9700\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.2905\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.1756\t Accuracy 0.9500\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.3083\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.2730\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.3519\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.2692\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.4138\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.3408\t Accuracy 0.8800\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.2295\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.3504\t Accuracy 0.9000\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2762\t Average training accuracy 0.9233\n",
      "Epoch [6]\t Average validation loss 0.2268\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.3404\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.2965\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.2307\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.4420\t Accuracy 0.8600\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.1789\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.2302\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.1809\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.2571\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.3721\t Accuracy 0.8900\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.2881\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.3078\t Accuracy 0.9300\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2726\t Average training accuracy 0.9239\n",
      "Epoch [7]\t Average validation loss 0.2262\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.2900\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.2936\t Accuracy 0.9200\n",
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.3231\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.2752\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.3045\t Accuracy 0.9400\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.2448\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.3353\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.2478\t Accuracy 0.9200\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.1969\t Accuracy 0.9600\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.3451\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.2649\t Accuracy 0.9200\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2711\t Average training accuracy 0.9242\n",
      "Epoch [8]\t Average validation loss 0.2235\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.1767\t Accuracy 0.9600\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.2023\t Accuracy 0.9700\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.1650\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.2268\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.1593\t Accuracy 0.9600\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.1892\t Accuracy 0.9600\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.4217\t Accuracy 0.8700\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.3903\t Accuracy 0.8800\n",
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.3994\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.3231\t Accuracy 0.8700\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.2486\t Accuracy 0.9200\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2692\t Average training accuracy 0.9249\n",
      "Epoch [9]\t Average validation loss 0.2236\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.2061\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.3440\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.3199\t Accuracy 0.8800\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.3799\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.1566\t Accuracy 0.9500\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.3329\t Accuracy 0.9100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.1679\t Accuracy 0.9700\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.2836\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.1977\t Accuracy 0.9600\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.3552\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.2963\t Accuracy 0.9300\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2675\t Average training accuracy 0.9254\n",
      "Epoch [10]\t Average validation loss 0.2223\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.2979\t Accuracy 0.8900\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.3295\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.3010\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.1954\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.2012\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.1950\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.2827\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.3108\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.1972\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.1443\t Accuracy 0.9600\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.1945\t Accuracy 0.9300\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2655\t Average training accuracy 0.9258\n",
      "Epoch [11]\t Average validation loss 0.2226\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.1816\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.3194\t Accuracy 0.8900\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.5325\t Accuracy 0.9000\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.2456\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.2286\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.2805\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.2558\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.2953\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.2558\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.3136\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.2782\t Accuracy 0.9000\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2639\t Average training accuracy 0.9264\n",
      "Epoch [12]\t Average validation loss 0.2253\t Average validation accuracy 0.9380\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.1642\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.3590\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.1803\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.2128\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.1876\t Accuracy 0.9600\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.2639\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.2784\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.2531\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.2447\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.3048\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.3741\t Accuracy 0.8900\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2633\t Average training accuracy 0.9266\n",
      "Epoch [13]\t Average validation loss 0.2212\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.1520\t Accuracy 0.9700\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.2179\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.2328\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.4538\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.1146\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.3644\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.1813\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.1341\t Accuracy 0.9800\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.1900\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.1657\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.2332\t Accuracy 0.9400\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2624\t Average training accuracy 0.9267\n",
      "Epoch [14]\t Average validation loss 0.2260\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.2661\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.3488\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.1564\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.3169\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.1965\t Accuracy 0.9200\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.2474\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.2359\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.2704\t Accuracy 0.9200\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.1755\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.3573\t Accuracy 0.8600\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.1609\t Accuracy 0.9500\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2610\t Average training accuracy 0.9270\n",
      "Epoch [15]\t Average validation loss 0.2284\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.1688\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.3076\t Accuracy 0.9000\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.1925\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.2045\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.2796\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.2694\t Accuracy 0.9000\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.2136\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.3262\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.1372\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.3542\t Accuracy 0.8700\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.1061\t Accuracy 0.9600\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2601\t Average training accuracy 0.9271\n",
      "Epoch [16]\t Average validation loss 0.2210\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.2271\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.1714\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.2169\t Accuracy 0.9600\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.1523\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.1240\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.3158\t Accuracy 0.8900\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.3661\t Accuracy 0.9000\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.3096\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.1832\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.2150\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.1869\t Accuracy 0.9400\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2598\t Average training accuracy 0.9265\n",
      "Epoch [17]\t Average validation loss 0.2209\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.1377\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.3413\t Accuracy 0.9000\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.1373\t Accuracy 0.9700\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.2055\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.2432\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.2971\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.3469\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.2565\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.2017\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.4860\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.2417\t Accuracy 0.9200\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2586\t Average training accuracy 0.9277\n",
      "Epoch [18]\t Average validation loss 0.2211\t Average validation accuracy 0.9402\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.2515\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.3907\t Accuracy 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.2537\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.1827\t Accuracy 0.9600\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.1704\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.2997\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.3599\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.1838\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.2983\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.0964\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.2581\t Accuracy 0.9500\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2578\t Average training accuracy 0.9281\n",
      "Epoch [19]\t Average validation loss 0.2263\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.3409\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.2213\t Accuracy 0.9000\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.1823\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.2215\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.2018\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.3524\t Accuracy 0.8900\n",
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.2121\t Accuracy 0.9500\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.2992\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.3416\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.3198\t Accuracy 0.8900\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.1944\t Accuracy 0.9300\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2563\t Average training accuracy 0.9288\n",
      "Epoch [20]\t Average validation loss 0.2209\t Average validation accuracy 0.9370\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.3076\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.3989\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.1402\t Accuracy 0.9800\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.2876\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.2832\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.2578\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.2990\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.3900\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.2093\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.1882\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.1665\t Accuracy 0.9700\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2565\t Average training accuracy 0.9286\n",
      "Epoch [21]\t Average validation loss 0.2220\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.2071\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.2295\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.3717\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.2127\t Accuracy 0.9600\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.2035\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.2031\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.2786\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.2635\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.2195\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.3183\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.2361\t Accuracy 0.9400\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2554\t Average training accuracy 0.9292\n",
      "Epoch [22]\t Average validation loss 0.2234\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.4170\t Accuracy 0.8800\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.1713\t Accuracy 0.9700\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.2063\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.3079\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.2046\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.2020\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.1590\t Accuracy 0.9800\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.2698\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.3052\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.3549\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.2080\t Accuracy 0.9300\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2543\t Average training accuracy 0.9293\n",
      "Epoch [23]\t Average validation loss 0.2229\t Average validation accuracy 0.9376\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.1530\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.1494\t Accuracy 0.9700\n",
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.1768\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.3533\t Accuracy 0.8900\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.4312\t Accuracy 0.8800\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.2885\t Accuracy 0.9100\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.2069\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.2618\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.2709\t Accuracy 0.8700\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.2733\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.1492\t Accuracy 0.9500\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2535\t Average training accuracy 0.9293\n",
      "Epoch [24]\t Average validation loss 0.2190\t Average validation accuracy 0.9410\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.2635\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.3021\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.1656\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.2189\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.4010\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.2560\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.3012\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.3846\t Accuracy 0.8800\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.1398\t Accuracy 0.9600\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.2362\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.4965\t Accuracy 0.9300\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2540\t Average training accuracy 0.9299\n",
      "Epoch [25]\t Average validation loss 0.2246\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.2505\t Accuracy 0.9000\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.2666\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.2035\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.1775\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.3632\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.1989\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.2402\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.1499\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.2042\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.2655\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.1974\t Accuracy 0.9400\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2531\t Average training accuracy 0.9299\n",
      "Epoch [26]\t Average validation loss 0.2220\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.2909\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.1914\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.3208\t Accuracy 0.8700\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.1390\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.2666\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.4074\t Accuracy 0.9000\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.1793\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.1518\t Accuracy 0.9800\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.2573\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.1348\t Accuracy 0.9700\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.1191\t Accuracy 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [27]\t Average training loss 0.2516\t Average training accuracy 0.9294\n",
      "Epoch [27]\t Average validation loss 0.2237\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.1307\t Accuracy 0.9600\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.2563\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.2918\t Accuracy 0.8900\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.3253\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.1808\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.2155\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.1932\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.2560\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.3545\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.3010\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.1050\t Accuracy 0.9700\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2520\t Average training accuracy 0.9298\n",
      "Epoch [28]\t Average validation loss 0.2229\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.2932\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.1997\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.2218\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.1641\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.2360\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.3219\t Accuracy 0.8800\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.1568\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.4074\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.1747\t Accuracy 0.9600\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.3978\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.2451\t Accuracy 0.9300\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2516\t Average training accuracy 0.9296\n",
      "Epoch [29]\t Average validation loss 0.2219\t Average validation accuracy 0.9430\n",
      "\n",
      "spend time: 42.20379829406738\n",
      "Final test accuracy 0.9216\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.4125\t Accuracy 0.1000\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.4753\t Accuracy 0.8700\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.4181\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.5131\t Accuracy 0.8100\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.3544\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.4887\t Accuracy 0.8700\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.2663\t Accuracy 0.9400\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.4383\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.2978\t Accuracy 0.9500\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.2887\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.4125\t Accuracy 0.8800\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3902\t Average training accuracy 0.8863\n",
      "Epoch [0]\t Average validation loss 0.2503\t Average validation accuracy 0.9294\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.4076\t Accuracy 0.8700\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.4411\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.3157\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.3109\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.3309\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.2635\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.2646\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.2189\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.2591\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.1676\t Accuracy 0.9500\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.2683\t Accuracy 0.9100\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3078\t Average training accuracy 0.9127\n",
      "Epoch [1]\t Average validation loss 0.2425\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.4558\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.3225\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.3539\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.4143\t Accuracy 0.8600\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.2070\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.3702\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.3414\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.2098\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.1811\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.2908\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.4592\t Accuracy 0.8300\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2941\t Average training accuracy 0.9170\n",
      "Epoch [2]\t Average validation loss 0.2312\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.4578\t Accuracy 0.8700\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.2972\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.2350\t Accuracy 0.9500\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.1341\t Accuracy 0.9700\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.1681\t Accuracy 0.9700\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.4883\t Accuracy 0.8700\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.2380\t Accuracy 0.9400\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.1986\t Accuracy 0.9600\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.2006\t Accuracy 0.9400\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.3270\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.3106\t Accuracy 0.9000\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2859\t Average training accuracy 0.9198\n",
      "Epoch [3]\t Average validation loss 0.2341\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.2557\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.3046\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.2071\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.4175\t Accuracy 0.8800\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.2499\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.2512\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.1925\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.2046\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.3109\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.3037\t Accuracy 0.8900\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.1802\t Accuracy 0.9500\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2816\t Average training accuracy 0.9199\n",
      "Epoch [4]\t Average validation loss 0.2289\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.3923\t Accuracy 0.8700\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.2797\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.2736\t Accuracy 0.8700\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.3544\t Accuracy 0.8900\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.3189\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.2588\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.1446\t Accuracy 0.9700\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.3247\t Accuracy 0.8800\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.2231\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.1673\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.2015\t Accuracy 0.9400\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2772\t Average training accuracy 0.9227\n",
      "Epoch [5]\t Average validation loss 0.2329\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.2338\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.3209\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.2276\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.3698\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.5160\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.4100\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.2611\t Accuracy 0.9600\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.2377\t Accuracy 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.3133\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.1933\t Accuracy 0.9600\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.3077\t Accuracy 0.8600\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2740\t Average training accuracy 0.9235\n",
      "Epoch [6]\t Average validation loss 0.2268\t Average validation accuracy 0.9354\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.3188\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.3461\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.2012\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.4979\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.4638\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.3181\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.3508\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.2903\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.3707\t Accuracy 0.8900\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.2380\t Accuracy 0.9500\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.2257\t Accuracy 0.9300\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2723\t Average training accuracy 0.9245\n",
      "Epoch [7]\t Average validation loss 0.2266\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.3427\t Accuracy 0.9200\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.1086\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.1962\t Accuracy 0.9400\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.3185\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.1416\t Accuracy 0.9600\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.1957\t Accuracy 0.9600\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.2619\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.2277\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.4752\t Accuracy 0.8600\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.2786\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.3140\t Accuracy 0.9200\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2707\t Average training accuracy 0.9242\n",
      "Epoch [8]\t Average validation loss 0.2258\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.2300\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.2053\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.2710\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.3319\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.4378\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.3056\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.1998\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.1475\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.2259\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.1516\t Accuracy 0.9500\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.3603\t Accuracy 0.9100\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2682\t Average training accuracy 0.9243\n",
      "Epoch [9]\t Average validation loss 0.2253\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.1086\t Accuracy 0.9900\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.1712\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.3185\t Accuracy 0.8900\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.4469\t Accuracy 0.8800\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.3608\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.3964\t Accuracy 0.8600\n",
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.2673\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.2231\t Accuracy 0.9500\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.2257\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.3038\t Accuracy 0.8900\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.2421\t Accuracy 0.9100\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2671\t Average training accuracy 0.9247\n",
      "Epoch [10]\t Average validation loss 0.2225\t Average validation accuracy 0.9418\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.2230\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.2642\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.2365\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.2233\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.2980\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.1544\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.2162\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.2015\t Accuracy 0.9600\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.3483\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.2701\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.2640\t Accuracy 0.9000\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2661\t Average training accuracy 0.9252\n",
      "Epoch [11]\t Average validation loss 0.2252\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.2403\t Accuracy 0.9600\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.1867\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.1372\t Accuracy 0.9500\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.2178\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.3060\t Accuracy 0.8800\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.2958\t Accuracy 0.9000\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.3621\t Accuracy 0.8800\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.3214\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.2551\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.5012\t Accuracy 0.8500\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.2844\t Accuracy 0.9400\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2645\t Average training accuracy 0.9261\n",
      "Epoch [12]\t Average validation loss 0.2253\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.2798\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.2006\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.4147\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.2674\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.2464\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.2423\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.2327\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.2966\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.2416\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.1978\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.2956\t Accuracy 0.9000\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2633\t Average training accuracy 0.9262\n",
      "Epoch [13]\t Average validation loss 0.2226\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.2301\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.2198\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.2785\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.4619\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.2886\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.3160\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.1886\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.2389\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.2232\t Accuracy 0.9600\n",
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.2090\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.4528\t Accuracy 0.9000\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2627\t Average training accuracy 0.9257\n",
      "Epoch [14]\t Average validation loss 0.2254\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.1253\t Accuracy 0.9700\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.2446\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.2040\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.1064\t Accuracy 0.9700\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.1449\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.3639\t Accuracy 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.3927\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.2005\t Accuracy 0.9600\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.2487\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.3552\t Accuracy 0.8700\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.1403\t Accuracy 0.9400\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2611\t Average training accuracy 0.9267\n",
      "Epoch [15]\t Average validation loss 0.2270\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.1494\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.1917\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.2178\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.5049\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.2307\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.3848\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.1093\t Accuracy 0.9800\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.1707\t Accuracy 0.9600\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.1479\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.1615\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.2744\t Accuracy 0.9300\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2602\t Average training accuracy 0.9274\n",
      "Epoch [16]\t Average validation loss 0.2250\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.1422\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.2052\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.4422\t Accuracy 0.9000\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.2183\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.1713\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.2964\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.2959\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.1652\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.1476\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.1889\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.3083\t Accuracy 0.9100\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2595\t Average training accuracy 0.9267\n",
      "Epoch [17]\t Average validation loss 0.2232\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.1985\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.2143\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.2254\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.2004\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.2883\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.1967\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.2904\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.3637\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.1989\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.3470\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.4552\t Accuracy 0.9200\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2587\t Average training accuracy 0.9272\n",
      "Epoch [18]\t Average validation loss 0.2250\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.1731\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.2007\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.1791\t Accuracy 0.9600\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.2453\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.2771\t Accuracy 0.9000\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.3394\t Accuracy 0.8900\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.3286\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.1514\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.3953\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.1561\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.4771\t Accuracy 0.8500\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2575\t Average training accuracy 0.9280\n",
      "Epoch [19]\t Average validation loss 0.2288\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.2140\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.3236\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.4004\t Accuracy 0.8900\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.1734\t Accuracy 0.9700\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.2214\t Accuracy 0.9000\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.2281\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.2675\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.3219\t Accuracy 0.9000\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.2235\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.2022\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.2485\t Accuracy 0.9200\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2568\t Average training accuracy 0.9281\n",
      "Epoch [20]\t Average validation loss 0.2291\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.2204\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.4012\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.4123\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.2788\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.2154\t Accuracy 0.9600\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.3169\t Accuracy 0.9600\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.2409\t Accuracy 0.9600\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.2849\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.2432\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.3728\t Accuracy 0.8900\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.2948\t Accuracy 0.9100\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2565\t Average training accuracy 0.9279\n",
      "Epoch [21]\t Average validation loss 0.2296\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.2645\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.2253\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.3010\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.2980\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.2272\t Accuracy 0.9600\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.3952\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.1399\t Accuracy 0.9600\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.1320\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.2151\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.2692\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.3055\t Accuracy 0.9500\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2561\t Average training accuracy 0.9288\n",
      "Epoch [22]\t Average validation loss 0.2242\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.2271\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.3338\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.1828\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.2715\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.2320\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.3726\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.0747\t Accuracy 0.9900\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.4742\t Accuracy 0.8900\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.1176\t Accuracy 0.9700\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.2192\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.2896\t Accuracy 0.9200\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2544\t Average training accuracy 0.9288\n",
      "Epoch [23]\t Average validation loss 0.2287\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.2129\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.1981\t Accuracy 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.2716\t Accuracy 0.9100\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.2465\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.4368\t Accuracy 0.9100\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.1036\t Accuracy 0.9600\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.2803\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.3360\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.1852\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.1455\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.3489\t Accuracy 0.8900\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2541\t Average training accuracy 0.9289\n",
      "Epoch [24]\t Average validation loss 0.2279\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.2501\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.2464\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.4580\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.3757\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.1753\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.2223\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.3216\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.1725\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.1342\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.2120\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.4527\t Accuracy 0.9300\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2536\t Average training accuracy 0.9293\n",
      "Epoch [25]\t Average validation loss 0.2263\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.2747\t Accuracy 0.9600\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.1837\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.2825\t Accuracy 0.9100\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.2199\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.1851\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.1311\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.2320\t Accuracy 0.9000\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.1356\t Accuracy 0.9600\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.2020\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.3084\t Accuracy 0.8700\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.2842\t Accuracy 0.9100\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2542\t Average training accuracy 0.9286\n",
      "Epoch [26]\t Average validation loss 0.2327\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.1827\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.2927\t Accuracy 0.9000\n",
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.2685\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.2052\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.1430\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.2727\t Accuracy 0.9000\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.1685\t Accuracy 0.9800\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.2165\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.2920\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.4256\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.1741\t Accuracy 0.9500\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2534\t Average training accuracy 0.9289\n",
      "Epoch [27]\t Average validation loss 0.2284\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.1891\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.5173\t Accuracy 0.8800\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.2984\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.1584\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.2839\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.4253\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.2082\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.1690\t Accuracy 0.9600\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.3652\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.1977\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.2209\t Accuracy 0.9400\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2530\t Average training accuracy 0.9294\n",
      "Epoch [28]\t Average validation loss 0.2266\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.1352\t Accuracy 0.9600\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.3237\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.2404\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.1861\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.2484\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.1746\t Accuracy 0.9700\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.2778\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.1842\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.1994\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.3203\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.2084\t Accuracy 0.9700\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2532\t Average training accuracy 0.9296\n",
      "Epoch [29]\t Average validation loss 0.2281\t Average validation accuracy 0.9382\n",
      "\n",
      "spend time: 45.1870801448822\n",
      "Final test accuracy 0.9236\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.6182\t Accuracy 0.0500\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.4212\t Accuracy 0.8400\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.3496\t Accuracy 0.8700\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.5579\t Accuracy 0.8200\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.1816\t Accuracy 0.9600\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.4670\t Accuracy 0.8700\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.3514\t Accuracy 0.9300\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.3329\t Accuracy 0.9400\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.2655\t Accuracy 0.9400\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.1880\t Accuracy 0.9400\n",
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.3555\t Accuracy 0.9100\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3831\t Average training accuracy 0.8876\n",
      "Epoch [0]\t Average validation loss 0.2434\t Average validation accuracy 0.9336\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.2218\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.2231\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.2697\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.4303\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.2866\t Accuracy 0.9500\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.3561\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.2853\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.3504\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.2767\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.3116\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.2754\t Accuracy 0.9000\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3064\t Average training accuracy 0.9132\n",
      "Epoch [1]\t Average validation loss 0.2382\t Average validation accuracy 0.9370\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.3054\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.1500\t Accuracy 0.9600\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.3408\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.6445\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.4058\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.2400\t Accuracy 0.9500\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.2431\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.2763\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.2756\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.2446\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.2354\t Accuracy 0.9300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2]\t Average training loss 0.2955\t Average training accuracy 0.9158\n",
      "Epoch [2]\t Average validation loss 0.2340\t Average validation accuracy 0.9380\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.2072\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.3159\t Accuracy 0.9000\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.2776\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.2111\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.1908\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.3623\t Accuracy 0.8800\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.3699\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.3651\t Accuracy 0.9000\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.2278\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.2100\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.2317\t Accuracy 0.9300\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2882\t Average training accuracy 0.9184\n",
      "Epoch [3]\t Average validation loss 0.2253\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.1637\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.2176\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.3848\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.2816\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.2232\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.2850\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.1612\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.3022\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.2914\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.2143\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.4492\t Accuracy 0.8800\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2821\t Average training accuracy 0.9212\n",
      "Epoch [4]\t Average validation loss 0.2268\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.4707\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.2544\t Accuracy 0.9700\n",
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.1739\t Accuracy 0.9600\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.4175\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.3633\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.3558\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.1899\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.3839\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.2388\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.3742\t Accuracy 0.9400\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.0934\t Accuracy 0.9900\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2794\t Average training accuracy 0.9224\n",
      "Epoch [5]\t Average validation loss 0.2329\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.3729\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.2935\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.2547\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.1998\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.2548\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.1938\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.3439\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.3828\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.3469\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.4083\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.2668\t Accuracy 0.9200\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2753\t Average training accuracy 0.9227\n",
      "Epoch [6]\t Average validation loss 0.2269\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.2258\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.2875\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.1631\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.1743\t Accuracy 0.9500\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.3307\t Accuracy 0.8700\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.3586\t Accuracy 0.8900\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.1755\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.2384\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.2874\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.2630\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.3333\t Accuracy 0.8800\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2740\t Average training accuracy 0.9237\n",
      "Epoch [7]\t Average validation loss 0.2261\t Average validation accuracy 0.9354\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.3998\t Accuracy 0.8700\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.3034\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.3166\t Accuracy 0.8900\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.4631\t Accuracy 0.8700\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.1501\t Accuracy 0.9600\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.3494\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.3139\t Accuracy 0.8900\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.2625\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.3069\t Accuracy 0.8800\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.2246\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.3250\t Accuracy 0.8900\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2722\t Average training accuracy 0.9238\n",
      "Epoch [8]\t Average validation loss 0.2260\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.3433\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.2624\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.1612\t Accuracy 0.9700\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.2440\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.4444\t Accuracy 0.8700\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.3562\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.1734\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.4735\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.4426\t Accuracy 0.8800\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.3373\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.2652\t Accuracy 0.9200\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2704\t Average training accuracy 0.9244\n",
      "Epoch [9]\t Average validation loss 0.2281\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.1818\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.1136\t Accuracy 0.9700\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.4000\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.2046\t Accuracy 0.9500\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.3211\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.2217\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.1969\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.2820\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.2541\t Accuracy 0.9500\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.2083\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.2819\t Accuracy 0.9300\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2696\t Average training accuracy 0.9239\n",
      "Epoch [10]\t Average validation loss 0.2252\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.3589\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.2168\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.2349\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.3059\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.3773\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.3006\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.2585\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.1494\t Accuracy 0.9700\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.4287\t Accuracy 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.1416\t Accuracy 0.9600\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.2884\t Accuracy 0.8800\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2669\t Average training accuracy 0.9245\n",
      "Epoch [11]\t Average validation loss 0.2318\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.2222\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.1960\t Accuracy 0.9600\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.2654\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.1542\t Accuracy 0.9500\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.3512\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.5061\t Accuracy 0.8900\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.2852\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.1833\t Accuracy 0.9600\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.1265\t Accuracy 0.9500\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.2919\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.2372\t Accuracy 0.9200\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2665\t Average training accuracy 0.9253\n",
      "Epoch [12]\t Average validation loss 0.2324\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.3025\t Accuracy 0.8800\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.3264\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.3205\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.4131\t Accuracy 0.8600\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.1369\t Accuracy 0.9500\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.3140\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.2605\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.2384\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.1767\t Accuracy 0.9700\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.2471\t Accuracy 0.9500\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.1809\t Accuracy 0.9500\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2646\t Average training accuracy 0.9253\n",
      "Epoch [13]\t Average validation loss 0.2258\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.3076\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.4074\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.3065\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.1846\t Accuracy 0.9600\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.1493\t Accuracy 0.9600\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.2667\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.3720\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.2153\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.4050\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.2021\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.1989\t Accuracy 0.9300\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2657\t Average training accuracy 0.9255\n",
      "Epoch [14]\t Average validation loss 0.2413\t Average validation accuracy 0.9352\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.1093\t Accuracy 0.9800\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.3097\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.1419\t Accuracy 0.9700\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.1959\t Accuracy 0.9200\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.2431\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.2398\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.5000\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.1886\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.2202\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.1590\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.3234\t Accuracy 0.9200\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2635\t Average training accuracy 0.9259\n",
      "Epoch [15]\t Average validation loss 0.2279\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.2410\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.1657\t Accuracy 0.9600\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.2601\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.1629\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.1850\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.1879\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.1972\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.3212\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.1736\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.3263\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.3303\t Accuracy 0.8600\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2620\t Average training accuracy 0.9260\n",
      "Epoch [16]\t Average validation loss 0.2378\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.1345\t Accuracy 0.9600\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.1405\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.1885\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.2539\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.3733\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.2790\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.0621\t Accuracy 1.0000\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.1598\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.3038\t Accuracy 0.9000\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.2457\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.4400\t Accuracy 0.9400\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2621\t Average training accuracy 0.9257\n",
      "Epoch [17]\t Average validation loss 0.2265\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.1713\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.2381\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.3381\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.1569\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.3391\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.3392\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.2702\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.2569\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.1191\t Accuracy 0.9700\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.4390\t Accuracy 0.8700\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.2755\t Accuracy 0.9300\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2608\t Average training accuracy 0.9271\n",
      "Epoch [18]\t Average validation loss 0.2285\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.1466\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.2633\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.3080\t Accuracy 0.8800\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.2613\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.2841\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.2036\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.3050\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.2910\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.2947\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.1611\t Accuracy 0.9600\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.2824\t Accuracy 0.9400\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2617\t Average training accuracy 0.9257\n",
      "Epoch [19]\t Average validation loss 0.2248\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.1763\t Accuracy 0.9600\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.0949\t Accuracy 0.9600\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.3767\t Accuracy 0.8600\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.2388\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.2837\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.3301\t Accuracy 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.1817\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.3883\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.1641\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.1509\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.3043\t Accuracy 0.8800\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2603\t Average training accuracy 0.9268\n",
      "Epoch [20]\t Average validation loss 0.2294\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.1327\t Accuracy 0.9700\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.4380\t Accuracy 0.8700\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.1189\t Accuracy 0.9800\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.1339\t Accuracy 0.9800\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.2794\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.2702\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.3252\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.1708\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.1838\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.4163\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.4017\t Accuracy 0.8700\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2588\t Average training accuracy 0.9279\n",
      "Epoch [21]\t Average validation loss 0.2311\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.1297\t Accuracy 0.9800\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.1754\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.1523\t Accuracy 0.9600\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.3418\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.1471\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.1865\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.2256\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.3245\t Accuracy 0.8900\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.2168\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.2114\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.2519\t Accuracy 0.8900\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2584\t Average training accuracy 0.9271\n",
      "Epoch [22]\t Average validation loss 0.2347\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.3148\t Accuracy 0.8900\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.2682\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.3591\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.2135\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.1708\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.1764\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.4187\t Accuracy 0.8800\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.1963\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.1839\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.3571\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.3812\t Accuracy 0.9200\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2587\t Average training accuracy 0.9274\n",
      "Epoch [23]\t Average validation loss 0.2263\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.1714\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.1925\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.2235\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.4064\t Accuracy 0.8900\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.2896\t Accuracy 0.8900\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.3519\t Accuracy 0.8900\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.2576\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.1744\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.1648\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.1589\t Accuracy 0.9700\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.2959\t Accuracy 0.9100\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2575\t Average training accuracy 0.9275\n",
      "Epoch [24]\t Average validation loss 0.2340\t Average validation accuracy 0.9380\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.1332\t Accuracy 0.9800\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.3642\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.1925\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.3236\t Accuracy 0.9000\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.1196\t Accuracy 0.9600\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.1739\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.4587\t Accuracy 0.8300\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.1774\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.4158\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.2962\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.5417\t Accuracy 0.8900\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2574\t Average training accuracy 0.9266\n",
      "Epoch [25]\t Average validation loss 0.2352\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.2548\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.2689\t Accuracy 0.9100\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.1802\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.1748\t Accuracy 0.9600\n",
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.1811\t Accuracy 0.9600\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.1280\t Accuracy 0.9800\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.2514\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.2581\t Accuracy 0.9100\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.2751\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.4509\t Accuracy 0.8900\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.2369\t Accuracy 0.9500\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2542\t Average training accuracy 0.9288\n",
      "Epoch [26]\t Average validation loss 0.2345\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.1702\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.2830\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.2499\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.2691\t Accuracy 0.8900\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.3823\t Accuracy 0.8800\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.2878\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.2389\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.2023\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.2122\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.5894\t Accuracy 0.8700\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.2269\t Accuracy 0.9500\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2562\t Average training accuracy 0.9277\n",
      "Epoch [27]\t Average validation loss 0.2392\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.2899\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.1641\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.1917\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.3176\t Accuracy 0.8800\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.4230\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.1978\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.1884\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.2814\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.2326\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.4152\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.2633\t Accuracy 0.9200\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2544\t Average training accuracy 0.9281\n",
      "Epoch [28]\t Average validation loss 0.2339\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.2550\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.2587\t Accuracy 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.3957\t Accuracy 0.8800\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.2300\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.1407\t Accuracy 0.9600\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.1647\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.3019\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.2269\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.1685\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.3286\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.2929\t Accuracy 0.8900\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2545\t Average training accuracy 0.9282\n",
      "Epoch [29]\t Average validation loss 0.2423\t Average validation accuracy 0.9318\n",
      "\n",
      "spend time: 42.72574591636658\n",
      "Final test accuracy 0.9167\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.4879\t Accuracy 0.0900\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.5197\t Accuracy 0.8700\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.3659\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.4551\t Accuracy 0.8500\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.3546\t Accuracy 0.9200\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.3943\t Accuracy 0.8700\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.3007\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.2900\t Accuracy 0.9200\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.3545\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.3170\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.3207\t Accuracy 0.9400\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3745\t Average training accuracy 0.8910\n",
      "Epoch [0]\t Average validation loss 0.2582\t Average validation accuracy 0.9222\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.3815\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.2681\t Accuracy 0.9400\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.2715\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.2083\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.2488\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.5329\t Accuracy 0.8400\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.3448\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.4084\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.2964\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.3322\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.0783\t Accuracy 0.9900\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3090\t Average training accuracy 0.9128\n",
      "Epoch [1]\t Average validation loss 0.2453\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.2746\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.2919\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.1977\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.2625\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.4312\t Accuracy 0.8400\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.2204\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.2516\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.2404\t Accuracy 0.9600\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.2036\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.2746\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.1940\t Accuracy 0.9500\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2969\t Average training accuracy 0.9169\n",
      "Epoch [2]\t Average validation loss 0.2425\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.3949\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.1122\t Accuracy 0.9600\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.2793\t Accuracy 0.9000\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.1759\t Accuracy 0.9500\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.1340\t Accuracy 0.9600\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.2203\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.2549\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.4133\t Accuracy 0.8900\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.3212\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.1660\t Accuracy 0.9700\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.1373\t Accuracy 0.9600\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2894\t Average training accuracy 0.9183\n",
      "Epoch [3]\t Average validation loss 0.2362\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.2305\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.2032\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.1884\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.2957\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.1837\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.1971\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.2678\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.2343\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.2220\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.0661\t Accuracy 1.0000\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.2771\t Accuracy 0.9000\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2866\t Average training accuracy 0.9193\n",
      "Epoch [4]\t Average validation loss 0.2358\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.1715\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.2325\t Accuracy 0.9400\n",
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.2826\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.4210\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.1746\t Accuracy 0.9600\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.2042\t Accuracy 0.9400\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.3453\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.2969\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.2289\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.2064\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.3037\t Accuracy 0.9000\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2844\t Average training accuracy 0.9199\n",
      "Epoch [5]\t Average validation loss 0.2368\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.3777\t Accuracy 0.8600\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.2907\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.3665\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.2604\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.1396\t Accuracy 0.9700\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.3961\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.2213\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.3071\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.2370\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.1928\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.3643\t Accuracy 0.9300\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2792\t Average training accuracy 0.9217\n",
      "Epoch [6]\t Average validation loss 0.2358\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.1849\t Accuracy 0.9700\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.2019\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.1906\t Accuracy 0.9500\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.2936\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.2194\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.2859\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.2115\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.3387\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.2673\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.1594\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.2746\t Accuracy 0.9000\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2779\t Average training accuracy 0.9215\n",
      "Epoch [7]\t Average validation loss 0.2389\t Average validation accuracy 0.9300\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.1056\t Accuracy 0.9700\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.1901\t Accuracy 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.4093\t Accuracy 0.9200\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.1596\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.3665\t Accuracy 0.8900\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.1619\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.2548\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.2781\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.2503\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.2917\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.4435\t Accuracy 0.9500\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2776\t Average training accuracy 0.9223\n",
      "Epoch [8]\t Average validation loss 0.2420\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.3562\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.2048\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.2719\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.3277\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.3907\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.4824\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.2303\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.1991\t Accuracy 0.9600\n",
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.3990\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.3272\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.1832\t Accuracy 0.9200\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2756\t Average training accuracy 0.9219\n",
      "Epoch [9]\t Average validation loss 0.2269\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.2418\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.3866\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.4769\t Accuracy 0.8800\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.3286\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.2263\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.2265\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.0824\t Accuracy 0.9900\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.1698\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.2655\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.2922\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.2422\t Accuracy 0.9300\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2735\t Average training accuracy 0.9227\n",
      "Epoch [10]\t Average validation loss 0.2406\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.3285\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.2276\t Accuracy 0.9600\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.2471\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.1389\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.2042\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.3537\t Accuracy 0.8900\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.3368\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.2073\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.3213\t Accuracy 0.8900\n",
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.2469\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.2902\t Accuracy 0.9300\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2735\t Average training accuracy 0.9231\n",
      "Epoch [11]\t Average validation loss 0.2360\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.3030\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.4897\t Accuracy 0.8700\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.1908\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.2622\t Accuracy 0.8900\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.2496\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.2323\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.0978\t Accuracy 0.9800\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.4455\t Accuracy 0.8800\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.3050\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.0931\t Accuracy 0.9700\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.2732\t Accuracy 0.9100\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2721\t Average training accuracy 0.9232\n",
      "Epoch [12]\t Average validation loss 0.2310\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.2019\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.1883\t Accuracy 0.9600\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.2024\t Accuracy 0.9500\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.3029\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.2537\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.3365\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.1643\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.4261\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.2338\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.1303\t Accuracy 0.9700\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.2137\t Accuracy 0.9100\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2692\t Average training accuracy 0.9247\n",
      "Epoch [13]\t Average validation loss 0.2460\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.4590\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.4891\t Accuracy 0.8900\n",
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.2460\t Accuracy 0.8900\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.1208\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.2229\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.5977\t Accuracy 0.8700\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.3620\t Accuracy 0.8900\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.2164\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.2913\t Accuracy 0.8900\n",
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.2554\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.2472\t Accuracy 0.9300\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2680\t Average training accuracy 0.9245\n",
      "Epoch [14]\t Average validation loss 0.2392\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.3726\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.1017\t Accuracy 0.9800\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.2074\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.1536\t Accuracy 0.9600\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.1917\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.2966\t Accuracy 0.8800\n",
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.1761\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.3329\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.2626\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.2346\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.1864\t Accuracy 0.9400\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2682\t Average training accuracy 0.9245\n",
      "Epoch [15]\t Average validation loss 0.2424\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.2938\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.4209\t Accuracy 0.8900\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.2132\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.3899\t Accuracy 0.8800\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.2395\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.2755\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.2438\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.2291\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.1892\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.2550\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.3521\t Accuracy 0.9100\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2678\t Average training accuracy 0.9254\n",
      "Epoch [16]\t Average validation loss 0.2431\t Average validation accuracy 0.9344\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.3813\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.4264\t Accuracy 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.4358\t Accuracy 0.8700\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.2964\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.1513\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.3950\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.1366\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.1920\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.1753\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.5318\t Accuracy 0.8600\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.3800\t Accuracy 0.9000\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2664\t Average training accuracy 0.9257\n",
      "Epoch [17]\t Average validation loss 0.2417\t Average validation accuracy 0.9322\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.1472\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.2489\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.3506\t Accuracy 0.9000\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.1766\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.2669\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.3305\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.1568\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.4515\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.2847\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.4110\t Accuracy 0.9000\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.2123\t Accuracy 0.9200\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2654\t Average training accuracy 0.9263\n",
      "Epoch [18]\t Average validation loss 0.2424\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.2376\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.2170\t Accuracy 0.9000\n",
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.3228\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.4326\t Accuracy 0.8600\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.2857\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.4512\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.3553\t Accuracy 0.8600\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.1903\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.2448\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.3222\t Accuracy 0.8700\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.1959\t Accuracy 0.9300\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2654\t Average training accuracy 0.9255\n",
      "Epoch [19]\t Average validation loss 0.2395\t Average validation accuracy 0.9334\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.2273\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.2675\t Accuracy 0.9500\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.3160\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.1802\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.1708\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.1652\t Accuracy 0.9600\n",
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.1874\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.2428\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.3583\t Accuracy 0.9000\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.2819\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.2376\t Accuracy 0.9000\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2651\t Average training accuracy 0.9248\n",
      "Epoch [20]\t Average validation loss 0.2362\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.3418\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.1775\t Accuracy 0.9500\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.1006\t Accuracy 0.9500\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.3007\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.2551\t Accuracy 0.9500\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.1107\t Accuracy 0.9600\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.3733\t Accuracy 0.8200\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.1713\t Accuracy 0.9600\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.1960\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.2293\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.1852\t Accuracy 0.8900\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2643\t Average training accuracy 0.9254\n",
      "Epoch [21]\t Average validation loss 0.2349\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.1810\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.1534\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.2325\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.2319\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.1524\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.2188\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.2741\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.1466\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.3551\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.5452\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.1527\t Accuracy 0.9400\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2633\t Average training accuracy 0.9271\n",
      "Epoch [22]\t Average validation loss 0.2363\t Average validation accuracy 0.9344\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.2632\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.2789\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.4287\t Accuracy 0.8800\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.1338\t Accuracy 0.9600\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.3941\t Accuracy 0.8800\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.2463\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.3260\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.2177\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.2705\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.2150\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.3350\t Accuracy 0.9300\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2630\t Average training accuracy 0.9260\n",
      "Epoch [23]\t Average validation loss 0.2489\t Average validation accuracy 0.9314\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.2515\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.2236\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.1939\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.2105\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.2209\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.2147\t Accuracy 0.9100\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.2024\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.2093\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.1275\t Accuracy 0.9600\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.2606\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.4807\t Accuracy 0.8900\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2608\t Average training accuracy 0.9265\n",
      "Epoch [24]\t Average validation loss 0.2459\t Average validation accuracy 0.9334\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.2087\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.1949\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.1554\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.1916\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.2379\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.3063\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.4537\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.4718\t Accuracy 0.8600\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.5130\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.4175\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.2174\t Accuracy 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [25]\t Average training loss 0.2604\t Average training accuracy 0.9264\n",
      "Epoch [25]\t Average validation loss 0.2337\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.1584\t Accuracy 0.9700\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.2731\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.2880\t Accuracy 0.8900\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.2533\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.2833\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.2290\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.3441\t Accuracy 0.9100\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.1732\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.2758\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.1568\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.1264\t Accuracy 0.9600\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2611\t Average training accuracy 0.9266\n",
      "Epoch [26]\t Average validation loss 0.2340\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.0790\t Accuracy 0.9900\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.1840\t Accuracy 0.9700\n",
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.3454\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.2042\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.2673\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.2072\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.2062\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.2772\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.2082\t Accuracy 0.9600\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.1051\t Accuracy 0.9800\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.3836\t Accuracy 0.9400\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2617\t Average training accuracy 0.9258\n",
      "Epoch [27]\t Average validation loss 0.2435\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.3624\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.2270\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.1624\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.3169\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.2542\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.3082\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.1511\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.1444\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.3756\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.2821\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.2886\t Accuracy 0.9400\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2598\t Average training accuracy 0.9270\n",
      "Epoch [28]\t Average validation loss 0.2378\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.1245\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.3368\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.3633\t Accuracy 0.8900\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.1972\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.1931\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.3317\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.3071\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.5021\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.3101\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.5402\t Accuracy 0.8900\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.2568\t Accuracy 0.9200\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2596\t Average training accuracy 0.9278\n",
      "Epoch [29]\t Average validation loss 0.2467\t Average validation accuracy 0.9348\n",
      "\n",
      "spend time: 44.5462760925293\n",
      "Final test accuracy 0.9189\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.6053\t Accuracy 0.0200\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.4392\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.4636\t Accuracy 0.8400\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.3256\t Accuracy 0.8700\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.2683\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.3177\t Accuracy 0.9500\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.3950\t Accuracy 0.8800\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.1884\t Accuracy 0.9500\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.3421\t Accuracy 0.8900\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.2490\t Accuracy 0.9200\n",
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.2795\t Accuracy 0.8900\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3784\t Average training accuracy 0.8884\n",
      "Epoch [0]\t Average validation loss 0.2642\t Average validation accuracy 0.9258\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.2780\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.6598\t Accuracy 0.8900\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.3073\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.2224\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.3256\t Accuracy 0.9400\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.3477\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.1382\t Accuracy 0.9500\n",
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.2805\t Accuracy 0.8900\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.4519\t Accuracy 0.8500\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.4266\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.3776\t Accuracy 0.9000\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3123\t Average training accuracy 0.9120\n",
      "Epoch [1]\t Average validation loss 0.2342\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.2718\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.3385\t Accuracy 0.8700\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.2573\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.2636\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.2040\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.3376\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.2676\t Accuracy 0.9500\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.1384\t Accuracy 0.9500\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.3675\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.3170\t Accuracy 0.9100\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.1682\t Accuracy 0.9500\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3011\t Average training accuracy 0.9153\n",
      "Epoch [2]\t Average validation loss 0.2474\t Average validation accuracy 0.9290\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.2369\t Accuracy 0.9400\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.3056\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.2948\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.2276\t Accuracy 0.9500\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.3134\t Accuracy 0.8900\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.2198\t Accuracy 0.9400\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.1716\t Accuracy 0.9600\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.4746\t Accuracy 0.8700\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.2014\t Accuracy 0.9500\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.4933\t Accuracy 0.8600\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.4873\t Accuracy 0.8600\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2933\t Average training accuracy 0.9179\n",
      "Epoch [3]\t Average validation loss 0.2415\t Average validation accuracy 0.9320\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.3171\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.3397\t Accuracy 0.8800\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.3910\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.2690\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.2388\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.4239\t Accuracy 0.8900\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.2898\t Accuracy 0.9300\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.1336\t Accuracy 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.4015\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.3041\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.6109\t Accuracy 0.8700\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2929\t Average training accuracy 0.9180\n",
      "Epoch [4]\t Average validation loss 0.2382\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.1438\t Accuracy 0.9600\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.3200\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.2526\t Accuracy 0.8900\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.4716\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.1847\t Accuracy 0.9400\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.1976\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.2375\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.2616\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.2375\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.4482\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.0973\t Accuracy 0.9800\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2857\t Average training accuracy 0.9189\n",
      "Epoch [5]\t Average validation loss 0.2350\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.2882\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.1787\t Accuracy 0.9500\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.3519\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.6034\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.2188\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.2439\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.2639\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.3619\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.2071\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.1140\t Accuracy 0.9700\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.2314\t Accuracy 0.9700\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2832\t Average training accuracy 0.9212\n",
      "Epoch [6]\t Average validation loss 0.2379\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.2900\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.3138\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.2474\t Accuracy 0.9600\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.2451\t Accuracy 0.9500\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.3406\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.1250\t Accuracy 0.9700\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.2071\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.2134\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.3849\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.4828\t Accuracy 0.8800\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.5162\t Accuracy 0.8800\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2810\t Average training accuracy 0.9194\n",
      "Epoch [7]\t Average validation loss 0.2388\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.1569\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.2286\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.1566\t Accuracy 0.9600\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.2909\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.1897\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.1809\t Accuracy 0.9200\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.2036\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.3224\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.1451\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.4376\t Accuracy 0.8900\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.2135\t Accuracy 0.9100\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2786\t Average training accuracy 0.9219\n",
      "Epoch [8]\t Average validation loss 0.2349\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.2894\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.1474\t Accuracy 0.9500\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.2930\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.2715\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.2924\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.2457\t Accuracy 0.8700\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.5743\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.3556\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.4343\t Accuracy 0.8800\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.2671\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.1762\t Accuracy 0.9500\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2766\t Average training accuracy 0.9233\n",
      "Epoch [9]\t Average validation loss 0.2412\t Average validation accuracy 0.9330\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.2294\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.2802\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.3925\t Accuracy 0.8800\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.3515\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.3382\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.3294\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.1559\t Accuracy 0.9500\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.3775\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.2460\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.2971\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.1213\t Accuracy 0.9600\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2763\t Average training accuracy 0.9234\n",
      "Epoch [10]\t Average validation loss 0.2311\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.4664\t Accuracy 0.8900\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.2485\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.1906\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.3713\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.3141\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.2596\t Accuracy 0.8600\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.1830\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.1508\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.4029\t Accuracy 0.8900\n",
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.2546\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.1894\t Accuracy 0.9500\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2735\t Average training accuracy 0.9224\n",
      "Epoch [11]\t Average validation loss 0.2442\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.1277\t Accuracy 0.9600\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.3270\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.2947\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.2589\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.3364\t Accuracy 0.8900\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.1815\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.3646\t Accuracy 0.9500\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.3957\t Accuracy 0.9000\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.4218\t Accuracy 0.9000\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.3141\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.2130\t Accuracy 0.9000\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2758\t Average training accuracy 0.9219\n",
      "Epoch [12]\t Average validation loss 0.2498\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.2484\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.1277\t Accuracy 0.9800\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.4774\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.1201\t Accuracy 0.9700\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.4423\t Accuracy 0.8600\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.1406\t Accuracy 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.3809\t Accuracy 0.8800\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.3720\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.1569\t Accuracy 0.9600\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.2507\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.3025\t Accuracy 0.9100\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2731\t Average training accuracy 0.9226\n",
      "Epoch [13]\t Average validation loss 0.2408\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.2019\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.2072\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.2811\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.1783\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.1953\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.2767\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.2055\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.2149\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.2788\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.2766\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.1700\t Accuracy 0.9000\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2723\t Average training accuracy 0.9228\n",
      "Epoch [14]\t Average validation loss 0.2418\t Average validation accuracy 0.9320\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.1852\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.4339\t Accuracy 0.8600\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.3492\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.4560\t Accuracy 0.8600\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.1996\t Accuracy 0.9700\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.1526\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.2525\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.3014\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.2390\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.1657\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.4232\t Accuracy 0.9200\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2734\t Average training accuracy 0.9223\n",
      "Epoch [15]\t Average validation loss 0.2352\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.2255\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.1551\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.2622\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.5126\t Accuracy 0.9000\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.2618\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.1530\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.4073\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.1123\t Accuracy 0.9600\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.1598\t Accuracy 0.9700\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.3221\t Accuracy 0.9000\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.2428\t Accuracy 0.9500\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2723\t Average training accuracy 0.9237\n",
      "Epoch [16]\t Average validation loss 0.2497\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.1902\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.2471\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.5085\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.0949\t Accuracy 0.9600\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.3677\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.2479\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.3615\t Accuracy 0.8800\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.2437\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.2617\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.2234\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.2031\t Accuracy 0.9300\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2713\t Average training accuracy 0.9233\n",
      "Epoch [17]\t Average validation loss 0.2482\t Average validation accuracy 0.9336\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.2121\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.1174\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.2780\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.2392\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.3288\t Accuracy 0.8600\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.1892\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.3451\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.2270\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.2211\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.3953\t Accuracy 0.8700\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.2101\t Accuracy 0.9600\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2697\t Average training accuracy 0.9248\n",
      "Epoch [18]\t Average validation loss 0.2330\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.2123\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.3442\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.4301\t Accuracy 0.9000\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.3257\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.2580\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.3716\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.1559\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.3502\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.3915\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.2053\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.1448\t Accuracy 0.9500\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2677\t Average training accuracy 0.9250\n",
      "Epoch [19]\t Average validation loss 0.2445\t Average validation accuracy 0.9344\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.2296\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.1732\t Accuracy 0.9500\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.3369\t Accuracy 0.8900\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.1512\t Accuracy 0.9500\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.3685\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.3584\t Accuracy 0.9000\n",
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.5184\t Accuracy 0.8900\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.1510\t Accuracy 0.9500\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.4039\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.3738\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.1407\t Accuracy 0.9500\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2670\t Average training accuracy 0.9243\n",
      "Epoch [20]\t Average validation loss 0.2437\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.3302\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.3701\t Accuracy 0.8900\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.1920\t Accuracy 0.9600\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.2301\t Accuracy 0.9500\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.2844\t Accuracy 0.8800\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.3911\t Accuracy 0.8900\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.2016\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.3919\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.2372\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.1907\t Accuracy 0.9500\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.5660\t Accuracy 0.8800\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2663\t Average training accuracy 0.9251\n",
      "Epoch [21]\t Average validation loss 0.2547\t Average validation accuracy 0.9302\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.2365\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.1008\t Accuracy 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.2417\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.1541\t Accuracy 0.9700\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.5365\t Accuracy 0.8800\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.3244\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.1728\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.2446\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.2075\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.3070\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.1627\t Accuracy 0.9500\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2664\t Average training accuracy 0.9247\n",
      "Epoch [22]\t Average validation loss 0.2323\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.4466\t Accuracy 0.8700\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.3024\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.4678\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.2175\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.3508\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.3698\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.1770\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.2499\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.2736\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.2028\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.1929\t Accuracy 0.9700\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2676\t Average training accuracy 0.9237\n",
      "Epoch [23]\t Average validation loss 0.2410\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.2039\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.3440\t Accuracy 0.8900\n",
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.1601\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.3069\t Accuracy 0.9100\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.1606\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.3370\t Accuracy 0.8900\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.1686\t Accuracy 0.9600\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.1614\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.1813\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.4065\t Accuracy 0.8700\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.2399\t Accuracy 0.9400\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2652\t Average training accuracy 0.9251\n",
      "Epoch [24]\t Average validation loss 0.2503\t Average validation accuracy 0.9302\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.1095\t Accuracy 0.9600\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.2206\t Accuracy 0.8800\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.2341\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.3192\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.4488\t Accuracy 0.8800\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.3148\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.2869\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.2935\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.5228\t Accuracy 0.8500\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.3081\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.1504\t Accuracy 0.9700\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2648\t Average training accuracy 0.9246\n",
      "Epoch [25]\t Average validation loss 0.2411\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.2383\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.4468\t Accuracy 0.8400\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.2331\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.1672\t Accuracy 0.9600\n",
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.1697\t Accuracy 0.9700\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.1648\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.1705\t Accuracy 0.9700\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.2672\t Accuracy 0.9600\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.1056\t Accuracy 0.9700\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.3013\t Accuracy 0.9000\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.3404\t Accuracy 0.9100\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2640\t Average training accuracy 0.9256\n",
      "Epoch [26]\t Average validation loss 0.2383\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.1112\t Accuracy 0.9700\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.4055\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.3975\t Accuracy 0.9000\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.2610\t Accuracy 0.9000\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.1122\t Accuracy 0.9700\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.3130\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.2457\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.1697\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.1666\t Accuracy 0.9700\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.2688\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.4686\t Accuracy 0.9000\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2630\t Average training accuracy 0.9259\n",
      "Epoch [27]\t Average validation loss 0.2444\t Average validation accuracy 0.9354\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.3428\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.1803\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.2698\t Accuracy 0.8900\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.1715\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.3076\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.1766\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.1229\t Accuracy 0.9600\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.2358\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.3291\t Accuracy 0.8900\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.1824\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.2622\t Accuracy 0.9300\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2643\t Average training accuracy 0.9245\n",
      "Epoch [28]\t Average validation loss 0.2462\t Average validation accuracy 0.9354\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.2031\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.2242\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.1799\t Accuracy 0.9600\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.4151\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.2153\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.2095\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.1696\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.2573\t Accuracy 0.8900\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.3135\t Accuracy 0.8900\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.3416\t Accuracy 0.8800\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.2301\t Accuracy 0.9300\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2645\t Average training accuracy 0.9256\n",
      "Epoch [29]\t Average validation loss 0.2413\t Average validation accuracy 0.9348\n",
      "\n",
      "spend time: 45.29905366897583\n",
      "Final test accuracy 0.9214\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.5010\t Accuracy 0.0500\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.4013\t Accuracy 0.8400\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.4164\t Accuracy 0.8500\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.4069\t Accuracy 0.8900\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.4576\t Accuracy 0.8500\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.2182\t Accuracy 0.9400\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.1207\t Accuracy 0.9700\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.4442\t Accuracy 0.8900\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.3672\t Accuracy 0.9400\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.4468\t Accuracy 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.2364\t Accuracy 0.9300\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3839\t Average training accuracy 0.8873\n",
      "Epoch [0]\t Average validation loss 0.2564\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.2529\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.3266\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.2371\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.3015\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.3711\t Accuracy 0.8900\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.2949\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.2034\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.3314\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.2595\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.4161\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.2674\t Accuracy 0.9100\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3131\t Average training accuracy 0.9117\n",
      "Epoch [1]\t Average validation loss 0.2422\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.2143\t Accuracy 0.9500\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.4717\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.3809\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.2249\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.3397\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.3617\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.3647\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.4189\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.4012\t Accuracy 0.8600\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.2108\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.2755\t Accuracy 0.9100\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3049\t Average training accuracy 0.9145\n",
      "Epoch [2]\t Average validation loss 0.2435\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.1696\t Accuracy 0.9700\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.2461\t Accuracy 0.9400\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.4327\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.3453\t Accuracy 0.8500\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.4043\t Accuracy 0.8800\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.2269\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.2640\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.2831\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.1937\t Accuracy 0.9400\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.3716\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.3705\t Accuracy 0.9500\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2970\t Average training accuracy 0.9165\n",
      "Epoch [3]\t Average validation loss 0.2361\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.3672\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.2311\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.3059\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.4375\t Accuracy 0.8600\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.2108\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.2207\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.4133\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.5181\t Accuracy 0.8700\n",
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.3643\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.1295\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.1893\t Accuracy 0.9200\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2949\t Average training accuracy 0.9170\n",
      "Epoch [4]\t Average validation loss 0.2460\t Average validation accuracy 0.9314\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.3121\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.2470\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.2844\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.1202\t Accuracy 0.9700\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.3431\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.3636\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.2395\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.2276\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.2538\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.1729\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.3426\t Accuracy 0.9100\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2936\t Average training accuracy 0.9165\n",
      "Epoch [5]\t Average validation loss 0.2388\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.2992\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.2458\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.4668\t Accuracy 0.8700\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.1914\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.2344\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.3772\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.3689\t Accuracy 0.8800\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.1804\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.1506\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.2474\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.2041\t Accuracy 0.9700\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2883\t Average training accuracy 0.9192\n",
      "Epoch [6]\t Average validation loss 0.2447\t Average validation accuracy 0.9318\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.2792\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.2083\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.4288\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.1785\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.2160\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.1333\t Accuracy 0.9500\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.1864\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.4072\t Accuracy 0.8800\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.2076\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.2439\t Accuracy 0.9300\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.3750\t Accuracy 0.9000\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2859\t Average training accuracy 0.9188\n",
      "Epoch [7]\t Average validation loss 0.2393\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.2732\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.3934\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.1687\t Accuracy 0.9600\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.2926\t Accuracy 0.8900\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.2577\t Accuracy 0.9200\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.3188\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.2516\t Accuracy 0.8700\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.2862\t Accuracy 0.8600\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.4357\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.2083\t Accuracy 0.9400\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.3468\t Accuracy 0.8900\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2841\t Average training accuracy 0.9194\n",
      "Epoch [8]\t Average validation loss 0.2455\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.2938\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.5179\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.3475\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.2467\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.3199\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.2709\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.3978\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.4603\t Accuracy 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.2975\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.3120\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.3565\t Accuracy 0.9100\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2829\t Average training accuracy 0.9201\n",
      "Epoch [9]\t Average validation loss 0.2363\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.2783\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.3559\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.2849\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.1663\t Accuracy 0.9700\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.2504\t Accuracy 0.9500\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.1605\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.4479\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.2500\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.2881\t Accuracy 0.8800\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.3911\t Accuracy 0.8900\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.2134\t Accuracy 0.9200\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2805\t Average training accuracy 0.9214\n",
      "Epoch [10]\t Average validation loss 0.2446\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.1841\t Accuracy 0.9600\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.2938\t Accuracy 0.8800\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.3088\t Accuracy 0.8900\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.4569\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.2522\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.1718\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.2840\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.2240\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.3686\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.1639\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.4709\t Accuracy 0.9200\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2781\t Average training accuracy 0.9222\n",
      "Epoch [11]\t Average validation loss 0.2386\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.4669\t Accuracy 0.8600\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.1541\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.1542\t Accuracy 0.9500\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.2787\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.4362\t Accuracy 0.8500\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.1576\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.2233\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.3680\t Accuracy 0.8900\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.2713\t Accuracy 0.8700\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.3019\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.3600\t Accuracy 0.9000\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2808\t Average training accuracy 0.9210\n",
      "Epoch [12]\t Average validation loss 0.2488\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.3558\t Accuracy 0.8700\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.3696\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.2925\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.2508\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.2847\t Accuracy 0.9700\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.2186\t Accuracy 0.9500\n",
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.3070\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.2518\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.3850\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.4224\t Accuracy 0.8900\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.2164\t Accuracy 0.9400\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2769\t Average training accuracy 0.9219\n",
      "Epoch [13]\t Average validation loss 0.2438\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.1501\t Accuracy 0.9600\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.1773\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.3220\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.2667\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.1895\t Accuracy 0.9600\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.1931\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.1780\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.3271\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.4467\t Accuracy 0.8800\n",
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.3255\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.4326\t Accuracy 0.8600\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2783\t Average training accuracy 0.9213\n",
      "Epoch [14]\t Average validation loss 0.2574\t Average validation accuracy 0.9280\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.2567\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.2545\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.3024\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.1298\t Accuracy 0.9600\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.7003\t Accuracy 0.8800\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.2879\t Accuracy 0.8900\n",
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.2696\t Accuracy 0.8800\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.1955\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.2309\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.3227\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.1974\t Accuracy 0.9600\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2773\t Average training accuracy 0.9205\n",
      "Epoch [15]\t Average validation loss 0.2422\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.3190\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.2697\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.3127\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.1753\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.1815\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.1829\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.2535\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.2037\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.2061\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.1930\t Accuracy 0.9400\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.3011\t Accuracy 0.9200\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2745\t Average training accuracy 0.9227\n",
      "Epoch [16]\t Average validation loss 0.2410\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.3123\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.2721\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.1703\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.2235\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.4647\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.2995\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.2071\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.2878\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.1028\t Accuracy 0.9800\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.2060\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.1866\t Accuracy 0.9300\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2742\t Average training accuracy 0.9232\n",
      "Epoch [17]\t Average validation loss 0.2437\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.2752\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.2401\t Accuracy 0.9000\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.2443\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.1894\t Accuracy 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.2157\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.3445\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.2332\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.1184\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.5562\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.1315\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.4121\t Accuracy 0.8800\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2728\t Average training accuracy 0.9224\n",
      "Epoch [18]\t Average validation loss 0.2680\t Average validation accuracy 0.9292\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.3999\t Accuracy 0.9000\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.3814\t Accuracy 0.9000\n",
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.2161\t Accuracy 0.8900\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.2817\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.3345\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.1576\t Accuracy 0.9600\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.1785\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.2837\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.3797\t Accuracy 0.8800\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.3117\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.2543\t Accuracy 0.9200\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2721\t Average training accuracy 0.9228\n",
      "Epoch [19]\t Average validation loss 0.2429\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.3254\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.2162\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.3496\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.2077\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.2052\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.1479\t Accuracy 0.9600\n",
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.1421\t Accuracy 0.9600\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.2982\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.3166\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.1783\t Accuracy 0.9600\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.3684\t Accuracy 0.8900\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2709\t Average training accuracy 0.9233\n",
      "Epoch [20]\t Average validation loss 0.2653\t Average validation accuracy 0.9282\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.1893\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.2404\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.3298\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.2233\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.2145\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.3361\t Accuracy 0.8800\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.1664\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.2712\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.2510\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.4161\t Accuracy 0.8900\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.3379\t Accuracy 0.9300\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2701\t Average training accuracy 0.9237\n",
      "Epoch [21]\t Average validation loss 0.2607\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.2359\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.2077\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.2366\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.2730\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.2639\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.1924\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.1620\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.3378\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.1329\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.4761\t Accuracy 0.8700\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.3359\t Accuracy 0.8800\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2692\t Average training accuracy 0.9231\n",
      "Epoch [22]\t Average validation loss 0.2585\t Average validation accuracy 0.9280\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.1820\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.3844\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.1742\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.4650\t Accuracy 0.8900\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.2443\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.2439\t Accuracy 0.8900\n",
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.1783\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.1616\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.1622\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.2721\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.3478\t Accuracy 0.9100\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2683\t Average training accuracy 0.9248\n",
      "Epoch [23]\t Average validation loss 0.2505\t Average validation accuracy 0.9330\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.4782\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.4292\t Accuracy 0.8800\n",
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.1797\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.2150\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.6029\t Accuracy 0.8500\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.2031\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.2604\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.1874\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.4723\t Accuracy 0.9000\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.1933\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.2393\t Accuracy 0.9100\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2711\t Average training accuracy 0.9228\n",
      "Epoch [24]\t Average validation loss 0.2475\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.1742\t Accuracy 0.9600\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.1791\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.4212\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.4007\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.3478\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.2856\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.2085\t Accuracy 0.9000\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.3353\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.3042\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.3216\t Accuracy 0.9000\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.4470\t Accuracy 0.8500\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2713\t Average training accuracy 0.9238\n",
      "Epoch [25]\t Average validation loss 0.2414\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.1977\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.3747\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.5989\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.5132\t Accuracy 0.8700\n",
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.3979\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.2667\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.2443\t Accuracy 0.9000\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.2865\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.3429\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.2666\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.1957\t Accuracy 0.9400\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2683\t Average training accuracy 0.9246\n",
      "Epoch [26]\t Average validation loss 0.2444\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.1956\t Accuracy 0.9600\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.3347\t Accuracy 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.1099\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.1750\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.2105\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.4680\t Accuracy 0.8600\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.3905\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.2809\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.1672\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.1984\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.2566\t Accuracy 0.9200\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2676\t Average training accuracy 0.9249\n",
      "Epoch [27]\t Average validation loss 0.2450\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.2142\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.1331\t Accuracy 0.9600\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.2042\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.1753\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.3827\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.2757\t Accuracy 0.9600\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.4087\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.2952\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.1205\t Accuracy 0.9700\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.2603\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.3403\t Accuracy 0.9100\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2664\t Average training accuracy 0.9247\n",
      "Epoch [28]\t Average validation loss 0.2375\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.1757\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.1265\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.2842\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.1675\t Accuracy 0.9600\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.4164\t Accuracy 0.8700\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.3712\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.2574\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.2598\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.2990\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.4507\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.1627\t Accuracy 0.9500\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2682\t Average training accuracy 0.9241\n",
      "Epoch [29]\t Average validation loss 0.2570\t Average validation accuracy 0.9306\n",
      "\n",
      "spend time: 45.69915294647217\n",
      "Final test accuracy 0.9181\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.5254\t Accuracy 0.0600\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.2512\t Accuracy 0.9400\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.3978\t Accuracy 0.9000\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.3289\t Accuracy 0.8700\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.4187\t Accuracy 0.9200\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.3558\t Accuracy 0.9200\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.3122\t Accuracy 0.8900\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.5010\t Accuracy 0.8800\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.3690\t Accuracy 0.8500\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.3016\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.3374\t Accuracy 0.8700\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3842\t Average training accuracy 0.8885\n",
      "Epoch [0]\t Average validation loss 0.2531\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.5139\t Accuracy 0.8900\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.3243\t Accuracy 0.9400\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.4781\t Accuracy 0.8600\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.1423\t Accuracy 0.9500\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.2501\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.2446\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.2750\t Accuracy 0.9100\n",
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.3989\t Accuracy 0.8600\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.4899\t Accuracy 0.8900\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.3115\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.5066\t Accuracy 0.8700\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3177\t Average training accuracy 0.9094\n",
      "Epoch [1]\t Average validation loss 0.2435\t Average validation accuracy 0.9318\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.2888\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.4608\t Accuracy 0.9100\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.2609\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.3532\t Accuracy 0.9500\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.4586\t Accuracy 0.8800\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.1806\t Accuracy 0.9600\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.2312\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.1793\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.4262\t Accuracy 0.8800\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.2782\t Accuracy 0.9100\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.3227\t Accuracy 0.8800\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3083\t Average training accuracy 0.9136\n",
      "Epoch [2]\t Average validation loss 0.2519\t Average validation accuracy 0.9280\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.3316\t Accuracy 0.9000\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.2123\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.1600\t Accuracy 0.9600\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.2978\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.5438\t Accuracy 0.8700\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.2777\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.2110\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.2739\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.3332\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.2455\t Accuracy 0.9500\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.2910\t Accuracy 0.9300\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2998\t Average training accuracy 0.9169\n",
      "Epoch [3]\t Average validation loss 0.2429\t Average validation accuracy 0.9314\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.2083\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.2326\t Accuracy 0.8900\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.4904\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.2677\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.2400\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.3731\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.2617\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.3197\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.1834\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.2373\t Accuracy 0.9500\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.2434\t Accuracy 0.9300\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2985\t Average training accuracy 0.9178\n",
      "Epoch [4]\t Average validation loss 0.2333\t Average validation accuracy 0.9376\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.1856\t Accuracy 0.9400\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.2644\t Accuracy 0.9700\n",
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.2636\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.3141\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.1911\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.2255\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.3421\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.2236\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.4814\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.3939\t Accuracy 0.8800\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.1448\t Accuracy 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5]\t Average training loss 0.2923\t Average training accuracy 0.9181\n",
      "Epoch [5]\t Average validation loss 0.2510\t Average validation accuracy 0.9292\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.3988\t Accuracy 0.8700\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.4683\t Accuracy 0.8800\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.3116\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.2031\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.2372\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.1790\t Accuracy 0.9500\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.3438\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.3741\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.4079\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.3063\t Accuracy 0.9200\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.1521\t Accuracy 0.9400\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2910\t Average training accuracy 0.9178\n",
      "Epoch [6]\t Average validation loss 0.2495\t Average validation accuracy 0.9332\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.4589\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.1683\t Accuracy 0.9500\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.2265\t Accuracy 0.9500\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.4051\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.3258\t Accuracy 0.8700\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.6368\t Accuracy 0.8500\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.3494\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.2811\t Accuracy 0.8900\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.2962\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.4604\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.3531\t Accuracy 0.9100\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2890\t Average training accuracy 0.9191\n",
      "Epoch [7]\t Average validation loss 0.2500\t Average validation accuracy 0.9298\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.2746\t Accuracy 0.9200\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.1623\t Accuracy 0.9400\n",
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.4063\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.1436\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.3250\t Accuracy 0.8900\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.3998\t Accuracy 0.8700\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.2333\t Accuracy 0.9600\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.3616\t Accuracy 0.9400\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.3334\t Accuracy 0.9100\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.3776\t Accuracy 0.9200\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.3172\t Accuracy 0.9200\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2883\t Average training accuracy 0.9197\n",
      "Epoch [8]\t Average validation loss 0.2505\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.2095\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.3884\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.2065\t Accuracy 0.9500\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.2973\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.2042\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.1648\t Accuracy 0.9500\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.3643\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.1991\t Accuracy 0.9500\n",
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.1170\t Accuracy 0.9400\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.1395\t Accuracy 0.9600\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.2787\t Accuracy 0.9000\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2863\t Average training accuracy 0.9199\n",
      "Epoch [9]\t Average validation loss 0.2526\t Average validation accuracy 0.9276\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.2499\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.3422\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.2684\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.3642\t Accuracy 0.8900\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.1965\t Accuracy 0.9400\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.1413\t Accuracy 0.9600\n",
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.2765\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.2762\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.2965\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.3710\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.4400\t Accuracy 0.8900\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2873\t Average training accuracy 0.9184\n",
      "Epoch [10]\t Average validation loss 0.2482\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.2664\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.4500\t Accuracy 0.8500\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.2085\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.1898\t Accuracy 0.9600\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.2314\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.3810\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.2546\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.4910\t Accuracy 0.8900\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.3115\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.2412\t Accuracy 0.9400\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.4393\t Accuracy 0.8700\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2855\t Average training accuracy 0.9203\n",
      "Epoch [11]\t Average validation loss 0.2543\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.1233\t Accuracy 0.9600\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.1697\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.3351\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.4869\t Accuracy 0.8800\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.1936\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.5174\t Accuracy 0.9000\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.2159\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.1522\t Accuracy 0.9300\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.1967\t Accuracy 0.9400\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.3148\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.2831\t Accuracy 0.9100\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2830\t Average training accuracy 0.9206\n",
      "Epoch [12]\t Average validation loss 0.2540\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.2631\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.3692\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.2820\t Accuracy 0.8800\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.2569\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.2872\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.3901\t Accuracy 0.8800\n",
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.2112\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.1761\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.3369\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.2695\t Accuracy 0.9400\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.2845\t Accuracy 0.8900\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2813\t Average training accuracy 0.9207\n",
      "Epoch [13]\t Average validation loss 0.2861\t Average validation accuracy 0.9206\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.4486\t Accuracy 0.8900\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.3349\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.4599\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.3159\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.2114\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.2470\t Accuracy 0.9300\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.3468\t Accuracy 0.8900\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.3209\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.2048\t Accuracy 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.3288\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.3127\t Accuracy 0.8900\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2787\t Average training accuracy 0.9213\n",
      "Epoch [14]\t Average validation loss 0.2519\t Average validation accuracy 0.9310\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.1187\t Accuracy 0.9800\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.2169\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.2038\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.1671\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.1804\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.2124\t Accuracy 0.8900\n",
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.1498\t Accuracy 0.9600\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.4028\t Accuracy 0.8800\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.3316\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.1215\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.2738\t Accuracy 0.9000\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2799\t Average training accuracy 0.9218\n",
      "Epoch [15]\t Average validation loss 0.2887\t Average validation accuracy 0.9166\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.3092\t Accuracy 0.8800\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.2537\t Accuracy 0.8900\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.2307\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.3892\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.2401\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.2447\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.4376\t Accuracy 0.8800\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.1997\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.3501\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.1811\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.4466\t Accuracy 0.8600\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2778\t Average training accuracy 0.9212\n",
      "Epoch [16]\t Average validation loss 0.2452\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.1849\t Accuracy 0.9500\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.2233\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.2847\t Accuracy 0.9600\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.3727\t Accuracy 0.8700\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.2160\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.3033\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.2732\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.2109\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.3946\t Accuracy 0.8800\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.2396\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.2044\t Accuracy 0.9500\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2781\t Average training accuracy 0.9215\n",
      "Epoch [17]\t Average validation loss 0.2697\t Average validation accuracy 0.9262\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.2407\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.2917\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.2428\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.4069\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.5628\t Accuracy 0.8800\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.1512\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.1325\t Accuracy 0.9700\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.1039\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.3233\t Accuracy 0.9300\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.1235\t Accuracy 0.9600\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.3952\t Accuracy 0.9000\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2782\t Average training accuracy 0.9232\n",
      "Epoch [18]\t Average validation loss 0.2631\t Average validation accuracy 0.9268\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.1710\t Accuracy 0.9700\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.3256\t Accuracy 0.9000\n",
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.3445\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.1903\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.3670\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.2505\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.6188\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.1287\t Accuracy 0.9700\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.3034\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.2877\t Accuracy 0.9300\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.5839\t Accuracy 0.8400\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2802\t Average training accuracy 0.9210\n",
      "Epoch [19]\t Average validation loss 0.2417\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.2975\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.3507\t Accuracy 0.8900\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.2757\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.2636\t Accuracy 0.9000\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.2906\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.1254\t Accuracy 0.9500\n",
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.2602\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.4413\t Accuracy 0.8900\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.2811\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.3908\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.2539\t Accuracy 0.9000\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2773\t Average training accuracy 0.9213\n",
      "Epoch [20]\t Average validation loss 0.2472\t Average validation accuracy 0.9310\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.2626\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.1268\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.2049\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.2282\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.3412\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.2818\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.2222\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.1449\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.1298\t Accuracy 0.9500\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.1211\t Accuracy 0.9600\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.2369\t Accuracy 0.9400\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2762\t Average training accuracy 0.9229\n",
      "Epoch [21]\t Average validation loss 0.2624\t Average validation accuracy 0.9330\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.4350\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.3987\t Accuracy 0.9000\n",
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.1339\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.4056\t Accuracy 0.8300\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.5025\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.2842\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.2792\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.3421\t Accuracy 0.8900\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.4202\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.3237\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.1311\t Accuracy 0.9800\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2766\t Average training accuracy 0.9214\n",
      "Epoch [22]\t Average validation loss 0.2420\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.2058\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.3285\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.1070\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.1638\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.4797\t Accuracy 0.8700\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.1892\t Accuracy 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.3321\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.2999\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.4213\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.1502\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.1625\t Accuracy 0.9600\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2738\t Average training accuracy 0.9221\n",
      "Epoch [23]\t Average validation loss 0.2695\t Average validation accuracy 0.9236\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.3774\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.3112\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.1154\t Accuracy 0.9700\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.2499\t Accuracy 0.9500\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.4481\t Accuracy 0.8800\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.3782\t Accuracy 0.9000\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.2736\t Accuracy 0.9100\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.2027\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.4397\t Accuracy 0.8900\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.3521\t Accuracy 0.8800\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.2555\t Accuracy 0.9300\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2758\t Average training accuracy 0.9221\n",
      "Epoch [24]\t Average validation loss 0.2651\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.2821\t Accuracy 0.9000\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.2346\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.2453\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.2483\t Accuracy 0.9000\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.1854\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.3516\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.1814\t Accuracy 0.9400\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.1524\t Accuracy 0.9600\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.3171\t Accuracy 0.8800\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.2959\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.1543\t Accuracy 0.9500\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2737\t Average training accuracy 0.9229\n",
      "Epoch [25]\t Average validation loss 0.2577\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.1207\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.3036\t Accuracy 0.8900\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.3745\t Accuracy 0.8900\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.1634\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.2717\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.1239\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.3115\t Accuracy 0.9000\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.1045\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.2969\t Accuracy 0.8900\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.4565\t Accuracy 0.9100\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.1152\t Accuracy 0.9600\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2715\t Average training accuracy 0.9229\n",
      "Epoch [26]\t Average validation loss 0.2548\t Average validation accuracy 0.9314\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.5551\t Accuracy 0.8600\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.2833\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.3506\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.1474\t Accuracy 0.9600\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.2765\t Accuracy 0.9100\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.2509\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.1420\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.2964\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.3040\t Accuracy 0.8900\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.1841\t Accuracy 0.9800\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.2149\t Accuracy 0.9400\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2778\t Average training accuracy 0.9223\n",
      "Epoch [27]\t Average validation loss 0.2457\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.1970\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.1771\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.3566\t Accuracy 0.8900\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.2500\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.5081\t Accuracy 0.8600\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.1833\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.2515\t Accuracy 0.9000\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.2333\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.1907\t Accuracy 0.9600\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.4282\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.1989\t Accuracy 0.9400\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2748\t Average training accuracy 0.9217\n",
      "Epoch [28]\t Average validation loss 0.2637\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.3286\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.3397\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.2023\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.3026\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.3633\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.1824\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.4824\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.2850\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.2064\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.1921\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.2426\t Accuracy 0.9100\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2737\t Average training accuracy 0.9230\n",
      "Epoch [29]\t Average validation loss 0.2613\t Average validation accuracy 0.9328\n",
      "\n",
      "spend time: 44.08819532394409\n",
      "Final test accuracy 0.9178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate_list = [0.01,0.03,0.05,0.07,0.1,0.13,0.15,0.17,0.19]\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "for i in range(len(learning_rate_list)):\n",
    "    cfg = {\n",
    "        'data_root': 'data',\n",
    "        'max_epoch': 30,\n",
    "        'batch_size': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'momentum': 0.9,\n",
    "        'display_freq': 50,\n",
    "    }\n",
    "    cfg['learning_rate'] = learning_rate_list[i]\n",
    "    runner = Solver(cfg)\n",
    "    s = time.time()\n",
    "    loss2, acc2 = runner.train()\n",
    "    e = time.time()\n",
    "    print(\"spend time:\",e-s)\n",
    "    test_loss, test_acc = runner.test()\n",
    "    print('Final test accuracy {:.4f}\\n'.format(test_acc))\n",
    "    acc_list.append(test_acc)\n",
    "    loss_list.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9223, 0.9251, 0.9216000000000001, 0.9236000000000001, 0.9167000000000001, 0.9189000000000002, 0.9214000000000003, 0.9180999999999999, 0.9178000000000002]\n"
     ]
    }
   ],
   "source": [
    "print(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27047012960693206, 0.2712488449346456, 0.27629792103158546, 0.27764285684187034, 0.2939353095649948, 0.3028752801983608, 0.2922028568221604, 0.3020037518337462, 0.3157371926257847]\n"
     ]
    }
   ],
   "source": [
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "xdata = learning_rate_list\n",
    "ydata = acc_list\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('acc_list')\n",
    "#xdata:x轴数据，ydata:y轴数据\n",
    "plt.plot(xdata,ydata, '--or')\n",
    "_ = plt.xticks(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][550]\t Training Loss 2.3467\t Accuracy 0.1300\n",
      "Epoch [0][30]\t Batch [50][550]\t Training Loss 0.8415\t Accuracy 0.7500\n",
      "Epoch [0][30]\t Batch [100][550]\t Training Loss 0.5343\t Accuracy 0.8800\n",
      "Epoch [0][30]\t Batch [150][550]\t Training Loss 0.5186\t Accuracy 0.8400\n",
      "Epoch [0][30]\t Batch [200][550]\t Training Loss 0.4655\t Accuracy 0.8600\n",
      "Epoch [0][30]\t Batch [250][550]\t Training Loss 0.5104\t Accuracy 0.8800\n",
      "Epoch [0][30]\t Batch [300][550]\t Training Loss 0.3603\t Accuracy 0.9300\n",
      "Epoch [0][30]\t Batch [350][550]\t Training Loss 0.4658\t Accuracy 0.8600\n",
      "Epoch [0][30]\t Batch [400][550]\t Training Loss 0.3748\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [450][550]\t Training Loss 0.3502\t Accuracy 0.9100\n",
      "Epoch [0][30]\t Batch [500][550]\t Training Loss 0.5792\t Accuracy 0.8700\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5730\t Average training accuracy 0.8448\n",
      "Epoch [0]\t Average validation loss 0.3146\t Average validation accuracy 0.9174\n",
      "\n",
      "Epoch [1][30]\t Batch [0][550]\t Training Loss 0.2895\t Accuracy 0.9500\n",
      "Epoch [1][30]\t Batch [50][550]\t Training Loss 0.4281\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [100][550]\t Training Loss 0.3144\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [150][550]\t Training Loss 0.4447\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [200][550]\t Training Loss 0.3413\t Accuracy 0.9300\n",
      "Epoch [1][30]\t Batch [250][550]\t Training Loss 0.3510\t Accuracy 0.8900\n",
      "Epoch [1][30]\t Batch [300][550]\t Training Loss 0.3193\t Accuracy 0.9200\n",
      "Epoch [1][30]\t Batch [350][550]\t Training Loss 0.2421\t Accuracy 0.9400\n",
      "Epoch [1][30]\t Batch [400][550]\t Training Loss 0.3522\t Accuracy 0.9000\n",
      "Epoch [1][30]\t Batch [450][550]\t Training Loss 0.4422\t Accuracy 0.8600\n",
      "Epoch [1][30]\t Batch [500][550]\t Training Loss 0.2703\t Accuracy 0.9200\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3736\t Average training accuracy 0.8961\n",
      "Epoch [1]\t Average validation loss 0.2789\t Average validation accuracy 0.9274\n",
      "\n",
      "Epoch [2][30]\t Batch [0][550]\t Training Loss 0.4082\t Accuracy 0.9100\n",
      "Epoch [2][30]\t Batch [50][550]\t Training Loss 0.3686\t Accuracy 0.9200\n",
      "Epoch [2][30]\t Batch [100][550]\t Training Loss 0.4564\t Accuracy 0.8300\n",
      "Epoch [2][30]\t Batch [150][550]\t Training Loss 0.3900\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [200][550]\t Training Loss 0.5001\t Accuracy 0.8800\n",
      "Epoch [2][30]\t Batch [250][550]\t Training Loss 0.3293\t Accuracy 0.9300\n",
      "Epoch [2][30]\t Batch [300][550]\t Training Loss 0.5442\t Accuracy 0.8600\n",
      "Epoch [2][30]\t Batch [350][550]\t Training Loss 0.2914\t Accuracy 0.9400\n",
      "Epoch [2][30]\t Batch [400][550]\t Training Loss 0.3883\t Accuracy 0.9100\n",
      "Epoch [2][30]\t Batch [450][550]\t Training Loss 0.3492\t Accuracy 0.8900\n",
      "Epoch [2][30]\t Batch [500][550]\t Training Loss 0.4134\t Accuracy 0.8800\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3437\t Average training accuracy 0.9042\n",
      "Epoch [2]\t Average validation loss 0.2628\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [3][30]\t Batch [0][550]\t Training Loss 0.3362\t Accuracy 0.8800\n",
      "Epoch [3][30]\t Batch [50][550]\t Training Loss 0.3349\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [100][550]\t Training Loss 0.2802\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [150][550]\t Training Loss 0.2858\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [200][550]\t Training Loss 0.4068\t Accuracy 0.8900\n",
      "Epoch [3][30]\t Batch [250][550]\t Training Loss 0.3976\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [300][550]\t Training Loss 0.2857\t Accuracy 0.9300\n",
      "Epoch [3][30]\t Batch [350][550]\t Training Loss 0.4060\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [400][550]\t Training Loss 0.4393\t Accuracy 0.8500\n",
      "Epoch [3][30]\t Batch [450][550]\t Training Loss 0.2478\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [500][550]\t Training Loss 0.4084\t Accuracy 0.8700\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3279\t Average training accuracy 0.9079\n",
      "Epoch [3]\t Average validation loss 0.2551\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [4][30]\t Batch [0][550]\t Training Loss 0.2700\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [50][550]\t Training Loss 0.3842\t Accuracy 0.9000\n",
      "Epoch [4][30]\t Batch [100][550]\t Training Loss 0.3803\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [150][550]\t Training Loss 0.2395\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [200][550]\t Training Loss 0.2633\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [250][550]\t Training Loss 0.2413\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [300][550]\t Training Loss 0.4431\t Accuracy 0.8700\n",
      "Epoch [4][30]\t Batch [350][550]\t Training Loss 0.2811\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [400][550]\t Training Loss 0.2927\t Accuracy 0.9100\n",
      "Epoch [4][30]\t Batch [450][550]\t Training Loss 0.3654\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [500][550]\t Training Loss 0.2474\t Accuracy 0.9200\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3175\t Average training accuracy 0.9117\n",
      "Epoch [4]\t Average validation loss 0.2477\t Average validation accuracy 0.9320\n",
      "\n",
      "Epoch [5][30]\t Batch [0][550]\t Training Loss 0.3450\t Accuracy 0.8900\n",
      "Epoch [5][30]\t Batch [50][550]\t Training Loss 0.3359\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [100][550]\t Training Loss 0.2990\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [150][550]\t Training Loss 0.2167\t Accuracy 0.9400\n",
      "Epoch [5][30]\t Batch [200][550]\t Training Loss 0.2314\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [250][550]\t Training Loss 0.3197\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [300][550]\t Training Loss 0.2746\t Accuracy 0.9500\n",
      "Epoch [5][30]\t Batch [350][550]\t Training Loss 0.1149\t Accuracy 0.9700\n",
      "Epoch [5][30]\t Batch [400][550]\t Training Loss 0.2119\t Accuracy 0.9300\n",
      "Epoch [5][30]\t Batch [450][550]\t Training Loss 0.3908\t Accuracy 0.8900\n",
      "Epoch [5][30]\t Batch [500][550]\t Training Loss 0.2362\t Accuracy 0.9000\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3104\t Average training accuracy 0.9132\n",
      "Epoch [5]\t Average validation loss 0.2456\t Average validation accuracy 0.9314\n",
      "\n",
      "Epoch [6][30]\t Batch [0][550]\t Training Loss 0.3406\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [50][550]\t Training Loss 0.2253\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [100][550]\t Training Loss 0.3192\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [150][550]\t Training Loss 0.3652\t Accuracy 0.8600\n",
      "Epoch [6][30]\t Batch [200][550]\t Training Loss 0.2564\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [250][550]\t Training Loss 0.3168\t Accuracy 0.8900\n",
      "Epoch [6][30]\t Batch [300][550]\t Training Loss 0.2222\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [350][550]\t Training Loss 0.2278\t Accuracy 0.9400\n",
      "Epoch [6][30]\t Batch [400][550]\t Training Loss 0.3317\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [450][550]\t Training Loss 0.2098\t Accuracy 0.9300\n",
      "Epoch [6][30]\t Batch [500][550]\t Training Loss 0.1895\t Accuracy 0.9500\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3047\t Average training accuracy 0.9153\n",
      "Epoch [6]\t Average validation loss 0.2404\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [7][30]\t Batch [0][550]\t Training Loss 0.2258\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [50][550]\t Training Loss 0.2158\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [100][550]\t Training Loss 0.2883\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [150][550]\t Training Loss 0.2601\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [200][550]\t Training Loss 0.1984\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [250][550]\t Training Loss 0.1588\t Accuracy 0.9500\n",
      "Epoch [7][30]\t Batch [300][550]\t Training Loss 0.2743\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [350][550]\t Training Loss 0.2901\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [400][550]\t Training Loss 0.2765\t Accuracy 0.9100\n",
      "Epoch [7][30]\t Batch [450][550]\t Training Loss 0.2645\t Accuracy 0.9400\n",
      "Epoch [7][30]\t Batch [500][550]\t Training Loss 0.2659\t Accuracy 0.9300\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3001\t Average training accuracy 0.9163\n",
      "Epoch [7]\t Average validation loss 0.2375\t Average validation accuracy 0.9344\n",
      "\n",
      "Epoch [8][30]\t Batch [0][550]\t Training Loss 0.2217\t Accuracy 0.9400\n",
      "Epoch [8][30]\t Batch [50][550]\t Training Loss 0.2310\t Accuracy 0.9300\n",
      "Epoch [8][30]\t Batch [100][550]\t Training Loss 0.1679\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [150][550]\t Training Loss 0.4570\t Accuracy 0.8700\n",
      "Epoch [8][30]\t Batch [200][550]\t Training Loss 0.1788\t Accuracy 0.9600\n",
      "Epoch [8][30]\t Batch [250][550]\t Training Loss 0.2019\t Accuracy 0.9400\n",
      "Epoch [8][30]\t Batch [300][550]\t Training Loss 0.1668\t Accuracy 0.9700\n",
      "Epoch [8][30]\t Batch [350][550]\t Training Loss 0.2519\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [400][550]\t Training Loss 0.2106\t Accuracy 0.9700\n",
      "Epoch [8][30]\t Batch [450][550]\t Training Loss 0.2376\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [500][550]\t Training Loss 0.2349\t Accuracy 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.2961\t Average training accuracy 0.9174\n",
      "Epoch [8]\t Average validation loss 0.2370\t Average validation accuracy 0.9352\n",
      "\n",
      "Epoch [9][30]\t Batch [0][550]\t Training Loss 0.3250\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [50][550]\t Training Loss 0.2808\t Accuracy 0.9500\n",
      "Epoch [9][30]\t Batch [100][550]\t Training Loss 0.3436\t Accuracy 0.8700\n",
      "Epoch [9][30]\t Batch [150][550]\t Training Loss 0.2182\t Accuracy 0.9500\n",
      "Epoch [9][30]\t Batch [200][550]\t Training Loss 0.4387\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [250][550]\t Training Loss 0.1325\t Accuracy 0.9700\n",
      "Epoch [9][30]\t Batch [300][550]\t Training Loss 0.2421\t Accuracy 0.9200\n",
      "Epoch [9][30]\t Batch [350][550]\t Training Loss 0.1196\t Accuracy 0.9900\n",
      "Epoch [9][30]\t Batch [400][550]\t Training Loss 0.2035\t Accuracy 0.9300\n",
      "Epoch [9][30]\t Batch [450][550]\t Training Loss 0.3949\t Accuracy 0.9000\n",
      "Epoch [9][30]\t Batch [500][550]\t Training Loss 0.2272\t Accuracy 0.9400\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2932\t Average training accuracy 0.9181\n",
      "Epoch [9]\t Average validation loss 0.2334\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [10][30]\t Batch [0][550]\t Training Loss 0.2734\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [50][550]\t Training Loss 0.1866\t Accuracy 0.9600\n",
      "Epoch [10][30]\t Batch [100][550]\t Training Loss 0.4687\t Accuracy 0.8900\n",
      "Epoch [10][30]\t Batch [150][550]\t Training Loss 0.3384\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [200][550]\t Training Loss 0.3461\t Accuracy 0.9100\n",
      "Epoch [10][30]\t Batch [250][550]\t Training Loss 0.3382\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [300][550]\t Training Loss 0.3103\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [350][550]\t Training Loss 0.2855\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [400][550]\t Training Loss 0.3301\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [450][550]\t Training Loss 0.3004\t Accuracy 0.9000\n",
      "Epoch [10][30]\t Batch [500][550]\t Training Loss 0.1920\t Accuracy 0.9600\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2905\t Average training accuracy 0.9188\n",
      "Epoch [10]\t Average validation loss 0.2330\t Average validation accuracy 0.9368\n",
      "\n",
      "Epoch [11][30]\t Batch [0][550]\t Training Loss 0.2776\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [50][550]\t Training Loss 0.2971\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [100][550]\t Training Loss 0.3863\t Accuracy 0.8800\n",
      "Epoch [11][30]\t Batch [150][550]\t Training Loss 0.3019\t Accuracy 0.9100\n",
      "Epoch [11][30]\t Batch [200][550]\t Training Loss 0.3469\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [250][550]\t Training Loss 0.3138\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [300][550]\t Training Loss 0.2778\t Accuracy 0.9200\n",
      "Epoch [11][30]\t Batch [350][550]\t Training Loss 0.3149\t Accuracy 0.8900\n",
      "Epoch [11][30]\t Batch [400][550]\t Training Loss 0.1662\t Accuracy 0.9500\n",
      "Epoch [11][30]\t Batch [450][550]\t Training Loss 0.3222\t Accuracy 0.9300\n",
      "Epoch [11][30]\t Batch [500][550]\t Training Loss 0.3571\t Accuracy 0.9100\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2880\t Average training accuracy 0.9199\n",
      "Epoch [11]\t Average validation loss 0.2305\t Average validation accuracy 0.9370\n",
      "\n",
      "Epoch [12][30]\t Batch [0][550]\t Training Loss 0.3985\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [50][550]\t Training Loss 0.3776\t Accuracy 0.8900\n",
      "Epoch [12][30]\t Batch [100][550]\t Training Loss 0.2218\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [150][550]\t Training Loss 0.3909\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [200][550]\t Training Loss 0.5268\t Accuracy 0.8700\n",
      "Epoch [12][30]\t Batch [250][550]\t Training Loss 0.2098\t Accuracy 0.9500\n",
      "Epoch [12][30]\t Batch [300][550]\t Training Loss 0.2850\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [350][550]\t Training Loss 0.3226\t Accuracy 0.8800\n",
      "Epoch [12][30]\t Batch [400][550]\t Training Loss 0.5409\t Accuracy 0.8400\n",
      "Epoch [12][30]\t Batch [450][550]\t Training Loss 0.2360\t Accuracy 0.9600\n",
      "Epoch [12][30]\t Batch [500][550]\t Training Loss 0.2609\t Accuracy 0.9100\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2859\t Average training accuracy 0.9197\n",
      "Epoch [12]\t Average validation loss 0.2300\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [13][30]\t Batch [0][550]\t Training Loss 0.2473\t Accuracy 0.9500\n",
      "Epoch [13][30]\t Batch [50][550]\t Training Loss 0.1593\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [100][550]\t Training Loss 0.3367\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [150][550]\t Training Loss 0.3988\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [200][550]\t Training Loss 0.3525\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [250][550]\t Training Loss 0.4685\t Accuracy 0.8600\n",
      "Epoch [13][30]\t Batch [300][550]\t Training Loss 0.2947\t Accuracy 0.8900\n",
      "Epoch [13][30]\t Batch [350][550]\t Training Loss 0.2930\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [400][550]\t Training Loss 0.2583\t Accuracy 0.9100\n",
      "Epoch [13][30]\t Batch [450][550]\t Training Loss 0.2840\t Accuracy 0.9300\n",
      "Epoch [13][30]\t Batch [500][550]\t Training Loss 0.2142\t Accuracy 0.9500\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2840\t Average training accuracy 0.9207\n",
      "Epoch [13]\t Average validation loss 0.2294\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [14][30]\t Batch [0][550]\t Training Loss 0.3117\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [50][550]\t Training Loss 0.4236\t Accuracy 0.8900\n",
      "Epoch [14][30]\t Batch [100][550]\t Training Loss 0.2182\t Accuracy 0.9400\n",
      "Epoch [14][30]\t Batch [150][550]\t Training Loss 0.3080\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [200][550]\t Training Loss 0.3058\t Accuracy 0.9100\n",
      "Epoch [14][30]\t Batch [250][550]\t Training Loss 0.2173\t Accuracy 0.9500\n",
      "Epoch [14][30]\t Batch [300][550]\t Training Loss 0.2172\t Accuracy 0.9200\n",
      "Epoch [14][30]\t Batch [350][550]\t Training Loss 0.3739\t Accuracy 0.8800\n",
      "Epoch [14][30]\t Batch [400][550]\t Training Loss 0.3042\t Accuracy 0.8800\n",
      "Epoch [14][30]\t Batch [450][550]\t Training Loss 0.3466\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [500][550]\t Training Loss 0.3053\t Accuracy 0.9200\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2823\t Average training accuracy 0.9213\n",
      "Epoch [14]\t Average validation loss 0.2284\t Average validation accuracy 0.9376\n",
      "\n",
      "Epoch [15][30]\t Batch [0][550]\t Training Loss 0.2624\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [50][550]\t Training Loss 0.3717\t Accuracy 0.8900\n",
      "Epoch [15][30]\t Batch [100][550]\t Training Loss 0.1601\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [150][550]\t Training Loss 0.2486\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [200][550]\t Training Loss 0.2439\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [250][550]\t Training Loss 0.3273\t Accuracy 0.9400\n",
      "Epoch [15][30]\t Batch [300][550]\t Training Loss 0.1649\t Accuracy 0.9500\n",
      "Epoch [15][30]\t Batch [350][550]\t Training Loss 0.3835\t Accuracy 0.9000\n",
      "Epoch [15][30]\t Batch [400][550]\t Training Loss 0.2816\t Accuracy 0.9100\n",
      "Epoch [15][30]\t Batch [450][550]\t Training Loss 0.2785\t Accuracy 0.9300\n",
      "Epoch [15][30]\t Batch [500][550]\t Training Loss 0.3041\t Accuracy 0.9200\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2807\t Average training accuracy 0.9215\n",
      "Epoch [15]\t Average validation loss 0.2271\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [16][30]\t Batch [0][550]\t Training Loss 0.2025\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [50][550]\t Training Loss 0.3884\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [100][550]\t Training Loss 0.2703\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [150][550]\t Training Loss 0.3439\t Accuracy 0.8900\n",
      "Epoch [16][30]\t Batch [200][550]\t Training Loss 0.2861\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [250][550]\t Training Loss 0.1623\t Accuracy 0.9500\n",
      "Epoch [16][30]\t Batch [300][550]\t Training Loss 0.2785\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [350][550]\t Training Loss 0.3919\t Accuracy 0.8900\n",
      "Epoch [16][30]\t Batch [400][550]\t Training Loss 0.3725\t Accuracy 0.8900\n",
      "Epoch [16][30]\t Batch [450][550]\t Training Loss 0.2188\t Accuracy 0.9300\n",
      "Epoch [16][30]\t Batch [500][550]\t Training Loss 0.3299\t Accuracy 0.9100\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2795\t Average training accuracy 0.9220\n",
      "Epoch [16]\t Average validation loss 0.2261\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [17][30]\t Batch [0][550]\t Training Loss 0.2132\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [50][550]\t Training Loss 0.3026\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [100][550]\t Training Loss 0.2415\t Accuracy 0.9600\n",
      "Epoch [17][30]\t Batch [150][550]\t Training Loss 0.2459\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [200][550]\t Training Loss 0.1793\t Accuracy 0.9700\n",
      "Epoch [17][30]\t Batch [250][550]\t Training Loss 0.2590\t Accuracy 0.9300\n",
      "Epoch [17][30]\t Batch [300][550]\t Training Loss 0.2204\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [350][550]\t Training Loss 0.1282\t Accuracy 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][30]\t Batch [400][550]\t Training Loss 0.2986\t Accuracy 0.9200\n",
      "Epoch [17][30]\t Batch [450][550]\t Training Loss 0.2055\t Accuracy 0.9400\n",
      "Epoch [17][30]\t Batch [500][550]\t Training Loss 0.2194\t Accuracy 0.9300\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2779\t Average training accuracy 0.9223\n",
      "Epoch [17]\t Average validation loss 0.2264\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [18][30]\t Batch [0][550]\t Training Loss 0.3760\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [50][550]\t Training Loss 0.2991\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [100][550]\t Training Loss 0.1748\t Accuracy 0.9400\n",
      "Epoch [18][30]\t Batch [150][550]\t Training Loss 0.5714\t Accuracy 0.8700\n",
      "Epoch [18][30]\t Batch [200][550]\t Training Loss 0.1613\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [250][550]\t Training Loss 0.1254\t Accuracy 0.9700\n",
      "Epoch [18][30]\t Batch [300][550]\t Training Loss 0.2232\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [350][550]\t Training Loss 0.3834\t Accuracy 0.9000\n",
      "Epoch [18][30]\t Batch [400][550]\t Training Loss 0.2317\t Accuracy 0.9000\n",
      "Epoch [18][30]\t Batch [450][550]\t Training Loss 0.2307\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [500][550]\t Training Loss 0.5116\t Accuracy 0.8700\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2768\t Average training accuracy 0.9223\n",
      "Epoch [18]\t Average validation loss 0.2270\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [19][30]\t Batch [0][550]\t Training Loss 0.3088\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [50][550]\t Training Loss 0.2958\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [100][550]\t Training Loss 0.3738\t Accuracy 0.8900\n",
      "Epoch [19][30]\t Batch [150][550]\t Training Loss 0.3529\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [200][550]\t Training Loss 0.2459\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [250][550]\t Training Loss 0.2161\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [300][550]\t Training Loss 0.2077\t Accuracy 0.9400\n",
      "Epoch [19][30]\t Batch [350][550]\t Training Loss 0.1661\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [400][550]\t Training Loss 0.1825\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [450][550]\t Training Loss 0.2332\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [500][550]\t Training Loss 0.1850\t Accuracy 0.9700\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2756\t Average training accuracy 0.9231\n",
      "Epoch [19]\t Average validation loss 0.2244\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [20][30]\t Batch [0][550]\t Training Loss 0.2402\t Accuracy 0.8800\n",
      "Epoch [20][30]\t Batch [50][550]\t Training Loss 0.2173\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [100][550]\t Training Loss 0.3622\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [150][550]\t Training Loss 0.4495\t Accuracy 0.9100\n",
      "Epoch [20][30]\t Batch [200][550]\t Training Loss 0.4013\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [250][550]\t Training Loss 0.4299\t Accuracy 0.8700\n",
      "Epoch [20][30]\t Batch [300][550]\t Training Loss 0.1779\t Accuracy 0.9300\n",
      "Epoch [20][30]\t Batch [350][550]\t Training Loss 0.2960\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [400][550]\t Training Loss 0.2116\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [450][550]\t Training Loss 0.4265\t Accuracy 0.9200\n",
      "Epoch [20][30]\t Batch [500][550]\t Training Loss 0.3419\t Accuracy 0.9000\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2744\t Average training accuracy 0.9230\n",
      "Epoch [20]\t Average validation loss 0.2247\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [21][30]\t Batch [0][550]\t Training Loss 0.2880\t Accuracy 0.9000\n",
      "Epoch [21][30]\t Batch [50][550]\t Training Loss 0.3417\t Accuracy 0.8600\n",
      "Epoch [21][30]\t Batch [100][550]\t Training Loss 0.3144\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [150][550]\t Training Loss 0.2663\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [200][550]\t Training Loss 0.3195\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [250][550]\t Training Loss 0.2145\t Accuracy 0.9300\n",
      "Epoch [21][30]\t Batch [300][550]\t Training Loss 0.2952\t Accuracy 0.9200\n",
      "Epoch [21][30]\t Batch [350][550]\t Training Loss 0.2167\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [400][550]\t Training Loss 0.2148\t Accuracy 0.9500\n",
      "Epoch [21][30]\t Batch [450][550]\t Training Loss 0.4530\t Accuracy 0.8700\n",
      "Epoch [21][30]\t Batch [500][550]\t Training Loss 0.3027\t Accuracy 0.8800\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2734\t Average training accuracy 0.9235\n",
      "Epoch [21]\t Average validation loss 0.2247\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [22][30]\t Batch [0][550]\t Training Loss 0.3513\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [50][550]\t Training Loss 0.4969\t Accuracy 0.8600\n",
      "Epoch [22][30]\t Batch [100][550]\t Training Loss 0.2756\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [150][550]\t Training Loss 0.3408\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [200][550]\t Training Loss 0.1459\t Accuracy 0.9700\n",
      "Epoch [22][30]\t Batch [250][550]\t Training Loss 0.1884\t Accuracy 0.9600\n",
      "Epoch [22][30]\t Batch [300][550]\t Training Loss 0.3496\t Accuracy 0.9100\n",
      "Epoch [22][30]\t Batch [350][550]\t Training Loss 0.3488\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [400][550]\t Training Loss 0.2993\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [450][550]\t Training Loss 0.2456\t Accuracy 0.9400\n",
      "Epoch [22][30]\t Batch [500][550]\t Training Loss 0.3849\t Accuracy 0.8500\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2726\t Average training accuracy 0.9239\n",
      "Epoch [22]\t Average validation loss 0.2236\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [23][30]\t Batch [0][550]\t Training Loss 0.3867\t Accuracy 0.9100\n",
      "Epoch [23][30]\t Batch [50][550]\t Training Loss 0.3735\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [100][550]\t Training Loss 0.1756\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [150][550]\t Training Loss 0.5204\t Accuracy 0.8500\n",
      "Epoch [23][30]\t Batch [200][550]\t Training Loss 0.5179\t Accuracy 0.8700\n",
      "Epoch [23][30]\t Batch [250][550]\t Training Loss 0.1670\t Accuracy 0.9400\n",
      "Epoch [23][30]\t Batch [300][550]\t Training Loss 0.1997\t Accuracy 0.9500\n",
      "Epoch [23][30]\t Batch [350][550]\t Training Loss 0.2985\t Accuracy 0.9000\n",
      "Epoch [23][30]\t Batch [400][550]\t Training Loss 0.2282\t Accuracy 0.9200\n",
      "Epoch [23][30]\t Batch [450][550]\t Training Loss 0.2509\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [500][550]\t Training Loss 0.2963\t Accuracy 0.9000\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2717\t Average training accuracy 0.9241\n",
      "Epoch [23]\t Average validation loss 0.2230\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [24][30]\t Batch [0][550]\t Training Loss 0.2261\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [50][550]\t Training Loss 0.2868\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [100][550]\t Training Loss 0.2724\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [150][550]\t Training Loss 0.1720\t Accuracy 0.9600\n",
      "Epoch [24][30]\t Batch [200][550]\t Training Loss 0.2584\t Accuracy 0.9200\n",
      "Epoch [24][30]\t Batch [250][550]\t Training Loss 0.1715\t Accuracy 0.9600\n",
      "Epoch [24][30]\t Batch [300][550]\t Training Loss 0.2583\t Accuracy 0.9000\n",
      "Epoch [24][30]\t Batch [350][550]\t Training Loss 0.5046\t Accuracy 0.8900\n",
      "Epoch [24][30]\t Batch [400][550]\t Training Loss 0.1877\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [450][550]\t Training Loss 0.2244\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [500][550]\t Training Loss 0.1511\t Accuracy 0.9500\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2709\t Average training accuracy 0.9245\n",
      "Epoch [24]\t Average validation loss 0.2234\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [25][30]\t Batch [0][550]\t Training Loss 0.2262\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [50][550]\t Training Loss 0.2676\t Accuracy 0.9200\n",
      "Epoch [25][30]\t Batch [100][550]\t Training Loss 0.2307\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [150][550]\t Training Loss 0.1248\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [200][550]\t Training Loss 0.2728\t Accuracy 0.9500\n",
      "Epoch [25][30]\t Batch [250][550]\t Training Loss 0.4360\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [300][550]\t Training Loss 0.2930\t Accuracy 0.9600\n",
      "Epoch [25][30]\t Batch [350][550]\t Training Loss 0.3478\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [400][550]\t Training Loss 0.3626\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [450][550]\t Training Loss 0.3151\t Accuracy 0.8900\n",
      "Epoch [25][30]\t Batch [500][550]\t Training Loss 0.3790\t Accuracy 0.9300\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2702\t Average training accuracy 0.9250\n",
      "Epoch [25]\t Average validation loss 0.2222\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [26][30]\t Batch [0][550]\t Training Loss 0.0957\t Accuracy 0.9700\n",
      "Epoch [26][30]\t Batch [50][550]\t Training Loss 0.2976\t Accuracy 0.8900\n",
      "Epoch [26][30]\t Batch [100][550]\t Training Loss 0.3442\t Accuracy 0.9200\n",
      "Epoch [26][30]\t Batch [150][550]\t Training Loss 0.2933\t Accuracy 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26][30]\t Batch [200][550]\t Training Loss 0.2427\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [250][550]\t Training Loss 0.4726\t Accuracy 0.8900\n",
      "Epoch [26][30]\t Batch [300][550]\t Training Loss 0.3834\t Accuracy 0.9100\n",
      "Epoch [26][30]\t Batch [350][550]\t Training Loss 0.3817\t Accuracy 0.8600\n",
      "Epoch [26][30]\t Batch [400][550]\t Training Loss 0.3236\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [450][550]\t Training Loss 0.2099\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [500][550]\t Training Loss 0.1437\t Accuracy 0.9600\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2694\t Average training accuracy 0.9251\n",
      "Epoch [26]\t Average validation loss 0.2230\t Average validation accuracy 0.9402\n",
      "\n",
      "Epoch [27][30]\t Batch [0][550]\t Training Loss 0.1976\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [50][550]\t Training Loss 0.2580\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [100][550]\t Training Loss 0.1868\t Accuracy 0.9600\n",
      "Epoch [27][30]\t Batch [150][550]\t Training Loss 0.4567\t Accuracy 0.8500\n",
      "Epoch [27][30]\t Batch [200][550]\t Training Loss 0.2143\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [250][550]\t Training Loss 0.3959\t Accuracy 0.8900\n",
      "Epoch [27][30]\t Batch [300][550]\t Training Loss 0.5775\t Accuracy 0.8500\n",
      "Epoch [27][30]\t Batch [350][550]\t Training Loss 0.2241\t Accuracy 0.9200\n",
      "Epoch [27][30]\t Batch [400][550]\t Training Loss 0.2322\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [450][550]\t Training Loss 0.3482\t Accuracy 0.8800\n",
      "Epoch [27][30]\t Batch [500][550]\t Training Loss 0.1953\t Accuracy 0.9700\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2688\t Average training accuracy 0.9253\n",
      "Epoch [27]\t Average validation loss 0.2219\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [28][30]\t Batch [0][550]\t Training Loss 0.4528\t Accuracy 0.8800\n",
      "Epoch [28][30]\t Batch [50][550]\t Training Loss 0.2885\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [100][550]\t Training Loss 0.2655\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [150][550]\t Training Loss 0.2255\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [200][550]\t Training Loss 0.2036\t Accuracy 0.9100\n",
      "Epoch [28][30]\t Batch [250][550]\t Training Loss 0.1769\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [300][550]\t Training Loss 0.1566\t Accuracy 0.9500\n",
      "Epoch [28][30]\t Batch [350][550]\t Training Loss 0.4134\t Accuracy 0.8900\n",
      "Epoch [28][30]\t Batch [400][550]\t Training Loss 0.1617\t Accuracy 0.9400\n",
      "Epoch [28][30]\t Batch [450][550]\t Training Loss 0.2109\t Accuracy 0.9300\n",
      "Epoch [28][30]\t Batch [500][550]\t Training Loss 0.1933\t Accuracy 0.9500\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2681\t Average training accuracy 0.9254\n",
      "Epoch [28]\t Average validation loss 0.2213\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [29][30]\t Batch [0][550]\t Training Loss 0.2175\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [50][550]\t Training Loss 0.1590\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [100][550]\t Training Loss 0.2415\t Accuracy 0.9000\n",
      "Epoch [29][30]\t Batch [150][550]\t Training Loss 0.2851\t Accuracy 0.9100\n",
      "Epoch [29][30]\t Batch [200][550]\t Training Loss 0.1903\t Accuracy 0.9300\n",
      "Epoch [29][30]\t Batch [250][550]\t Training Loss 0.1902\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [300][550]\t Training Loss 0.2401\t Accuracy 0.9200\n",
      "Epoch [29][30]\t Batch [350][550]\t Training Loss 0.1796\t Accuracy 0.9400\n",
      "Epoch [29][30]\t Batch [400][550]\t Training Loss 0.1728\t Accuracy 0.9600\n",
      "Epoch [29][30]\t Batch [450][550]\t Training Loss 0.1644\t Accuracy 0.9600\n",
      "Epoch [29][30]\t Batch [500][550]\t Training Loss 0.2577\t Accuracy 0.9200\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2671\t Average training accuracy 0.9256\n",
      "Epoch [29]\t Average validation loss 0.2220\t Average validation accuracy 0.9390\n",
      "\n",
      "spend time: 40.84233784675598\n",
      "Final test accuracy 0.9233\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][275]\t Training Loss 2.4892\t Accuracy 0.0900\n",
      "Epoch [0][30]\t Batch [50][275]\t Training Loss 0.8348\t Accuracy 0.8100\n",
      "Epoch [0][30]\t Batch [100][275]\t Training Loss 0.6687\t Accuracy 0.8250\n",
      "Epoch [0][30]\t Batch [150][275]\t Training Loss 0.4514\t Accuracy 0.8900\n",
      "Epoch [0][30]\t Batch [200][275]\t Training Loss 0.4310\t Accuracy 0.8850\n",
      "Epoch [0][30]\t Batch [250][275]\t Training Loss 0.5082\t Accuracy 0.8650\n",
      "\n",
      "Epoch [0]\t Average training loss 0.7324\t Average training accuracy 0.7988\n",
      "Epoch [0]\t Average validation loss 0.3734\t Average validation accuracy 0.9092\n",
      "\n",
      "Epoch [1][30]\t Batch [0][275]\t Training Loss 0.4882\t Accuracy 0.8800\n",
      "Epoch [1][30]\t Batch [50][275]\t Training Loss 0.4553\t Accuracy 0.8850\n",
      "Epoch [1][30]\t Batch [100][275]\t Training Loss 0.4198\t Accuracy 0.8850\n",
      "Epoch [1][30]\t Batch [150][275]\t Training Loss 0.3783\t Accuracy 0.9150\n",
      "Epoch [1][30]\t Batch [200][275]\t Training Loss 0.4757\t Accuracy 0.8850\n",
      "Epoch [1][30]\t Batch [250][275]\t Training Loss 0.5081\t Accuracy 0.8700\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4269\t Average training accuracy 0.8831\n",
      "Epoch [1]\t Average validation loss 0.3163\t Average validation accuracy 0.9170\n",
      "\n",
      "Epoch [2][30]\t Batch [0][275]\t Training Loss 0.4434\t Accuracy 0.8600\n",
      "Epoch [2][30]\t Batch [50][275]\t Training Loss 0.4245\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [100][275]\t Training Loss 0.3565\t Accuracy 0.9100\n",
      "Epoch [2][30]\t Batch [150][275]\t Training Loss 0.3282\t Accuracy 0.9000\n",
      "Epoch [2][30]\t Batch [200][275]\t Training Loss 0.3036\t Accuracy 0.9250\n",
      "Epoch [2][30]\t Batch [250][275]\t Training Loss 0.3664\t Accuracy 0.9000\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3837\t Average training accuracy 0.8939\n",
      "Epoch [2]\t Average validation loss 0.2920\t Average validation accuracy 0.9246\n",
      "\n",
      "Epoch [3][30]\t Batch [0][275]\t Training Loss 0.4831\t Accuracy 0.8650\n",
      "Epoch [3][30]\t Batch [50][275]\t Training Loss 0.4703\t Accuracy 0.8550\n",
      "Epoch [3][30]\t Batch [100][275]\t Training Loss 0.3049\t Accuracy 0.9200\n",
      "Epoch [3][30]\t Batch [150][275]\t Training Loss 0.3367\t Accuracy 0.9100\n",
      "Epoch [3][30]\t Batch [200][275]\t Training Loss 0.3846\t Accuracy 0.9050\n",
      "Epoch [3][30]\t Batch [250][275]\t Training Loss 0.3518\t Accuracy 0.8900\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3613\t Average training accuracy 0.8995\n",
      "Epoch [3]\t Average validation loss 0.2788\t Average validation accuracy 0.9254\n",
      "\n",
      "Epoch [4][30]\t Batch [0][275]\t Training Loss 0.2978\t Accuracy 0.9150\n",
      "Epoch [4][30]\t Batch [50][275]\t Training Loss 0.2982\t Accuracy 0.9400\n",
      "Epoch [4][30]\t Batch [100][275]\t Training Loss 0.3463\t Accuracy 0.8950\n",
      "Epoch [4][30]\t Batch [150][275]\t Training Loss 0.3118\t Accuracy 0.9200\n",
      "Epoch [4][30]\t Batch [200][275]\t Training Loss 0.3572\t Accuracy 0.8850\n",
      "Epoch [4][30]\t Batch [250][275]\t Training Loss 0.4090\t Accuracy 0.8650\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3473\t Average training accuracy 0.9023\n",
      "Epoch [4]\t Average validation loss 0.2705\t Average validation accuracy 0.9282\n",
      "\n",
      "Epoch [5][30]\t Batch [0][275]\t Training Loss 0.3395\t Accuracy 0.8950\n",
      "Epoch [5][30]\t Batch [50][275]\t Training Loss 0.4044\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [100][275]\t Training Loss 0.3467\t Accuracy 0.8750\n",
      "Epoch [5][30]\t Batch [150][275]\t Training Loss 0.3169\t Accuracy 0.9100\n",
      "Epoch [5][30]\t Batch [200][275]\t Training Loss 0.3618\t Accuracy 0.9200\n",
      "Epoch [5][30]\t Batch [250][275]\t Training Loss 0.3077\t Accuracy 0.9300\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3370\t Average training accuracy 0.9055\n",
      "Epoch [5]\t Average validation loss 0.2639\t Average validation accuracy 0.9280\n",
      "\n",
      "Epoch [6][30]\t Batch [0][275]\t Training Loss 0.2585\t Accuracy 0.9050\n",
      "Epoch [6][30]\t Batch [50][275]\t Training Loss 0.2951\t Accuracy 0.9150\n",
      "Epoch [6][30]\t Batch [100][275]\t Training Loss 0.2901\t Accuracy 0.9050\n",
      "Epoch [6][30]\t Batch [150][275]\t Training Loss 0.3478\t Accuracy 0.9000\n",
      "Epoch [6][30]\t Batch [200][275]\t Training Loss 0.4119\t Accuracy 0.9100\n",
      "Epoch [6][30]\t Batch [250][275]\t Training Loss 0.3277\t Accuracy 0.9200\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3293\t Average training accuracy 0.9074\n",
      "Epoch [6]\t Average validation loss 0.2597\t Average validation accuracy 0.9290\n",
      "\n",
      "Epoch [7][30]\t Batch [0][275]\t Training Loss 0.3072\t Accuracy 0.9200\n",
      "Epoch [7][30]\t Batch [50][275]\t Training Loss 0.3595\t Accuracy 0.8950\n",
      "Epoch [7][30]\t Batch [100][275]\t Training Loss 0.3737\t Accuracy 0.9000\n",
      "Epoch [7][30]\t Batch [150][275]\t Training Loss 0.4081\t Accuracy 0.8900\n",
      "Epoch [7][30]\t Batch [200][275]\t Training Loss 0.3021\t Accuracy 0.9250\n",
      "Epoch [7][30]\t Batch [250][275]\t Training Loss 0.3017\t Accuracy 0.9200\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3232\t Average training accuracy 0.9092\n",
      "Epoch [7]\t Average validation loss 0.2547\t Average validation accuracy 0.9294\n",
      "\n",
      "Epoch [8][30]\t Batch [0][275]\t Training Loss 0.2815\t Accuracy 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8][30]\t Batch [50][275]\t Training Loss 0.2938\t Accuracy 0.9150\n",
      "Epoch [8][30]\t Batch [100][275]\t Training Loss 0.2119\t Accuracy 0.9500\n",
      "Epoch [8][30]\t Batch [150][275]\t Training Loss 0.2490\t Accuracy 0.9150\n",
      "Epoch [8][30]\t Batch [200][275]\t Training Loss 0.3331\t Accuracy 0.9000\n",
      "Epoch [8][30]\t Batch [250][275]\t Training Loss 0.3308\t Accuracy 0.9200\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3180\t Average training accuracy 0.9108\n",
      "Epoch [8]\t Average validation loss 0.2517\t Average validation accuracy 0.9314\n",
      "\n",
      "Epoch [9][30]\t Batch [0][275]\t Training Loss 0.2707\t Accuracy 0.9250\n",
      "Epoch [9][30]\t Batch [50][275]\t Training Loss 0.2672\t Accuracy 0.9150\n",
      "Epoch [9][30]\t Batch [100][275]\t Training Loss 0.3939\t Accuracy 0.8600\n",
      "Epoch [9][30]\t Batch [150][275]\t Training Loss 0.3333\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [200][275]\t Training Loss 0.3932\t Accuracy 0.9100\n",
      "Epoch [9][30]\t Batch [250][275]\t Training Loss 0.3870\t Accuracy 0.9050\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3139\t Average training accuracy 0.9122\n",
      "Epoch [9]\t Average validation loss 0.2492\t Average validation accuracy 0.9314\n",
      "\n",
      "Epoch [10][30]\t Batch [0][275]\t Training Loss 0.2630\t Accuracy 0.9250\n",
      "Epoch [10][30]\t Batch [50][275]\t Training Loss 0.3149\t Accuracy 0.9250\n",
      "Epoch [10][30]\t Batch [100][275]\t Training Loss 0.2550\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [150][275]\t Training Loss 0.2768\t Accuracy 0.9200\n",
      "Epoch [10][30]\t Batch [200][275]\t Training Loss 0.2652\t Accuracy 0.9300\n",
      "Epoch [10][30]\t Batch [250][275]\t Training Loss 0.4322\t Accuracy 0.8900\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3103\t Average training accuracy 0.9133\n",
      "Epoch [10]\t Average validation loss 0.2472\t Average validation accuracy 0.9322\n",
      "\n",
      "Epoch [11][30]\t Batch [0][275]\t Training Loss 0.3629\t Accuracy 0.9050\n",
      "Epoch [11][30]\t Batch [50][275]\t Training Loss 0.3497\t Accuracy 0.8950\n",
      "Epoch [11][30]\t Batch [100][275]\t Training Loss 0.2642\t Accuracy 0.9150\n",
      "Epoch [11][30]\t Batch [150][275]\t Training Loss 0.2452\t Accuracy 0.9350\n",
      "Epoch [11][30]\t Batch [200][275]\t Training Loss 0.4026\t Accuracy 0.9000\n",
      "Epoch [11][30]\t Batch [250][275]\t Training Loss 0.3094\t Accuracy 0.9100\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3071\t Average training accuracy 0.9140\n",
      "Epoch [11]\t Average validation loss 0.2446\t Average validation accuracy 0.9322\n",
      "\n",
      "Epoch [12][30]\t Batch [0][275]\t Training Loss 0.3692\t Accuracy 0.9200\n",
      "Epoch [12][30]\t Batch [50][275]\t Training Loss 0.3307\t Accuracy 0.9000\n",
      "Epoch [12][30]\t Batch [100][275]\t Training Loss 0.3214\t Accuracy 0.9100\n",
      "Epoch [12][30]\t Batch [150][275]\t Training Loss 0.1928\t Accuracy 0.9350\n",
      "Epoch [12][30]\t Batch [200][275]\t Training Loss 0.1676\t Accuracy 0.9550\n",
      "Epoch [12][30]\t Batch [250][275]\t Training Loss 0.3592\t Accuracy 0.8900\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3042\t Average training accuracy 0.9154\n",
      "Epoch [12]\t Average validation loss 0.2432\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [13][30]\t Batch [0][275]\t Training Loss 0.2731\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [50][275]\t Training Loss 0.2495\t Accuracy 0.9200\n",
      "Epoch [13][30]\t Batch [100][275]\t Training Loss 0.2498\t Accuracy 0.9150\n",
      "Epoch [13][30]\t Batch [150][275]\t Training Loss 0.3040\t Accuracy 0.9150\n",
      "Epoch [13][30]\t Batch [200][275]\t Training Loss 0.2825\t Accuracy 0.9000\n",
      "Epoch [13][30]\t Batch [250][275]\t Training Loss 0.2216\t Accuracy 0.9350\n",
      "\n",
      "Epoch [13]\t Average training loss 0.3016\t Average training accuracy 0.9161\n",
      "Epoch [13]\t Average validation loss 0.2415\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [14][30]\t Batch [0][275]\t Training Loss 0.3525\t Accuracy 0.8850\n",
      "Epoch [14][30]\t Batch [50][275]\t Training Loss 0.2776\t Accuracy 0.9150\n",
      "Epoch [14][30]\t Batch [100][275]\t Training Loss 0.3633\t Accuracy 0.9000\n",
      "Epoch [14][30]\t Batch [150][275]\t Training Loss 0.2680\t Accuracy 0.9350\n",
      "Epoch [14][30]\t Batch [200][275]\t Training Loss 0.2468\t Accuracy 0.9250\n",
      "Epoch [14][30]\t Batch [250][275]\t Training Loss 0.2880\t Accuracy 0.9200\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2995\t Average training accuracy 0.9167\n",
      "Epoch [14]\t Average validation loss 0.2403\t Average validation accuracy 0.9336\n",
      "\n",
      "Epoch [15][30]\t Batch [0][275]\t Training Loss 0.2733\t Accuracy 0.9250\n",
      "Epoch [15][30]\t Batch [50][275]\t Training Loss 0.3794\t Accuracy 0.8900\n",
      "Epoch [15][30]\t Batch [100][275]\t Training Loss 0.3382\t Accuracy 0.9150\n",
      "Epoch [15][30]\t Batch [150][275]\t Training Loss 0.2219\t Accuracy 0.9350\n",
      "Epoch [15][30]\t Batch [200][275]\t Training Loss 0.3844\t Accuracy 0.8900\n",
      "Epoch [15][30]\t Batch [250][275]\t Training Loss 0.3220\t Accuracy 0.8900\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2974\t Average training accuracy 0.9169\n",
      "Epoch [15]\t Average validation loss 0.2394\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [16][30]\t Batch [0][275]\t Training Loss 0.2795\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [50][275]\t Training Loss 0.3212\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [100][275]\t Training Loss 0.3548\t Accuracy 0.9100\n",
      "Epoch [16][30]\t Batch [150][275]\t Training Loss 0.2558\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [200][275]\t Training Loss 0.2422\t Accuracy 0.9200\n",
      "Epoch [16][30]\t Batch [250][275]\t Training Loss 0.3876\t Accuracy 0.8850\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2955\t Average training accuracy 0.9180\n",
      "Epoch [16]\t Average validation loss 0.2382\t Average validation accuracy 0.9344\n",
      "\n",
      "Epoch [17][30]\t Batch [0][275]\t Training Loss 0.3159\t Accuracy 0.9000\n",
      "Epoch [17][30]\t Batch [50][275]\t Training Loss 0.2650\t Accuracy 0.9150\n",
      "Epoch [17][30]\t Batch [100][275]\t Training Loss 0.3323\t Accuracy 0.9100\n",
      "Epoch [17][30]\t Batch [150][275]\t Training Loss 0.2918\t Accuracy 0.8900\n",
      "Epoch [17][30]\t Batch [200][275]\t Training Loss 0.2410\t Accuracy 0.9150\n",
      "Epoch [17][30]\t Batch [250][275]\t Training Loss 0.3731\t Accuracy 0.8950\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2940\t Average training accuracy 0.9176\n",
      "Epoch [17]\t Average validation loss 0.2366\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [18][30]\t Batch [0][275]\t Training Loss 0.3800\t Accuracy 0.9100\n",
      "Epoch [18][30]\t Batch [50][275]\t Training Loss 0.3248\t Accuracy 0.8950\n",
      "Epoch [18][30]\t Batch [100][275]\t Training Loss 0.3292\t Accuracy 0.8900\n",
      "Epoch [18][30]\t Batch [150][275]\t Training Loss 0.2425\t Accuracy 0.9500\n",
      "Epoch [18][30]\t Batch [200][275]\t Training Loss 0.3350\t Accuracy 0.9150\n",
      "Epoch [18][30]\t Batch [250][275]\t Training Loss 0.2356\t Accuracy 0.9350\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2923\t Average training accuracy 0.9190\n",
      "Epoch [18]\t Average validation loss 0.2359\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [19][30]\t Batch [0][275]\t Training Loss 0.2979\t Accuracy 0.9200\n",
      "Epoch [19][30]\t Batch [50][275]\t Training Loss 0.2904\t Accuracy 0.9150\n",
      "Epoch [19][30]\t Batch [100][275]\t Training Loss 0.3020\t Accuracy 0.9050\n",
      "Epoch [19][30]\t Batch [150][275]\t Training Loss 0.2047\t Accuracy 0.9500\n",
      "Epoch [19][30]\t Batch [200][275]\t Training Loss 0.3515\t Accuracy 0.9050\n",
      "Epoch [19][30]\t Batch [250][275]\t Training Loss 0.2712\t Accuracy 0.9200\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2909\t Average training accuracy 0.9188\n",
      "Epoch [19]\t Average validation loss 0.2350\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [20][30]\t Batch [0][275]\t Training Loss 0.1869\t Accuracy 0.9350\n",
      "Epoch [20][30]\t Batch [50][275]\t Training Loss 0.3317\t Accuracy 0.8950\n",
      "Epoch [20][30]\t Batch [100][275]\t Training Loss 0.2669\t Accuracy 0.9350\n",
      "Epoch [20][30]\t Batch [150][275]\t Training Loss 0.2722\t Accuracy 0.9050\n",
      "Epoch [20][30]\t Batch [200][275]\t Training Loss 0.1964\t Accuracy 0.9400\n",
      "Epoch [20][30]\t Batch [250][275]\t Training Loss 0.3000\t Accuracy 0.9100\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2894\t Average training accuracy 0.9195\n",
      "Epoch [20]\t Average validation loss 0.2341\t Average validation accuracy 0.9352\n",
      "\n",
      "Epoch [21][30]\t Batch [0][275]\t Training Loss 0.2331\t Accuracy 0.9350\n",
      "Epoch [21][30]\t Batch [50][275]\t Training Loss 0.3252\t Accuracy 0.9050\n",
      "Epoch [21][30]\t Batch [100][275]\t Training Loss 0.2485\t Accuracy 0.9400\n",
      "Epoch [21][30]\t Batch [150][275]\t Training Loss 0.2343\t Accuracy 0.9450\n",
      "Epoch [21][30]\t Batch [200][275]\t Training Loss 0.2766\t Accuracy 0.9100\n",
      "Epoch [21][30]\t Batch [250][275]\t Training Loss 0.2453\t Accuracy 0.9400\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2883\t Average training accuracy 0.9197\n",
      "Epoch [21]\t Average validation loss 0.2332\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [22][30]\t Batch [0][275]\t Training Loss 0.1798\t Accuracy 0.9500\n",
      "Epoch [22][30]\t Batch [50][275]\t Training Loss 0.3603\t Accuracy 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22][30]\t Batch [100][275]\t Training Loss 0.4035\t Accuracy 0.8800\n",
      "Epoch [22][30]\t Batch [150][275]\t Training Loss 0.3615\t Accuracy 0.8850\n",
      "Epoch [22][30]\t Batch [200][275]\t Training Loss 0.2606\t Accuracy 0.9200\n",
      "Epoch [22][30]\t Batch [250][275]\t Training Loss 0.2654\t Accuracy 0.9250\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2870\t Average training accuracy 0.9202\n",
      "Epoch [22]\t Average validation loss 0.2322\t Average validation accuracy 0.9370\n",
      "\n",
      "Epoch [23][30]\t Batch [0][275]\t Training Loss 0.2828\t Accuracy 0.9050\n",
      "Epoch [23][30]\t Batch [50][275]\t Training Loss 0.3493\t Accuracy 0.9050\n",
      "Epoch [23][30]\t Batch [100][275]\t Training Loss 0.3184\t Accuracy 0.9300\n",
      "Epoch [23][30]\t Batch [150][275]\t Training Loss 0.2651\t Accuracy 0.9450\n",
      "Epoch [23][30]\t Batch [200][275]\t Training Loss 0.3497\t Accuracy 0.9250\n",
      "Epoch [23][30]\t Batch [250][275]\t Training Loss 0.3063\t Accuracy 0.9150\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2859\t Average training accuracy 0.9206\n",
      "Epoch [23]\t Average validation loss 0.2318\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [24][30]\t Batch [0][275]\t Training Loss 0.3117\t Accuracy 0.9400\n",
      "Epoch [24][30]\t Batch [50][275]\t Training Loss 0.2734\t Accuracy 0.9300\n",
      "Epoch [24][30]\t Batch [100][275]\t Training Loss 0.1663\t Accuracy 0.9600\n",
      "Epoch [24][30]\t Batch [150][275]\t Training Loss 0.3068\t Accuracy 0.9150\n",
      "Epoch [24][30]\t Batch [200][275]\t Training Loss 0.2886\t Accuracy 0.9250\n",
      "Epoch [24][30]\t Batch [250][275]\t Training Loss 0.2524\t Accuracy 0.9350\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2849\t Average training accuracy 0.9207\n",
      "Epoch [24]\t Average validation loss 0.2315\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [25][30]\t Batch [0][275]\t Training Loss 0.1700\t Accuracy 0.9650\n",
      "Epoch [25][30]\t Batch [50][275]\t Training Loss 0.3578\t Accuracy 0.9000\n",
      "Epoch [25][30]\t Batch [100][275]\t Training Loss 0.2475\t Accuracy 0.9300\n",
      "Epoch [25][30]\t Batch [150][275]\t Training Loss 0.2939\t Accuracy 0.9000\n",
      "Epoch [25][30]\t Batch [200][275]\t Training Loss 0.3502\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [250][275]\t Training Loss 0.2086\t Accuracy 0.9300\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2838\t Average training accuracy 0.9210\n",
      "Epoch [25]\t Average validation loss 0.2309\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [26][30]\t Batch [0][275]\t Training Loss 0.2965\t Accuracy 0.9400\n",
      "Epoch [26][30]\t Batch [50][275]\t Training Loss 0.2644\t Accuracy 0.9500\n",
      "Epoch [26][30]\t Batch [100][275]\t Training Loss 0.3081\t Accuracy 0.9300\n",
      "Epoch [26][30]\t Batch [150][275]\t Training Loss 0.4686\t Accuracy 0.8850\n",
      "Epoch [26][30]\t Batch [200][275]\t Training Loss 0.3643\t Accuracy 0.9100\n",
      "Epoch [26][30]\t Batch [250][275]\t Training Loss 0.2296\t Accuracy 0.9350\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2829\t Average training accuracy 0.9211\n",
      "Epoch [26]\t Average validation loss 0.2302\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [27][30]\t Batch [0][275]\t Training Loss 0.2277\t Accuracy 0.9300\n",
      "Epoch [27][30]\t Batch [50][275]\t Training Loss 0.3149\t Accuracy 0.9050\n",
      "Epoch [27][30]\t Batch [100][275]\t Training Loss 0.2788\t Accuracy 0.8950\n",
      "Epoch [27][30]\t Batch [150][275]\t Training Loss 0.3329\t Accuracy 0.9000\n",
      "Epoch [27][30]\t Batch [200][275]\t Training Loss 0.2153\t Accuracy 0.9500\n",
      "Epoch [27][30]\t Batch [250][275]\t Training Loss 0.4278\t Accuracy 0.8900\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2821\t Average training accuracy 0.9215\n",
      "Epoch [27]\t Average validation loss 0.2299\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [28][30]\t Batch [0][275]\t Training Loss 0.2549\t Accuracy 0.9200\n",
      "Epoch [28][30]\t Batch [50][275]\t Training Loss 0.2381\t Accuracy 0.9450\n",
      "Epoch [28][30]\t Batch [100][275]\t Training Loss 0.3408\t Accuracy 0.9150\n",
      "Epoch [28][30]\t Batch [150][275]\t Training Loss 0.2516\t Accuracy 0.9350\n",
      "Epoch [28][30]\t Batch [200][275]\t Training Loss 0.2904\t Accuracy 0.9050\n",
      "Epoch [28][30]\t Batch [250][275]\t Training Loss 0.2576\t Accuracy 0.9450\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2811\t Average training accuracy 0.9213\n",
      "Epoch [28]\t Average validation loss 0.2292\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [29][30]\t Batch [0][275]\t Training Loss 0.2727\t Accuracy 0.9450\n",
      "Epoch [29][30]\t Batch [50][275]\t Training Loss 0.2101\t Accuracy 0.9550\n",
      "Epoch [29][30]\t Batch [100][275]\t Training Loss 0.2911\t Accuracy 0.9500\n",
      "Epoch [29][30]\t Batch [150][275]\t Training Loss 0.2941\t Accuracy 0.9450\n",
      "Epoch [29][30]\t Batch [200][275]\t Training Loss 0.1724\t Accuracy 0.9600\n",
      "Epoch [29][30]\t Batch [250][275]\t Training Loss 0.3417\t Accuracy 0.9150\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2804\t Average training accuracy 0.9219\n",
      "Epoch [29]\t Average validation loss 0.2293\t Average validation accuracy 0.9384\n",
      "\n",
      "spend time: 42.94035792350769\n",
      "Final test accuracy 0.9227\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][110]\t Training Loss 2.6933\t Accuracy 0.0580\n",
      "Epoch [0][30]\t Batch [50][110]\t Training Loss 0.8613\t Accuracy 0.7880\n",
      "Epoch [0][30]\t Batch [100][110]\t Training Loss 0.6281\t Accuracy 0.8400\n",
      "\n",
      "Epoch [0]\t Average training loss 1.0649\t Average training accuracy 0.7067\n",
      "Epoch [0]\t Average validation loss 0.5045\t Average validation accuracy 0.8894\n",
      "\n",
      "Epoch [1][30]\t Batch [0][110]\t Training Loss 0.5564\t Accuracy 0.8600\n",
      "Epoch [1][30]\t Batch [50][110]\t Training Loss 0.4992\t Accuracy 0.9060\n",
      "Epoch [1][30]\t Batch [100][110]\t Training Loss 0.4670\t Accuracy 0.8680\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5350\t Average training accuracy 0.8622\n",
      "Epoch [1]\t Average validation loss 0.3960\t Average validation accuracy 0.9074\n",
      "\n",
      "Epoch [2][30]\t Batch [0][110]\t Training Loss 0.4757\t Accuracy 0.8820\n",
      "Epoch [2][30]\t Batch [50][110]\t Training Loss 0.5283\t Accuracy 0.8420\n",
      "Epoch [2][30]\t Batch [100][110]\t Training Loss 0.4634\t Accuracy 0.8560\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4631\t Average training accuracy 0.8767\n",
      "Epoch [2]\t Average validation loss 0.3537\t Average validation accuracy 0.9136\n",
      "\n",
      "Epoch [3][30]\t Batch [0][110]\t Training Loss 0.4844\t Accuracy 0.8660\n",
      "Epoch [3][30]\t Batch [50][110]\t Training Loss 0.4354\t Accuracy 0.8700\n",
      "Epoch [3][30]\t Batch [100][110]\t Training Loss 0.4214\t Accuracy 0.8880\n",
      "\n",
      "Epoch [3]\t Average training loss 0.4275\t Average training accuracy 0.8846\n",
      "Epoch [3]\t Average validation loss 0.3297\t Average validation accuracy 0.9158\n",
      "\n",
      "Epoch [4][30]\t Batch [0][110]\t Training Loss 0.3846\t Accuracy 0.8900\n",
      "Epoch [4][30]\t Batch [50][110]\t Training Loss 0.5066\t Accuracy 0.8560\n",
      "Epoch [4][30]\t Batch [100][110]\t Training Loss 0.3722\t Accuracy 0.9000\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4052\t Average training accuracy 0.8897\n",
      "Epoch [4]\t Average validation loss 0.3141\t Average validation accuracy 0.9188\n",
      "\n",
      "Epoch [5][30]\t Batch [0][110]\t Training Loss 0.3659\t Accuracy 0.9020\n",
      "Epoch [5][30]\t Batch [50][110]\t Training Loss 0.3712\t Accuracy 0.9000\n",
      "Epoch [5][30]\t Batch [100][110]\t Training Loss 0.3474\t Accuracy 0.8940\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3896\t Average training accuracy 0.8937\n",
      "Epoch [5]\t Average validation loss 0.3027\t Average validation accuracy 0.9210\n",
      "\n",
      "Epoch [6][30]\t Batch [0][110]\t Training Loss 0.4112\t Accuracy 0.8980\n",
      "Epoch [6][30]\t Batch [50][110]\t Training Loss 0.3388\t Accuracy 0.9120\n",
      "Epoch [6][30]\t Batch [100][110]\t Training Loss 0.3677\t Accuracy 0.9000\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3776\t Average training accuracy 0.8961\n",
      "Epoch [6]\t Average validation loss 0.2943\t Average validation accuracy 0.9228\n",
      "\n",
      "Epoch [7][30]\t Batch [0][110]\t Training Loss 0.4188\t Accuracy 0.8880\n",
      "Epoch [7][30]\t Batch [50][110]\t Training Loss 0.4040\t Accuracy 0.8780\n",
      "Epoch [7][30]\t Batch [100][110]\t Training Loss 0.3274\t Accuracy 0.8980\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3682\t Average training accuracy 0.8984\n",
      "Epoch [7]\t Average validation loss 0.2875\t Average validation accuracy 0.9230\n",
      "\n",
      "Epoch [8][30]\t Batch [0][110]\t Training Loss 0.4033\t Accuracy 0.8900\n",
      "Epoch [8][30]\t Batch [50][110]\t Training Loss 0.3292\t Accuracy 0.9160\n",
      "Epoch [8][30]\t Batch [100][110]\t Training Loss 0.3527\t Accuracy 0.9080\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3606\t Average training accuracy 0.8997\n",
      "Epoch [8]\t Average validation loss 0.2821\t Average validation accuracy 0.9248\n",
      "\n",
      "Epoch [9][30]\t Batch [0][110]\t Training Loss 0.3207\t Accuracy 0.9140\n",
      "Epoch [9][30]\t Batch [50][110]\t Training Loss 0.3462\t Accuracy 0.8860\n",
      "Epoch [9][30]\t Batch [100][110]\t Training Loss 0.3189\t Accuracy 0.9040\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3541\t Average training accuracy 0.9011\n",
      "Epoch [9]\t Average validation loss 0.2777\t Average validation accuracy 0.9258\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10][30]\t Batch [0][110]\t Training Loss 0.3942\t Accuracy 0.8880\n",
      "Epoch [10][30]\t Batch [50][110]\t Training Loss 0.3845\t Accuracy 0.8880\n",
      "Epoch [10][30]\t Batch [100][110]\t Training Loss 0.3423\t Accuracy 0.9100\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3487\t Average training accuracy 0.9030\n",
      "Epoch [10]\t Average validation loss 0.2739\t Average validation accuracy 0.9274\n",
      "\n",
      "Epoch [11][30]\t Batch [0][110]\t Training Loss 0.3974\t Accuracy 0.8960\n",
      "Epoch [11][30]\t Batch [50][110]\t Training Loss 0.4122\t Accuracy 0.8720\n",
      "Epoch [11][30]\t Batch [100][110]\t Training Loss 0.4034\t Accuracy 0.8820\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3438\t Average training accuracy 0.9043\n",
      "Epoch [11]\t Average validation loss 0.2705\t Average validation accuracy 0.9276\n",
      "\n",
      "Epoch [12][30]\t Batch [0][110]\t Training Loss 0.3270\t Accuracy 0.9160\n",
      "Epoch [12][30]\t Batch [50][110]\t Training Loss 0.3904\t Accuracy 0.8760\n",
      "Epoch [12][30]\t Batch [100][110]\t Training Loss 0.3017\t Accuracy 0.9080\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3396\t Average training accuracy 0.9057\n",
      "Epoch [12]\t Average validation loss 0.2678\t Average validation accuracy 0.9278\n",
      "\n",
      "Epoch [13][30]\t Batch [0][110]\t Training Loss 0.2971\t Accuracy 0.9260\n",
      "Epoch [13][30]\t Batch [50][110]\t Training Loss 0.3897\t Accuracy 0.8880\n",
      "Epoch [13][30]\t Batch [100][110]\t Training Loss 0.3169\t Accuracy 0.9120\n",
      "\n",
      "Epoch [13]\t Average training loss 0.3359\t Average training accuracy 0.9067\n",
      "Epoch [13]\t Average validation loss 0.2647\t Average validation accuracy 0.9290\n",
      "\n",
      "Epoch [14][30]\t Batch [0][110]\t Training Loss 0.3566\t Accuracy 0.9060\n",
      "Epoch [14][30]\t Batch [50][110]\t Training Loss 0.3365\t Accuracy 0.8960\n",
      "Epoch [14][30]\t Batch [100][110]\t Training Loss 0.3192\t Accuracy 0.9140\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3325\t Average training accuracy 0.9077\n",
      "Epoch [14]\t Average validation loss 0.2625\t Average validation accuracy 0.9288\n",
      "\n",
      "Epoch [15][30]\t Batch [0][110]\t Training Loss 0.3912\t Accuracy 0.9060\n",
      "Epoch [15][30]\t Batch [50][110]\t Training Loss 0.2978\t Accuracy 0.9380\n",
      "Epoch [15][30]\t Batch [100][110]\t Training Loss 0.3583\t Accuracy 0.9020\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3295\t Average training accuracy 0.9084\n",
      "Epoch [15]\t Average validation loss 0.2601\t Average validation accuracy 0.9300\n",
      "\n",
      "Epoch [16][30]\t Batch [0][110]\t Training Loss 0.2880\t Accuracy 0.9080\n",
      "Epoch [16][30]\t Batch [50][110]\t Training Loss 0.3229\t Accuracy 0.9160\n",
      "Epoch [16][30]\t Batch [100][110]\t Training Loss 0.3395\t Accuracy 0.9100\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3267\t Average training accuracy 0.9090\n",
      "Epoch [16]\t Average validation loss 0.2587\t Average validation accuracy 0.9296\n",
      "\n",
      "Epoch [17][30]\t Batch [0][110]\t Training Loss 0.3674\t Accuracy 0.8900\n",
      "Epoch [17][30]\t Batch [50][110]\t Training Loss 0.3779\t Accuracy 0.8980\n",
      "Epoch [17][30]\t Batch [100][110]\t Training Loss 0.3764\t Accuracy 0.9120\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3242\t Average training accuracy 0.9097\n",
      "Epoch [17]\t Average validation loss 0.2569\t Average validation accuracy 0.9318\n",
      "\n",
      "Epoch [18][30]\t Batch [0][110]\t Training Loss 0.3125\t Accuracy 0.9200\n",
      "Epoch [18][30]\t Batch [50][110]\t Training Loss 0.2759\t Accuracy 0.9240\n",
      "Epoch [18][30]\t Batch [100][110]\t Training Loss 0.3743\t Accuracy 0.8900\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3219\t Average training accuracy 0.9105\n",
      "Epoch [18]\t Average validation loss 0.2553\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [19][30]\t Batch [0][110]\t Training Loss 0.3177\t Accuracy 0.9040\n",
      "Epoch [19][30]\t Batch [50][110]\t Training Loss 0.3465\t Accuracy 0.9020\n",
      "Epoch [19][30]\t Batch [100][110]\t Training Loss 0.2728\t Accuracy 0.9100\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3197\t Average training accuracy 0.9110\n",
      "Epoch [19]\t Average validation loss 0.2535\t Average validation accuracy 0.9316\n",
      "\n",
      "Epoch [20][30]\t Batch [0][110]\t Training Loss 0.3164\t Accuracy 0.8960\n",
      "Epoch [20][30]\t Batch [50][110]\t Training Loss 0.3534\t Accuracy 0.9020\n",
      "Epoch [20][30]\t Batch [100][110]\t Training Loss 0.2906\t Accuracy 0.9100\n",
      "\n",
      "Epoch [20]\t Average training loss 0.3176\t Average training accuracy 0.9113\n",
      "Epoch [20]\t Average validation loss 0.2522\t Average validation accuracy 0.9318\n",
      "\n",
      "Epoch [21][30]\t Batch [0][110]\t Training Loss 0.2489\t Accuracy 0.9360\n",
      "Epoch [21][30]\t Batch [50][110]\t Training Loss 0.3500\t Accuracy 0.9020\n",
      "Epoch [21][30]\t Batch [100][110]\t Training Loss 0.3247\t Accuracy 0.8900\n",
      "\n",
      "Epoch [21]\t Average training loss 0.3159\t Average training accuracy 0.9120\n",
      "Epoch [21]\t Average validation loss 0.2510\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [22][30]\t Batch [0][110]\t Training Loss 0.3429\t Accuracy 0.8900\n",
      "Epoch [22][30]\t Batch [50][110]\t Training Loss 0.2426\t Accuracy 0.9300\n",
      "Epoch [22][30]\t Batch [100][110]\t Training Loss 0.2947\t Accuracy 0.9260\n",
      "\n",
      "Epoch [22]\t Average training loss 0.3141\t Average training accuracy 0.9124\n",
      "Epoch [22]\t Average validation loss 0.2497\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [23][30]\t Batch [0][110]\t Training Loss 0.3787\t Accuracy 0.8920\n",
      "Epoch [23][30]\t Batch [50][110]\t Training Loss 0.3973\t Accuracy 0.8780\n",
      "Epoch [23][30]\t Batch [100][110]\t Training Loss 0.2353\t Accuracy 0.9340\n",
      "\n",
      "Epoch [23]\t Average training loss 0.3126\t Average training accuracy 0.9128\n",
      "Epoch [23]\t Average validation loss 0.2486\t Average validation accuracy 0.9330\n",
      "\n",
      "Epoch [24][30]\t Batch [0][110]\t Training Loss 0.2811\t Accuracy 0.9160\n",
      "Epoch [24][30]\t Batch [50][110]\t Training Loss 0.3502\t Accuracy 0.9060\n",
      "Epoch [24][30]\t Batch [100][110]\t Training Loss 0.2865\t Accuracy 0.9140\n",
      "\n",
      "Epoch [24]\t Average training loss 0.3110\t Average training accuracy 0.9131\n",
      "Epoch [24]\t Average validation loss 0.2478\t Average validation accuracy 0.9330\n",
      "\n",
      "Epoch [25][30]\t Batch [0][110]\t Training Loss 0.4366\t Accuracy 0.8820\n",
      "Epoch [25][30]\t Batch [50][110]\t Training Loss 0.2490\t Accuracy 0.9220\n",
      "Epoch [25][30]\t Batch [100][110]\t Training Loss 0.3237\t Accuracy 0.9080\n",
      "\n",
      "Epoch [25]\t Average training loss 0.3095\t Average training accuracy 0.9140\n",
      "Epoch [25]\t Average validation loss 0.2465\t Average validation accuracy 0.9334\n",
      "\n",
      "Epoch [26][30]\t Batch [0][110]\t Training Loss 0.3462\t Accuracy 0.9020\n",
      "Epoch [26][30]\t Batch [50][110]\t Training Loss 0.3151\t Accuracy 0.9060\n",
      "Epoch [26][30]\t Batch [100][110]\t Training Loss 0.3114\t Accuracy 0.9160\n",
      "\n",
      "Epoch [26]\t Average training loss 0.3083\t Average training accuracy 0.9142\n",
      "Epoch [26]\t Average validation loss 0.2458\t Average validation accuracy 0.9332\n",
      "\n",
      "Epoch [27][30]\t Batch [0][110]\t Training Loss 0.2840\t Accuracy 0.9400\n",
      "Epoch [27][30]\t Batch [50][110]\t Training Loss 0.2661\t Accuracy 0.9240\n",
      "Epoch [27][30]\t Batch [100][110]\t Training Loss 0.3395\t Accuracy 0.9020\n",
      "\n",
      "Epoch [27]\t Average training loss 0.3069\t Average training accuracy 0.9145\n",
      "Epoch [27]\t Average validation loss 0.2449\t Average validation accuracy 0.9336\n",
      "\n",
      "Epoch [28][30]\t Batch [0][110]\t Training Loss 0.2951\t Accuracy 0.9140\n",
      "Epoch [28][30]\t Batch [50][110]\t Training Loss 0.3137\t Accuracy 0.9320\n",
      "Epoch [28][30]\t Batch [100][110]\t Training Loss 0.2795\t Accuracy 0.9340\n",
      "\n",
      "Epoch [28]\t Average training loss 0.3057\t Average training accuracy 0.9154\n",
      "Epoch [28]\t Average validation loss 0.2442\t Average validation accuracy 0.9336\n",
      "\n",
      "Epoch [29][30]\t Batch [0][110]\t Training Loss 0.2724\t Accuracy 0.9280\n",
      "Epoch [29][30]\t Batch [50][110]\t Training Loss 0.3105\t Accuracy 0.9160\n",
      "Epoch [29][30]\t Batch [100][110]\t Training Loss 0.3307\t Accuracy 0.9220\n",
      "\n",
      "Epoch [29]\t Average training loss 0.3045\t Average training accuracy 0.9154\n",
      "Epoch [29]\t Average validation loss 0.2437\t Average validation accuracy 0.9336\n",
      "\n",
      "spend time: 39.70932483673096\n",
      "Final test accuracy 0.9189\n",
      "\n",
      "max_epoch = 30\n",
      "Epoch [0][30]\t Batch [0][55]\t Training Loss 2.4300\t Accuracy 0.1690\n",
      "Epoch [0][30]\t Batch [50][55]\t Training Loss 0.8216\t Accuracy 0.8130\n",
      "\n",
      "Epoch [0]\t Average training loss 1.3885\t Average training accuracy 0.6123\n",
      "Epoch [0]\t Average validation loss 0.6925\t Average validation accuracy 0.8638\n",
      "\n",
      "Epoch [1][30]\t Batch [0][55]\t Training Loss 0.7880\t Accuracy 0.7970\n",
      "Epoch [1][30]\t Batch [50][55]\t Training Loss 0.5600\t Accuracy 0.8790\n",
      "\n",
      "Epoch [1]\t Average training loss 0.6716\t Average training accuracy 0.8370\n",
      "Epoch [1]\t Average validation loss 0.4971\t Average validation accuracy 0.8928\n",
      "\n",
      "Epoch [2][30]\t Batch [0][55]\t Training Loss 0.5567\t Accuracy 0.8710\n",
      "Epoch [2][30]\t Batch [50][55]\t Training Loss 0.5670\t Accuracy 0.8520\n",
      "\n",
      "Epoch [2]\t Average training loss 0.5578\t Average training accuracy 0.8591\n",
      "Epoch [2]\t Average validation loss 0.4289\t Average validation accuracy 0.9002\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3][30]\t Batch [0][55]\t Training Loss 0.5334\t Accuracy 0.8580\n",
      "Epoch [3][30]\t Batch [50][55]\t Training Loss 0.5222\t Accuracy 0.8600\n",
      "\n",
      "Epoch [3]\t Average training loss 0.5051\t Average training accuracy 0.8685\n",
      "Epoch [3]\t Average validation loss 0.3919\t Average validation accuracy 0.9074\n",
      "\n",
      "Epoch [4][30]\t Batch [0][55]\t Training Loss 0.4515\t Accuracy 0.8740\n",
      "Epoch [4][30]\t Batch [50][55]\t Training Loss 0.4930\t Accuracy 0.8650\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4729\t Average training accuracy 0.8750\n",
      "Epoch [4]\t Average validation loss 0.3676\t Average validation accuracy 0.9130\n",
      "\n",
      "Epoch [5][30]\t Batch [0][55]\t Training Loss 0.4375\t Accuracy 0.8740\n",
      "Epoch [5][30]\t Batch [50][55]\t Training Loss 0.4631\t Accuracy 0.8830\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4504\t Average training accuracy 0.8799\n",
      "Epoch [5]\t Average validation loss 0.3505\t Average validation accuracy 0.9154\n",
      "\n",
      "Epoch [6][30]\t Batch [0][55]\t Training Loss 0.4433\t Accuracy 0.8750\n",
      "Epoch [6][30]\t Batch [50][55]\t Training Loss 0.4527\t Accuracy 0.8880\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4336\t Average training accuracy 0.8830\n",
      "Epoch [6]\t Average validation loss 0.3377\t Average validation accuracy 0.9178\n",
      "\n",
      "Epoch [7][30]\t Batch [0][55]\t Training Loss 0.4207\t Accuracy 0.8870\n",
      "Epoch [7][30]\t Batch [50][55]\t Training Loss 0.4431\t Accuracy 0.8650\n",
      "\n",
      "Epoch [7]\t Average training loss 0.4204\t Average training accuracy 0.8859\n",
      "Epoch [7]\t Average validation loss 0.3278\t Average validation accuracy 0.9188\n",
      "\n",
      "Epoch [8][30]\t Batch [0][55]\t Training Loss 0.4127\t Accuracy 0.8830\n",
      "Epoch [8][30]\t Batch [50][55]\t Training Loss 0.4211\t Accuracy 0.8900\n",
      "\n",
      "Epoch [8]\t Average training loss 0.4098\t Average training accuracy 0.8883\n",
      "Epoch [8]\t Average validation loss 0.3195\t Average validation accuracy 0.9214\n",
      "\n",
      "Epoch [9][30]\t Batch [0][55]\t Training Loss 0.4251\t Accuracy 0.8900\n",
      "Epoch [9][30]\t Batch [50][55]\t Training Loss 0.3818\t Accuracy 0.8970\n",
      "\n",
      "Epoch [9]\t Average training loss 0.4007\t Average training accuracy 0.8902\n",
      "Epoch [9]\t Average validation loss 0.3126\t Average validation accuracy 0.9222\n",
      "\n",
      "Epoch [10][30]\t Batch [0][55]\t Training Loss 0.3933\t Accuracy 0.8870\n",
      "Epoch [10][30]\t Batch [50][55]\t Training Loss 0.3984\t Accuracy 0.8810\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3931\t Average training accuracy 0.8917\n",
      "Epoch [10]\t Average validation loss 0.3070\t Average validation accuracy 0.9228\n",
      "\n",
      "Epoch [11][30]\t Batch [0][55]\t Training Loss 0.3921\t Accuracy 0.8800\n",
      "Epoch [11][30]\t Batch [50][55]\t Training Loss 0.3857\t Accuracy 0.8880\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3864\t Average training accuracy 0.8933\n",
      "Epoch [11]\t Average validation loss 0.3021\t Average validation accuracy 0.9240\n",
      "\n",
      "Epoch [12][30]\t Batch [0][55]\t Training Loss 0.4049\t Accuracy 0.8810\n",
      "Epoch [12][30]\t Batch [50][55]\t Training Loss 0.3805\t Accuracy 0.8830\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3806\t Average training accuracy 0.8947\n",
      "Epoch [12]\t Average validation loss 0.2975\t Average validation accuracy 0.9260\n",
      "\n",
      "Epoch [13][30]\t Batch [0][55]\t Training Loss 0.4163\t Accuracy 0.8920\n",
      "Epoch [13][30]\t Batch [50][55]\t Training Loss 0.3458\t Accuracy 0.9080\n",
      "\n",
      "Epoch [13]\t Average training loss 0.3755\t Average training accuracy 0.8962\n",
      "Epoch [13]\t Average validation loss 0.2939\t Average validation accuracy 0.9268\n",
      "\n",
      "Epoch [14][30]\t Batch [0][55]\t Training Loss 0.4096\t Accuracy 0.8920\n",
      "Epoch [14][30]\t Batch [50][55]\t Training Loss 0.3583\t Accuracy 0.8950\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3708\t Average training accuracy 0.8970\n",
      "Epoch [14]\t Average validation loss 0.2903\t Average validation accuracy 0.9268\n",
      "\n",
      "Epoch [15][30]\t Batch [0][55]\t Training Loss 0.3136\t Accuracy 0.9180\n",
      "Epoch [15][30]\t Batch [50][55]\t Training Loss 0.3459\t Accuracy 0.9140\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3667\t Average training accuracy 0.8982\n",
      "Epoch [15]\t Average validation loss 0.2874\t Average validation accuracy 0.9274\n",
      "\n",
      "Epoch [16][30]\t Batch [0][55]\t Training Loss 0.3640\t Accuracy 0.9020\n",
      "Epoch [16][30]\t Batch [50][55]\t Training Loss 0.3523\t Accuracy 0.9020\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3629\t Average training accuracy 0.8990\n",
      "Epoch [16]\t Average validation loss 0.2845\t Average validation accuracy 0.9284\n",
      "\n",
      "Epoch [17][30]\t Batch [0][55]\t Training Loss 0.3506\t Accuracy 0.9030\n",
      "Epoch [17][30]\t Batch [50][55]\t Training Loss 0.3687\t Accuracy 0.9100\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3594\t Average training accuracy 0.9002\n",
      "Epoch [17]\t Average validation loss 0.2820\t Average validation accuracy 0.9290\n",
      "\n",
      "Epoch [18][30]\t Batch [0][55]\t Training Loss 0.3785\t Accuracy 0.8890\n",
      "Epoch [18][30]\t Batch [50][55]\t Training Loss 0.3714\t Accuracy 0.8880\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3563\t Average training accuracy 0.9008\n",
      "Epoch [18]\t Average validation loss 0.2797\t Average validation accuracy 0.9290\n",
      "\n",
      "Epoch [19][30]\t Batch [0][55]\t Training Loss 0.3445\t Accuracy 0.9100\n",
      "Epoch [19][30]\t Batch [50][55]\t Training Loss 0.3523\t Accuracy 0.9040\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3533\t Average training accuracy 0.9016\n",
      "Epoch [19]\t Average validation loss 0.2775\t Average validation accuracy 0.9284\n",
      "\n",
      "Epoch [20][30]\t Batch [0][55]\t Training Loss 0.3378\t Accuracy 0.9160\n",
      "Epoch [20][30]\t Batch [50][55]\t Training Loss 0.3344\t Accuracy 0.9090\n",
      "\n",
      "Epoch [20]\t Average training loss 0.3505\t Average training accuracy 0.9026\n",
      "Epoch [20]\t Average validation loss 0.2757\t Average validation accuracy 0.9292\n",
      "\n",
      "Epoch [21][30]\t Batch [0][55]\t Training Loss 0.3369\t Accuracy 0.9060\n",
      "Epoch [21][30]\t Batch [50][55]\t Training Loss 0.3412\t Accuracy 0.8970\n",
      "\n",
      "Epoch [21]\t Average training loss 0.3480\t Average training accuracy 0.9029\n",
      "Epoch [21]\t Average validation loss 0.2739\t Average validation accuracy 0.9298\n",
      "\n",
      "Epoch [22][30]\t Batch [0][55]\t Training Loss 0.3521\t Accuracy 0.9050\n",
      "Epoch [22][30]\t Batch [50][55]\t Training Loss 0.3705\t Accuracy 0.8940\n",
      "\n",
      "Epoch [22]\t Average training loss 0.3456\t Average training accuracy 0.9037\n",
      "Epoch [22]\t Average validation loss 0.2723\t Average validation accuracy 0.9292\n",
      "\n",
      "Epoch [23][30]\t Batch [0][55]\t Training Loss 0.3469\t Accuracy 0.9060\n",
      "Epoch [23][30]\t Batch [50][55]\t Training Loss 0.3423\t Accuracy 0.8950\n",
      "\n",
      "Epoch [23]\t Average training loss 0.3434\t Average training accuracy 0.9043\n",
      "Epoch [23]\t Average validation loss 0.2704\t Average validation accuracy 0.9294\n",
      "\n",
      "Epoch [24][30]\t Batch [0][55]\t Training Loss 0.3645\t Accuracy 0.8960\n",
      "Epoch [24][30]\t Batch [50][55]\t Training Loss 0.3594\t Accuracy 0.8910\n",
      "\n",
      "Epoch [24]\t Average training loss 0.3413\t Average training accuracy 0.9049\n",
      "Epoch [24]\t Average validation loss 0.2691\t Average validation accuracy 0.9296\n",
      "\n",
      "Epoch [25][30]\t Batch [0][55]\t Training Loss 0.3289\t Accuracy 0.9100\n",
      "Epoch [25][30]\t Batch [50][55]\t Training Loss 0.3538\t Accuracy 0.9000\n",
      "\n",
      "Epoch [25]\t Average training loss 0.3393\t Average training accuracy 0.9056\n",
      "Epoch [25]\t Average validation loss 0.2676\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [26][30]\t Batch [0][55]\t Training Loss 0.3157\t Accuracy 0.9130\n",
      "Epoch [26][30]\t Batch [50][55]\t Training Loss 0.3298\t Accuracy 0.9090\n",
      "\n",
      "Epoch [26]\t Average training loss 0.3375\t Average training accuracy 0.9063\n",
      "Epoch [26]\t Average validation loss 0.2665\t Average validation accuracy 0.9292\n",
      "\n",
      "Epoch [27][30]\t Batch [0][55]\t Training Loss 0.3191\t Accuracy 0.9090\n",
      "Epoch [27][30]\t Batch [50][55]\t Training Loss 0.3228\t Accuracy 0.9030\n",
      "\n",
      "Epoch [27]\t Average training loss 0.3357\t Average training accuracy 0.9069\n",
      "Epoch [27]\t Average validation loss 0.2651\t Average validation accuracy 0.9300\n",
      "\n",
      "Epoch [28][30]\t Batch [0][55]\t Training Loss 0.3192\t Accuracy 0.9050\n",
      "Epoch [28][30]\t Batch [50][55]\t Training Loss 0.2772\t Accuracy 0.9300\n",
      "\n",
      "Epoch [28]\t Average training loss 0.3340\t Average training accuracy 0.9073\n",
      "Epoch [28]\t Average validation loss 0.2640\t Average validation accuracy 0.9296\n",
      "\n",
      "Epoch [29][30]\t Batch [0][55]\t Training Loss 0.3773\t Accuracy 0.9020\n",
      "Epoch [29][30]\t Batch [50][55]\t Training Loss 0.3419\t Accuracy 0.9010\n",
      "\n",
      "Epoch [29]\t Average training loss 0.3325\t Average training accuracy 0.9078\n",
      "Epoch [29]\t Average validation loss 0.2627\t Average validation accuracy 0.9296\n",
      "\n",
      "spend time: 40.0618257522583\n",
      "Final test accuracy 0.9141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size_list = [100,200,500,1000]\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "for i in range(len(batch_size_list)):\n",
    "    cfg = {\n",
    "        'data_root': 'data',\n",
    "        'max_epoch': 30,\n",
    "        'batch_size': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'momentum': 0.9,\n",
    "        'display_freq': 50,\n",
    "    }\n",
    "    cfg['batch_size'] = batch_size_list[i]\n",
    "    runner = Solver(cfg)\n",
    "    s = time.time()\n",
    "    loss2, acc2 = runner.train()\n",
    "    e = time.time()\n",
    "    print(\"spend time:\",e-s)\n",
    "    test_loss, test_acc = runner.test()\n",
    "    print('Final test accuracy {:.4f}\\n'.format(test_acc))\n",
    "    acc_list.append(test_acc)\n",
    "    loss_list.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xdata = batch_size_list\n",
    "ydata = acc_list\n",
    "plt.xlabel('batch_size')\n",
    "plt.ylabel('acc_list')\n",
    "#xdata:x轴数据，ydata:y轴数据\n",
    "plt.plot(xdata,ydata, '--ob')\n",
    "_ = plt.xticks(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
